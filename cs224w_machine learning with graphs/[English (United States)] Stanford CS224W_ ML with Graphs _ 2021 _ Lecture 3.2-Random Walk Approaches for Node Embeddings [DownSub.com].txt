So we are going to talk about random walk approaches,
uh, to node embeddings.
Um, and the idea here will be the following.
Uh, we are going to learn a vector z for every node
and this would be an embedding of the node and this is what we aim to find.
We are then going to, um,
also define a probability that, uh, basically will- uh,
will be the pre- predicted probability of how similar a given node u,
um, is uh, to some node, uh,
v. And given that we are going to use random walks to define this similarity,
this would be the probab- proba- uh,
predicted probability of visiting node v,
one random walks starting from node u.
Then we are also goin- need to, um,
nonlinear functions, uh, that will be used to,
uh, define or to produce these probabilities.
First, I'm going to define the notion of a softmax function,
which- which is- uh,
which returns a vector of k real values,
uh, um, and the- and these values sum to one.
So essentially, given a set of numbers z,
the- the softmax of that vector will be
a probability distribution over those values and the more likely that,
um, that- number is the maximum in the vector,
the higher the probability,
uh, it will have.
And essentially the way you can think of it, I take this, uh,
value z and I exponentiate them and then I normalize everything to sum to one.
So the idea is if the largest value, um,
in- in this vector z,
when I expo- exponentiate it,
it will be even larger than everything else.
So most probability mass,
um, will be concentrated on that value.
This is why this is called softmax because it's
a kind of a soft version of a maximum function.
Um, and then we are also going to define this notion of a sigmoid which is an
S shaped function that turns real values into,
uh, a range of- of 01,
um, and softmax is defined as 1 over 1 plus e to the minus x. Um,
and this is a nice way how to take something that lives on, uh,
minus infinity to plus infinity and kind of squish it,
uh, to, uh, value 01.
So that's- those are two important functions,
uh, I will- we will use.
Now let me define the notion of a random walk.
So a random walk is simply,
um, a process on top of the graph,
where we sa- let say start at some node and then out
of the outgoing neighbors of that node,
in this case it will be 1, 3, and 5,
we pick one at random and we move to it and this is one step of random- random walk.
Now we are in this,
uh, node 5, again,
we have four different ways in which we can go.
We can return back to four.
We can go to 8, 6 or 7 and we pick one of them at random and move there.
Um, and this process continues,
let say for a- for a- in this case for a fixed,
uh, number of steps.
So the way you can think of this is that we basically simulated this, uh,
random walk over- over this graph and let's say,
um, over our fixed, uh,
number of steps where the random walk can traverse the same edge multiple times,
can return, can go back and forth,
do, uh, whatever the random walk,
uh, wants to do.
All right. And this random walk is a seq- sequence of nodes visited this way,
uh, on a graph across the- across the edges.
So now how are we going to- to define
this notion of similarity and these probabilities that we talked about?
What we are going to do is to say we want to learn these
coordinate z such that the product of two,
uh, nodes u and v,
um, is similar or equals is, uh,
approximates the probability that u and v co-occur on a random walk, uh, over the graph.
So here is- here is the idea, right?
First we will need to estimate the probability of visiting
node v on a random walk starting,
uh, at some node u using some, let's say,
a random walk strategy R. I'm going to define
this notion of random walks strategy, uh, later,
but for now just think it's a simple random walk where we pick one of the, uh,
uh, neighbors uniformly at random and we move to it.
And then we wanna optimize the embeddings,
uh, in such a way to encode this random walk statistics.
Basically we want the- the cosine of the angle between the two vectors,
this is the dot product to be proportional or similar to the probability that,
uh, u and v are visited, uh,
uh, on the same random, uh, walk.
So why random walks?
We want to use random walks because they are expressive, they are flexible.
It gives us a flexible stochastic definition of node similarity that
incorporates both kind of local as well as higher order neighbor with information, right?
And the idea is that if a random walk, uh,
starting from node u visits v, um,
with high probability that u and v are similar, uh, uh,
they have kind of similar network neighborhood they are close together with each other,
there might be multiple paths between them and so on.
Um, and what is interesting is that this is in some sense also
efficient because we do not need to consider all the node pairs when- when training.
We only need to consider pairs that co-occur in random walks.
[BACKGROUND] So the supervised,
uh, feature learning, uh,
will work the following.
The intuition is that we find embedding of nodes in
d-dimensional space that preserves similarity.
Uh, the idea is that we want to learn node embedding such that nearby nodes in
the network are clo- are- are embedded close together in the embedding space.
Um, and given a node u the question is,
how do we define nearby?
And we are going to have these definition and,
uh, sub r of u, where, uh,
basically this is n labels the neighborhood,
uh, of u obtained by some random walk strategy or r, right?
So for a given node uh,
u we need to define what is the neighborhood?
And in our case, neighborhood will simply be a sequence of nodes that this,
um, uh, that the random walk starting at u has visited.
So now how are we setting this up as an optimization problem?
Given the graph, uh,
on nodes, uh, V and an edge set E,
our goal is to learn a mapping from the nodes, uh,
to their embeddings and we are going to maximize the following,
uh, maximum likelihood objective, right?
Our goal will be to find this function,
this mapping so basically find the coordinates z of the nodes such that,
uh, the summation over all the nodes, uh,
of log probabilities that given the node u, um, that, uh,
maximizes log probabilities of the nodes that appear in its- uh,
in its local uh, random-walk neighborhood, right?
So we want to basically to sum out- to maximize the sum,
which means we want to make nodes that are, um,
that are visited in the same random walk to be kind of embedded,
uh, close together, right?
So we wanna learn feature representations that are
predictive of the nodes in each, uh, uh, uh,
of the nodes that appear in it's, uh,
random walk neighborhood uh, uh,
N. That's the idea.
So how are we going to do this?
First, we are going to run short fixed length
random walk starting from each node u in the graph using,
uh, some random walks strategy R. Uh,
for each node u,
we are going to collect, uh,
N of u which is a multi set of nodes
visited in random walk starting from node u. Multi-set
meaning that a same node can appear
multiple times in the neighborhood because it may be visited multiple times.
Um, and then we are going to optimize- define
an optimization problem and optimize the embedding, so that, uh,
given node u we wanna be able to predict who are
the nodes that are in its neighborhood and defined again by the random walk.
So we are going to maximize, uh, uh, this,
uh, objective here, uh,
this maximum likelihood objective.
So how can we write this out?
We can write this out the following.
We- we- we write it as sum over all the starting nodes u,
sum over all the nodes that are in the neighborhood of u.
Let's call this nodes v and then we wanna maximize the log probability,
uh, that predicts, uh,
that node v, um,
is in the neighborhood of node U. Um,
and as I said opt- intuition is that we want to optimize embeddings to
maximize the likelihood of a random walk, uh, co-occurrence.
Um, how are we going to do this?
Um, we still need to define this, uh,
probability p and the way we are going to define it is we are going to use
the notion of softmax function that I have introduced a couple of slides ago.
So the idea here will be that what we wanna do is we wanna maximize
the dot product between the node u and node v. So node u is the starting node,
node v is the node,
um, in the neighborhood.
Random walk neighborhood of node, uh,
u we wanna maxima- we wanna to apply softf- softmax.
So this is the exponentiated value of the dot product of the node that is in
the neighborhood divided by sum of
the exponential dot product with all the other nodes in the network, right?
So the idea here is that we wanna assign as much probability mass to- uh,
to these dot product um,
and as little to all other, uh, dot products.
So now to write- to put it all together,
the way we can think of this is- this is- we are trying to optimize this function,
which is a sum over all the nodes for every node,
sum over all the nodes v that are seen on
random walks starting from this node u and then we wanna, uh, uh, uh,
optimize for a minus log probability of these softmax which says,
I want to, uh,
maximize the dot product between the- the starting node u and
the node v that is in the neighborhood and we- and we normalize this over all the nodes,
uh, in the network.
So now, um, you know what does it mean optimizing random walk embeddings,
it means finding this coordinates z, uh,
used here such that this likelihood function is, uh, minimized.
Now, uh, the question is,
how do we do this in practice?
And the problem is that this is very expensive because if you look at this,
we actually have to sum- two nested summations,
uh, over all nodes of the network.
We sum over all nodes in the- of- of the network
here for starting nodes of the random walks.
And here when we are normalizing softmax,
we are normalizing it over all of the nodes of the network again.
So this is a double summation,
which means that it will have complexity order, um, V squared.
So it will be, uh, quadratic in the number of nodes in the network,
and that's prohibitively expensive.
So let me tell you what, uh,
we do to make this, uh, practical.
And the- the issue here is that there is this problem with the softmax that, um,
we need to sum over all the nodes to basically
normalize it back to- to a distribution over the nodes.
So, um, can we approximate this theorem?
And the answer is yes.
And the solution to this is called negative sampling.
And intuitively, the idea is that rather than summing here over all the nodes,
uh, in the network, we are only going to sum over a subset of the nodes.
So essentially, we are going to sample a couple of
negative examples and sum, uh, over them.
So the way the approximation works out is that, um, we, um,
we can- we can view this as an approximation to the, um,
to the- to the softmax function, where we can,
uh, approximate it using,
uh, the following, uh, expression.
We are going to take log sigmoid function of the dot product between u and v. Uh,
this is for the, uh, for the theorem here.
And then we are going to say minus sum of i going from one to k,
so this is our k negative examples, logarithm, again,
of the sigmoid function between the, uh,
st- starting node u,
and the negative, um,
negative sample, uh, i,
where this negative samples,
this negative nodes will be, uh,
sampled at random, but not at ra- at uniform random,
but random in a biased way.
So the idea here is that,
instead of normalizing with respect to all nodes in the network,
we are going to normalize softmax against k random negative samples,
so negative nodes, uh, from the network.
And this negative samples will be carefully chosen.
So how do we choose negative samples?
We- we sample k negative nodes,
each with probabil- probability proportional to its degrees.
So it means that nodes that have higher degree,
uh, will be more likely to be chosen as a negative sample.
Um, there are two considerations for picking k in practice,
which means number of negative samples.
Higher values of k will give me more robust estimate.
Uh, but higher values of K also correspond, uh, to,
uh, to more, uh,
to more sampling again, to higher bias on negative events.
So what people tend to choose in practice is k between 5 and 20.
And if you think about it,
this is a very small number.
If you think network of a million nodes or 100,000 nodes,
rather than summing over 100,000,
uh, nodes, uh, every time here,
you are only summing over, you know,
5-20 nodes in this case.
And this way, your method and your estimation will be far,
um, much, much, much more, uh, efficient.
So, um, now how do we solve this optimization problem?
Uh, I won't go into too much detail,
but these things are today solved with,
uh, stochastic gradient descent.
And I just want to give you a quick introduction to stochastic gradient descent, uh,
two slides that are great lectures, uh,
a lot of really good tutorials on what is stochastic gradient descent,
how does it work and all the theoretical analysis of it.
But essentially, the idea is that if you have a smooth function,
then you can optimizing- optimize it by doing gradient descent,
by basically computing the gradient at- at a given point and then moving, um, uh, for,
uh, as a small step, um,
in- in the direction opposite of the gradient, right?
So this is the- this is the idea here, right?
Your start at some random point.
Um, in our case,
we can initialize embeddings of nodes,
uh, at- with random numbers,
and then we iterate until we converge.
We computed the derivative of
the likelihood function with respect to the embedding of a single node,
and now we find the direction of the derivative of the gradient.
And then we make, uh,
a step in the opposite direction of that gradient,
where, um, uh, this is the learning rate,
which means how big step do we take.
And we can actually even tune the step as we make,
uh, as we go and solve.
Uh, but essentially, this is what gradient descent is.
And in stochastic gradient descent,
we are approximating the gradient in a stochastic way.
So rather than evaluating the gradient over all the examples, we just do it, um,
uh, over, uh, a small batch of examples or over an individual examples.
So what does it mean? Is that rather than, um, uh, evaluating,
uh, the gradient over all the nodes- all the negative nodes, um,
and- or all the neighbors in the neighborhood of a given node,
and then make a- make a step,
we are going to do this only for a, uh,
for a given, uh,
for a given node in the neighborhood.
So basically, the idea is that, you know,
we'll sample node i,
and then for all, uh,
js that in the- in the,
uh, in the neighborhood,
we are going to compute the gradient, uh,
and then make a step and keep iterating this.
Uh, we- of course, we'll get
a stochastic estimates or kind of a random estimate of the gradient.
But we'll be able to up- to make updates much, much faster,
which in practice tends,
uh, to be much, uh, better.
So let me summarize.
We are going to run th-
a short fixed-length random walks starting from each node on the graph.
Uh, for each node u,
we are going to collect, um,
its neighborhood and as a multiset of
nodes visited on the random walks starting from node u.
And then we are going to optimize this embeddings
used- using stochastic gradient descent, which means, uh,
we are going to, uh, uh,
find the coordinate Z that maximize,
uh, this particular expression.
And we are going to efficiently approximate this expression,
uh, using negative sampling,
where we, um, sample negative nodes of each probability proportional to their degree.
And in practice, we sample about 5-20 negative examples,
uh, for, uh, for every node, for every step.
So, um, now, um,
the question that I wanna also talk is,
um, you know, how should we do this random walk?
Right? So far, I only described, uh,
how to optimize the embeddings,
uh, for a given the random walk, um, uh,
R. And we talked about this uniform random walk,
where basically we run fixed-length unbiased random walks starting at each node.
And the idea here is that, um,
there is the issue of this, uh,
type of similarity because in many cases,
it might be to constraint.
So the question is, can we use richer, um, random walks?
Can be made the- can be make random walks more
expressive so that we can tune these embeddings more?
And this is the idea of a method called, uh, node2vec,
where the idea is that we wanna, again,
embed nodes with similar network neighborhoods,
uh, closing the feature space.
Uh, we are going to frame.
The goal is again as, um,
maximum likelihood optimization problem,
uh, independent of the downstream prediction task.
Uh, and the key observation here is that
we have a flexible notion of network neighborhood,
um, which leads to much richer, uh, node embeddings.
And the extension of this simple random walk, uh,
here is that we are going to develop a second-order random
walk R to generate- to generate the network neighborhood,
uh, N, and then we are going to apply the same optimization problem.
So the only difference between deep walk and node2vec is how these, uh,
set of neighboring nodes, um,
is defined and how the random walk is defined.
So the idea is to use flexible, um,
biased random walks that can trade off between the local and global views,
uh, in the network.
Um, and what I mean by local and global,
when you are doing the random walk,
you can think of, uh, for example,
depth-first search, uh, as a way to explore as much of the network
as possible given a given budget of steps starting at node u.
But if you really want to get a good understanding how
the network looks like very locally around node u,
then perhaps you'd want to explore the network more in,
uh, um, breadth-first search, uh, fashion.
So this is really,
um, what this will allow us to do.
It will allow us to trade off or kind of extrapolate between breadth-first search, um,
and depth-first search, uh, type,
um, network exploration, right?
Um, uh, as I said, right, like, um,
in terms of strategies to explore the network neighborhood,
uh, and define the notion o- of, uh,
N from a given starting node,
you- you could imagine you wanna explore very
locally and would give you a very local view of the network,
and this will be just kind of breadth-first search exploration.
What you'd wanna look, perhaps a depth-first search explanation, right?
You wanna have these kind of global, uh,
macroscopic view of the network because you
capture much longer and larger, uh, distances.
All right. And that's essentially the intuition behind node2vec is
that you can- you can explore the network in different ways and you will get,
um, better resolution, uh, you know,
at more microscopic view versus more,
uh, macroscopic view, uh, of the network.
So how are we going to now do this in practice?
How are we going to define this random walk?
We are going to do biased fixed-length random walks are that,
that- so that a given node u generates its neighborhood,
uh, N, uh, of u.
And we are going to have two hyperparameters.
We'll have the return parameter p,
that will say how likely is the random walk maker step back,
backtrack to this- to the previous node.
And then we are going to have another parameter q that
are- we are going to call, uh, in-out parameter,
and it will allow us to trade off between moving outward,
kind of doing breadth-first search,
versus staying inwards, staying close to the starting node in this way,
mimicking, uh, breadth-first search.
And intuitively, we can think of q as
the ratio between the breadth-first and depth-first,
uh, exploration of the network.
To make this a bit more, uh, precise,
this is called a second-order random walk because it remembers where it came from.
Um, and then imagine for example in this- in this case that
the random walk just came from node S_1 to the node W. And now at W,
random walk needs to decide what to do, and there are- you know,
it needs to pick a node,
and there are actually three things that th- the- that the no- walker can do.
It can return back where it came from.
It can stay at the same distance,
um, uh, from, uh,
from where it came as it was before,
so you know, it's one hop,
our W is one hop from S_1,
so S_2 is also one hop from S_1.
So this means you stay at the same distance as from S_1 as you were,
or you can navigate farther out,
meaning navigate to someone- somebody who is at a distance 2 from the previous node S_1.
All right? So because we know where the random walker came,
the random walker needs to decide,
go back, stay at the same orbit,
at the same level, Or move one step further?
And the way we are going to parameterize this is using parameters p and q.
So we- di- if you think of this in terms of an unnormalized probabilities,
then we can think that, you know,
staying at the same distance,
we take this with probability proportional to some constant,
we return with probability 1 over p1,
and then we move farther away with probability one over
q one or proportional with- to- to 1 over q1, all right?
So here p is the return parameter,
and q is walk away type parameter.
So how are we going now to do this in- in practice is essentially,
as the random walker,
let's say goes from S_1 to the W,
now it needs to decide where to go.
We are going to have this unnormalized probability distribution of transitions,
which neighbor of W to navigate to.
We are going to normalize these to sum to one and then
flip a biased coin that will- that will navigate,
that will pick one of these four possible options, right?
Returning back, staying at the same distance, or navigating further.
And now, for example,
if I set a low value of p,
then this first term will be very high and the random walk will most likely return.
Uh, if we, uh,
want to navigate farther away,
we set a low value of q,
which means that S_3 and S_4,
will- will get a lot of, uh, probabilityness.
Um, and that's, uh, that's basically the idea.
And then again, the- the set n will be defined
by the nodes visited by this biased random walk
that is trading off the exploration of farther
out in the neigh- neighborhood versus exploration close,
uh, to the- to the- to the starting node,
um, S_1 in this case.
So that is, um,
that is the- that is the idea.
Um, so how does the algorithm work?
We are going to compute the random walk probabilities first.
Then we are going to simulate, the r, um,
biased random walks of some fixed length l starting from each node u.
And then we are going to, uh,
optimize the, uh, objective function, uh,
the same negative sampling objective function
that I- that I already discussed in DeepWalk,
uh, using stochastic gradient descent.
Um, the beauty in this,
is that there is linear time complexity in the optimization.
Because for every node,
we have a fixed set of random walks.
So it's linear, uh,
in the size o- of the graph.
And the- all these different three steps are also parallelizable.
So can- you can run them- uh, in parallel.
The drawback of this, uh,
no demanding approaches, uh,
is that we need to learn a separate,
uh, embedding, uh, for every node, uh, individually.
So with a bigger networks,
we need to learn, uh,
bigger, uh, embeddings, or more embeddings.
Um, of course, there's- there has been a lot of work, um,
after these, uh, these initial,
uh, papers that have proposed these ideas,
there are different kinds, uh,
of random walks that people kept proposed,
that our alternative optimization schemes, um,
and also different network pre-processing techniques, uh,
that allow us to define different notions, uh, of similarity.
Here are some papers that I linked,
uh, you know, if you are interested,
curious to learn more, um,
please, uh, please read them,
it will be a very good read.
So let me summarize what we have learned so far.
So the core idea was to embed nodes.
So the distances in the embedding space
reflect node similarities in the original network.
Um, and we talked about two different notions of node similarity.
Uh, first one was naive similarity where,
uh, if two node-,
if we could- for example,
do, uh, connect, uh,
make notes, uh, close together if they are simply connected by an edge.
We could, uh, do,
a neighborhood similarity, um,
and today we talked about random walk approaches, uh,
to, uh, node similarity where we said,
all the nodes visited on a random walk from a starting node,
those are, uh, similar to it.
So that's, uh, essentially the idea,
uh, for- uh, for today.
So, uh, now of course the question is,
which method should you use?
Um, and no method wins all cases.
So for example, node2vec performs better on a node classification,
while for example, a link prediction,
some alternative methods may perform better.
Uh, there is a very nice survey,
um, three years ago by Goyal and Ferrara.
That, um, surveyed many of these methods and compare them on a number of different tasks.
Um, er, and generally, you know,
random walk approaches are, uh,
quite efficient because you can simulate,
uh, a limited number of random walks.
They don't necessarily scale to the super big networks,
but they scale to lar- to rel- let say medium-size network.
Um , and, uh, in general, right?
You must choose the definition of node similarity
that best matches, uh, your application.