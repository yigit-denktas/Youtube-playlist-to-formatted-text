Local Transcript Files from: C:/Users/yigit/Documents/repos/Youtube-playlist-to-formatted-text/cs224w_machine learning with graphs

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 01.1 - Why Graphs.txt
Welcome to CS224W, Machine Learning with Graphs.
My name is Jure Leskovec.
I'm Associate Professor of Computer Science at
Stanford University and I will be your instructor.
What I'm going to do in the first lecture is to motivate and get you excited about graph,
uh, structured data and how can we apply novel machine learning methods to it?
So why graphs?
Graphs are a general language for describing and an-
analyzing entities with the relations in interactions.
This means that rather than thinking of the world
or a given domain as a set of isolated datapoints,
we really think of it in terms of networks and relations between these entities.
This means that there is
the underla- ler- underlying graph of relations between the entities,
and these entities are related, uh,
to each other, uh,
according to these connections or the structure of the graph.
And there are many types of data that can naturally be
represented as graphs and modeling these graphical relations,
these relational structure of the underlying domain,
uh, allows us to, uh,
build much more faithful,
much more accurate, uh,
models of the underlying,
uh, phenomena underlying data.
So for example, we can think of a computer networks, disease pathways, uh,
networks of particles in physics, uh,
networks of organisms in food webs,
infrastructure, as well as events can all be represented as a graphs.
Similarly, we can think of social networks,
uh, economic networks, communication networks,
say patients between different papers,
Internet as a giant communication network,
as well as ways on how neurons in our brain are connected.
Again, all these domains are inherently network or graphs.
And that representation allows us to capture
the relationships between different objects or entities,
uh, in these different, uh, domains.
And last, we can take knowledge and
represent facts as relationships between different entities.
We can describe the regulatory mechanisms in our cells,
um, as processes governed by the connections between different entities.
We can even take scenes from real world and presented them
as graphs of relationships between the objects in the scene.
These are called scene graphs.
We can take computer code software and represent it as a graph of, let's say,
calls between different functions or as
a structure of the code captures by the abstract syntax tree.
We can also naturally take molecules which are composed of nodes, uh,
of atoms and bonds as a set of graphs, um,
where we represent atoms as nodes and their bonds as edges between them.
And of course, in computer graphics,
we can take three-dimensional shapes and- and represent them, um, as a graphs.
So in all these domains,
graphical structure is the- is
the important part that allows us to model the under- underlying domain,
underlying phenomena in a fateful way.
So the way we are going to think about graph
relational data in this class is that there are essentially two big,
uh, parts, uh, of data that can be represented as graphs.
First are what is called natural graphs or networks,
where underlying domains can naturally be represented as graphs.
For example, social networks,
societies are collection of seven billion individuals and connections between them,
communications and transactions between electronic devices, phone calls,
financial transactions, all naturally form, uh, graphs.
In biomedicine we have genes,
proteins regulating biological processes,
and we can represent interactions between
these different biological entities with a graph.
And- and as I mentioned,
connections between neurons in our brains are,
um, essentially a network of, uh, connections.
And if we want to model these domains,
really present them as networks.
A second example of domains that also have relational structure,
um, where- and we can use graphs to represent that relational structure.
So for example, information and knowledge is many times organized and linked.
Software can be represented as a graph.
We can many times take, uh,
datapoints and connect similar data points.
And this will create our graph,
uh, a similarity network.
And we can take other, um, uh,
domains that have natural relational structure like molecules,
scene graphs, 3D shapes, as well as,
you know, in physics,
we can take particle-based simulation to simulate how,
uh, particles are related to each other through,
uh, and they represent this with the graph.
So this means that there are many different domains, either, uh,
as natural graphs or natural networks,
as well as other domains that can naturally be
modeled as graphs to capture the relational structure.
And the main question for this class that we are
going to address is to talk about how do we take
advantage of this relational structure to be- to make better, more accurate predictions.
And this is especially important because
couplings domains have reached a relational structure,
uh, which can be presented, uh, with a graph.
And by explicitly modeling these relationships,
we will be able to achieve, uh,
better performance, build more, uh,
accurate, uh, models, make more accurate predictions.
And this is especially interesting and important in the age of deep learning,
where the- today's deep learning modern toolbox is specialized for simple data types.
It is specialized for simple sequences, uh, and grids.
A sequence is a, uh,
like text or speech has this linear structure and there
has- there are been amazing tools developed to analyze this type of structure.
Images can all be resized and have this spatial locality so- so
they can be represented as fixed size grids or fixed size standards.
And again, deep learning methodology has been very good at processing this type of,
uh, fixed size images.
However, um, graphs, networks are much harder to process because they are more complex.
First, they have arbitrary size and arb- and complex topology.
Um, and there is also no spatial locality as in grids or as in text.
In text we know left and right,
in grids we have up and down, uh, left and right.
But in networks, there is no reference point,
there is no notion of,
uh, uh, spatial locality.
The second important thing is there is no reference point,
there is no fixed node ordering that would allow us,
uh, uh, to do, uh,
to do deep learning.
And often, these networks are dynamic and have multi-model features.
So in this course,
we are really going to, uh,
talk about how do we develop neural networks that are much more broadly applicable?
How do we develop neural networks that are applicable to complex data types like graphs?
And really, it is relational data graphs that are the- the new frontier,
uh, of deep learning and representation learning, uh, research.
So intuitively, what we would like to do is we would like to do,
uh, build neural networks,
but on the input we'll take, uh, uh,
our graph and on the output,
they will be able to make predictions.
And, uh, these predictions can be at the level of individual nodes,
can be at the level of pairs of nodes or links,
or it can be something much more complex like a brand new generated graph or, uh,
prediction of a property of a given molecule that can be represented,
um, as a graph on the input.
And the question is,
how do we design this neural network architecture
that will allow us to do this end to end,
meaning there will be no human feature engineering, uh, needed?
So what I mean by that is that, um,
in traditional, uh, machine learning approaches,
a lot of effort goes into des- designing proper features,
proper ways to capture the structure of the data so that machine learning models can,
uh, take advantage of it.
So what we would like to do in this class,
we will talk mostly about representation learning
where these feature engineering step is taken away.
And basically, as soon as we have our graph,
uh, uh, repr- graph data,
we can automatically learn a good representation of the graph so that it can be used for,
um, downstream machine learning algorithm.
So that a presentation learning is about automatically extracting or learning features,
uh, in the graph.
The way we can think of representation learning is to map
nodes of our graph to a d-dimensional embedding,
to the d-dimensional vectors,
such that seeming that are nodes in the network are
embedded close together in the embedding space.
So the goal is to learn this function f that will take
the nodes and map them into these d-dimensional,
um, real valued vectors,
where this vector will call this, uh, representation, uh,
or a feature representation or an embedding of a given node,
an embedding of an entire graph,
an embedding of a given link,
um, and so on.
So a big part of our class we'll be, uh,
investigating and learning about latest presentation learning,
deep learning approaches that can be applied,
uh, to graph, uh, structured data.
And we are going to, uh, uh,
talk about many different topics in
machine learning and the representation learning for graph structure data.
So first, we're going to talk about traditional methods
for machine learning and graphs like graphlets and graph kernels.
We are then going to talk about methods to generate, um,
generic node embeddings, methods like DeepWalk and Node2Vec.
We are going to spend quite a bit of time talking about
graph neural networks and popular graph neural network architectures like graph,
uh, convolutional neural network,
the GraphSage architecture or Graph Attention Network, uh, architecture.
We are also going to study the expressive power of graph neural networks,
um, the theory behind them,
and how do we scale them up to very large graphs.
Um, and then in the second part of this course,
we are also going to talk about heterogeneous graphs,
knowledge graphs, and applications,
uh, to logical reasoning.
We're learning about methods like TransE and BetaE.
We are also going to talk about how do we build deep generative models for
graphs where we can think of the prediction of the model to
be an entire newly generated graph.
And we are also going to discuss applications to biomedicine, um,
various scientific applications, as well
as applications to industry in terms of recommender systems,
fraud detection, and so on.
So here is the outline of this course.
Week by week, 10 weeks, starting, uh,
starting today and all the way to the middle of March,
um, where, uh, the- the course will finish.
We will have 20 lectures and we will cover all the topics,
uh, that I have discussed,
and in particular focus on
graph neural networks and the representation learning in graphs.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 01.2 - Applications of Graph ML.txt
Welcome back to Stanford 244W,
uh, Machine Learning with Graphs.
Um, in this part of the lecture,
I'm going to discuss applications of graph machine learning research and its impact,
uh, across many different, uh, applications.
So in mach- graph machine learning,
we can formulate different types of tasks.
We can formulate tasks at the level of individual nodes.
We can formulate tasks at the level of,
uh, edges, uh, which is pairs of nodes.
We can identify or define tasks at the level of subgraphs of nodes,
as well as the tasks at the level of the entire, um,
graphs like for a graph level prediction or, uh, graph generation.
And what I'm going to talk, uh,
next is go through these different levels of tasks and show you, uh,
different, uh, applications, uh,
and different domains where this type of methods models can be applied.
So for node level tasks,
we generally talk about node classification,
where we are trying to predict a property of a node.
For example, categorize, uh,
online users or categorize items.
In link prediction, we tried to predict whether there
are missing links between a pair of nodes.
One such example of this task is knowledge graph completion.
In, uh, graph level task like graph classification,
we try to categorize different graphs.
Uh, for example, we may want to represent molecules as,
uh, graphs and then predict properties of molecules.
This is especially interesting and important task
for drug design where we try to predict,
uh, properties of different,
uh, molecules, different drugs.
We can also perform clustering or community detection,
where the goal is to identify, um,
closely neat, uh, subparts of the graph, uh,
where nodes are densely connected or highly connected with each other.
Um, and application of these could be social circle detection.
And- and then there are also other types of tasks.
For example, graph generation or graph, um, uh, evolution,
where graph generation could be, for example,
used for drug discovery to generate novel molecular structures.
And graph- and predicting graph evolution is very useful, uh,
in physics where we wanna run accurate simulations of various kinds of physics phenomena,
and that can be represented, um, as a graph.
So in all these machine learning tasks, uh, we use, uh,
we use, uh, graphs, uh,
which leads to high, uh, impact applications.
And now I wanna give you some examples of them.
So first, I'm going to give you some examples of
node level machine learning applications.
So, um, a very recent one announced at the end of December,
uh, this year is,
uh, the following problem.
It's called protein folding,
where basically in our bodies,
we have these molecules called proteins that
regulate various biological processes, and for example,
the way that drugs, uh,
work is to bind or change behavior of
different proteins which then then changes the biological processes in our body,
and this way, uh, for example,
we- we- we get cured or we we heal.
Um, proteins are- are composed,
uh, um, of amino acids.
And we can think of our protein as a sequence of amino acids.
However, due magnetic and different kinds of forces,
these- these proteins are not these, um, uh,
chains or strains, but the- they are actually- they actually fold,
um, in very complex, uh, shapes.
And one of the very important problems in biology, a very, uh, um,
a problem that hasn't yet been solved is given up- a sequence of amino acids,
can you predict the 3D structure of the underlying protein?
So the computational task that, um,
scientists have been running competitions about since, um, uh,
'70s is about how do we computation- computationally predict
protein's 3D structure based solely on its amino acid sequence.
Um, and here I show you a few, um, uh,
the three-dimensional structure of two different proteins,
and what you can see is that- that this folding of
a protein is- is very complex based on its,
uh, amino acid structure.
So the question is,
given a sequence of amino acids,
can we predict the three-dimensional structure,
um, of the protein?
And this is the problem that has been just recently solved.
In the middle of December of, uh, 2020, uh,
DeepMind announced Alpa- AlphaFold that increased the performance, um,
or the accuracy of this,
uh, protein folding, uh,
applications by 30 percent all the way up to the values that are in high 90s.
And here I just show a couple of, um, uh,
titles of articles in media,
uh, about how an important, uh,
achievement this- this has been,
how it changed the biology forever,
how it solved the- one of the largest scientific open problems,
and how this will turbocharge drug discovery and all kinds of,
uh, important, um, implications that this has.
And what is interesting in this, uh, scientific, uh,
AI machine learning breakthrough is that the key idea that
made this possible was to represent the underlying,
uh, protein as a graph.
Uh, and here they represented it as a spatial graph,
where nodes in this graph were amino acids in the protein sequence,
and the edges corresponded to, um,
ami- to nodes- to amino acids that are spatially close to each other.
So this means that now given the positions, um,
of all the amino acids and the edges proximities between them,
the graph neural network, uh,
approach was trained that it predicted the new positions,
uh, of the- of the, um, amino acids.
And this way, uh, the folding of the protein was able to be simulated and the-
and the posi- the final positions of the molecules were able to be, uh, predicted.
So the key ingredient in making this work,
in making this scientific breakthrough in protein folding was
the use of graph representation and the graph neural network, uh, technology.
Now, uh, this was on the level of nodes,
where basically for every node in the graph,
we tried to predict its, um,
uh, position in space,
and this way, uh,
tell what is the three-dimensional organization of a protein.
Now we are going to talk about edge-level machine learning task,
where we are basically doing link prediction or trying to
understand relationship between different nodes.
The first example of this is in recommender systems,
where basically we can think of these as users interacting with items,
items being products, movies,
um, songs, and so on.
And nodes, uh, will be- we'll have two types of nodes.
We will have users, and we would have items.
And there is an edge between a user and an item if a user consumed, bought,
reviewed, uh, a given item or listened to a given song or,
uh, watched a given movie.
And based on the structure of this graph and the properties of the users and the items,
we would like to predict or recommend
what other items given users might be interested in, uh, in the future.
So we naturally have a bipartite graph and, um, a graph problem.
And the modern recommender systems used in companies like, uh,
Pinterest, LinkedIn, uh, Facebook,
uh, Instagram, uh, Alibaba,
um, and elsewhere are all based on these graphical representations
and use graph representation learning and graph neural networks to make predictions.
And the key insight here is that we can basically
learn how to embed or how to represent nodes,
um, of this graph such that related nodes are
embedded closer to each other than nodes that are not related.
And for example, in case of Pinterest,
we can think of, uh,
Pinterest images as nodes in the graph,
and the goal is to embed, um,
nodes that are related- images that are related
closer together than images that are not related.
For example, this, uh,
sweater and the cake.
And the way one can do this is to create this type of bipartite network,
where we have the images on the top, and we can have,
for example, users or Pinterest boards at the bottom.
And then we can define a neural network approach that will take
the feature information or attribute information of these different pins,
so basically the content of the image,
and transform it across the underlying graph to come up with a robust embedding,
uh, of a given, uh, image.
And it turns out that this approach works much,
much better than if you would just consider images by themselves.
So images plus the graph structure leads to
much better recommendations than the image themselves.
So here in this example of the task,
it is about understanding relationships between pairs of nodes or pairs of
images by basically saying that nodes that are
related should be embedded closer together,
the distance between them should be smaller than the distance between,
uh, pairs of images that are not related to each other.
Um, another example of a link level prediction task is very different.
This is about, uh,
drug combination side effects.
Uh, the problem here is that many patients take
multiple drugs simultaneously to trick- to treat complex and coexisting diseases.
For example, in the United States, basically,
fif- 50 percent of people over 70 years of age simultaneously take four or,
uh, five or more drugs.
And there are many patients who take
20- 20 plus drugs to treat many complex coexisting diseases.
For example, somebody who suffers,
uh, insomnia, suffers depression,
and has a heart disease,
all simultaneously will- will take many different drugs,
uh, altogether at once.
And the problem is that these drugs, uh,
interact with each other, um,
and they lead to new adverse side effects.
So basically, the interactions between drugs leads to additional, uh,
diseases, um, uh, or additional problems,
uh, in that human.
Uh, and of course, the number of combinations of different drugs is too big,
so we cannot experimentally or in clinical trials test
every combination of drugs to see what kind of side effects does it lead to.
So the question is, can we build up predictive engine that for
an arbitrary pair of drugs will predict how these drugs are going to interact,
and what kind of adverse side effects,
uh, they may cause?
And this is also a graph problem.
So let me tell you how we formulate it.
Um, we create this, um,
two-level heterogeneous network where triangles are the, uh, uh,
different drugs and, um,
circles are proteins in our bodies.
And then the way drugs work is that they target the different proteins.
So these are the edges between triangles and the circles.
And, um, biologists have been mapping out
the protein-protein interaction network where they
experimentally test whether two proteins physically come
together and interact to regulate a given biological process or function.
So we also know, experimentally,
which proteins interact with each other.
And this is called a protein-protein interaction network,
or also called the inter-rectum.
And then the last set of links we have in
this graph are the known side-effects where basically, for example,
the link between the node C and node M
says that if you take these two drus- drugs together,
the side-effect of type R is knowing- known to occur.
Of course, this network up here of side-effects is notoriously incomplete and,
uh, has a lot of missing connections.
So the question becomes, can we impute,
can we predict the missing edges,
missing connections, um, in this, uh,
network that would basically tell, us um,
how lay- what kind of side-effects can we expect if we take,
uh, or if a person takes two drugs simultaneously?
So the way we think of this,
we think of it as a link prediction between triangular nodes of g- um,
in the graph, where basically the question is, given, uh,
the two drugs, what kind of side effects, uh, may occur?
And what is interesting is that you can apply this method, um,
very accurately and you can discover
new side effects that haven't been known, uh, in the past.
For example, in this, uh,
in this case, um,
the mo- the model, uh,
outputted the top ten predictions it is most, uh, certain about,
where basically the way you read it is to say if you think these two drugs,
then this particular side effect is likely to occur.
And, uh, none of these side-effects are actually in the da- in the official FDA database.
So what the authors did here is they took
the top 10 predictions from the model and then they
looked in the medical literature and
clinical medical notes to see if there- are there any,
um, any reports that could,
uh, tell us whether, uh,
and provide evidence of whether this particular uh,
pair of drugs could lead to a given side-effect.
Then actually, for the five out of top 10,
we actually, um, found, uh,
that there is some research evidence that points that this,
um, that this predictions,
um, might actually be true.
So these were the machine learning tasks at the level of pairs of nodes.
So we talked about recommender systems and I talked about the side effect prediction.
Now, I wanna talk about the sub-graph level machine learning task.
Um, and here is one, um,
very recent that we are all using every day.
It's about traffic prediction.
So for example, if today you open
Google Maps and you say I wanna drive- drive from Stanford, uh,
all the way up to Berkeley, uh,
Google will tell you how long it will take you to get
there and what is your estimated time of arrival.
And I'm not sure you knew,
but actually, uh, in the end,
graph machine learning is used to make these predictions of the travel time,
and the way the graph is created is that nodes represent a road segments and,
uh, connectivity between road segments,
um, is captured by the edges of this network.
And then, um, our graph neural network approach is-
is trained that based on the conditions, uh, uh,
and traffic patterns on each of the road segment, um,
as well as the path between the source and the destination, um,
uh, of the- of the journey, uh,
the graph neural network approach is trained to predict the estimate that,
uh, time of arrival or, uh, travel time.
Um, and this- and it has been announced that actually this, um,
graph-based approach is used in production in Google Maps, so whenever, uh,
you are asking for directions,
there is actually a graph machine learning-based approach
that tells you when are you going to arrive,
uh, to a given location.
And last, I wanna talk about graph-level machine learning tasks,
uh, and some interesting impactful applications of graph-level tasks.
Um, one very recent is around drug discovery.
And actually, graph- graph-based machine learning was
used to discover new drugs, new antibiotics, right?
Antibiotics are small molecular graphs and we can represent molecules
as graphs where the nodes are atoms and edges correspond to chemical bonds.
So each molecule can be represented as a graph.
But then we ca- we have these banks,
uh, or collections of billions of molecules.
And the question is,
which molecules could have, uh, therapeutic effect.
So essentially, which molecules should be prioritized so that
biologists can pass them in the laboratory to validate or,
um, their ther- therapeutic effect.
And actually, a team at MIT was using, um, graph, uh,
based deep learning approach for
antibiotic discovery where they used a graph neural network, uh,
to classify different molecules and predict promising molecules from a pool of,
uh, billions of candidates.
And then these predictions would have further,
uh, validated, uh, in the lab.
And there is a very exciting, um,
breakthrough paper published in,
uh, journal cell, uh,
just this year about how
these graph-based approach allows us to efficiently and quickly discover,
uh, new drugs and new therapeutic uses of different,
uh, types of molecules.
Um, to further talk about drug discovery, uh,
we can think also about graph generation as a way to
discover new molecules that have never been synthesized or considered, uh, before.
And this is very useful because it allows us to generate new structures,
new molecules in various kinds of targeted ways.
For example, we can say generate new molecules that are non-toxic,
generate new molecules that have high solubility,
generate new molecules that have high drug likeness.
So we can generate now molecules as graphs in a targeted way. Not even that.
The second use case is that we can optimize
existing molecules to have a desirable property.
So basically, the use case here is that you have a small part of the molecule that has,
uh, a given therapeutic effect, for example.
And now we wanna complete, uh,
the rest of the molecule scaffold so that you improve,
um, a given property.
For example- for example, uh,
solubility and this type of deep graph, uh,
generative models, uh, can be used for tasks,
uh, like uh, molecule generation and optimization.
So, um, and the last graph-level task that I
want to talk about is a realistic, uh, physics-based simulation.
In this case, we can basically have different materials.
We represent the material as a set of particles and then
we can have a graph defined on top of,
uh, these, um, set of particles that capture which particles interact with each other.
And now the underlying task for the machine learning is to say,
predict how this graph is going to evolve in the future.
And this allows us to predict how this material is going to deform.
Um, so let me tell you how this is done.
The way this is done is that essentially we iterate,
um, the following approach.
We take the material and we represent it as a set of particles.
Based on the proximities,
interactions between the particles,
we generated the proximity graph.
Now, that we have this, uh, proximity graph,
we apply graph machine learning,
a graph neural network,
that takes the current properties, meaning positions,
as well as velocities of the particles and predict what will be the,
uh, positions and velocities of the particles in the future.
And now based on this prediction, we can move,
evolve the particles to their new positions, and then again,
we go to the first step where now based on this new proximities,
we create the new graph,
predict the new positions,
um, move the particles and keep iterating this.
And this allows for very fast and very accurate physics-based simulations.
So these were some examples of graph,
uh, of a graph-level tasks and, uh,
important applications of graph machine learning to various domains, um,
across, uh, across, uh,
sciences, industry, as well as different consumer products.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 01.3 - Choice of Graph Representation.txt
In this part of, uh, Stanford, CS 224 W, um,
machine-learning with graphs course,
I wanna talk about the choice of graph representation.
[NOISE] So what are components of a graph or a network?
So network is composed of two types of objects.
We- first, we have objects or entities themselves called,
uh, referred to as nodes, uh,
and vertices, and then we have interactions or edges between them,
uh, called links or, uh, edges.
And then the entire system, the entire, um,
domain we then call a- a network,
uh, or a graph.
Usually, for nodes, we will use, uh, uh,
the word- the letter capital N or capital V,
um, and then for edges,
we- we are usually using the-
the letter capital E so that the graph G is then composed of a set of nodes,
uh, N and a set of edges, uh,
E. What is important about graphs is that graphs are a common language,
meaning that I can take, for example, uh,
actors and connect them based on which movies they appeared in,
or I can take people based on the relationships they have with each other,
or I can take molecules, like proteins,
and build a network based on which proteins interact with each other.
If I look at what is the structure of this network,
what is the underlying mathematical representation,
in all these cases,
we have the same underlying mathematical representation,
which means that the same machine learning algorithm will be
able to make predictions be it that these nodes,
um, uh, correspond to actors, correspond to, uh,
people, or they correspond to molecules like proteins.
[NOISE] Of course, choosing a proper graph representation is very important.
So for example, if you have a set of people,
we can connect individuals that work with each
other and we will have a professional network.
However, we can also take the same set of individuals
and connect them based on sexual relationships, but then,
we'll ab- creating a sexual network, or for example,
if we have a set of scientific papers,
we can connect them based on citations,
which paper cites which other paper.
But for example, if we were to connect them based
on whether they use the same word in the title,
the- the quality of underlying network and the underlying,
uh, representations might be, uh, much worse.
So the choice of what the- the nodes are and what the links are is very important.
So whenever we are given a data set,
then we need to decide how are we going to design the underlying graph,
what will be the objects of interest nodes,
and what will be the relationships between them,
what will be the edges.
The choice of this proper network representation of a given domain or a given problem
deter- will determine our ability to use networks, uh, successfully.
In some cases, there will be a unique,
unambiguous way to represent this, um, problem,
this domain as a graph,
while in other cases,
this representation may, by no means, be unique.
Um, and the way we will assign links between the objects will determine, uh,
the nature of the questions we will be able to study and the nature of the,
um, predictions we will be able to make.
So to show you some examples of design choices we are faced with when co-creating graphs,
I will now go through some concepts and different types of graphs,
uh, that we can- that we can create from data.
First, I wi- I will distinguish between directed and undirected graphs, right?
Undirected graphs have links,
um, that- that are undirected, meaning,
that they are useful for modeling symmetric or reciprocal relationships,
like collaboration, friendship, um,
and interaction between proteins,
and so on, while directed, um,
relationships are captured by directed links,
where every link has a direction, has a source,
and has a destination denoted by a- by an arrow.
And examples of these types of, um,
links occurring in real-world would be phone calls,
financial transactions, uh, following on Twitter,
where there is a source and there is a destination.
The second type of, um- um, uh,
graphs that we are going to then talk about is that as we have, um,
created undirected graphs, then,
um, we can talk about the notion of a node degree.
And node degree is simply the number of edges,
um, adjacent to a given, uh, node.
So for example, the node a in this example has degree 4.
The average node degree is simply the- is
simply the average over the degrees of all the nodes in the network.
And if- if you work this out,
it turns out to be twice number of edges divided by the number of nodes,
uh, in the network.
The reason there is this number 2 is
because when we are computing the degrees of the nodes,
each edge gets counted twice, right?
Each endpoint of the n- of the edge gets
counted once because the edge has two end points,
every edge gets counted twice.
This also means that having a self edge or self-loop, um,
adds a degree of two to the node,
not a degree of one to the node because both end points attach to the same, uh, node.
This is for undirected networks.
In directed networks, we distinguish between, uh,
in-degree and out-degree, meaning
in-degree is the number of edges pointing towards the node.
For example, node C has in-degree 2 and the out-degree, um, 1,
which is the number of edges pointing outside- outward
from the- from the node, uh, c. Um,
another, uh, very popular type of graph structure
that is- that is used a lot and it's very natural in different domains,
it's called a bipartite graph.
And bipartite graph is a graph generally of nodes of two different types,
where nodes only interact with the other type of node,
but not with each other.
So for example, a bipartite graph is a graph where nodes can be split
into two partitions and the- the edges only go from left,
uh, to the right partition and not inside the same partition.
Examples of, uh, bipartite graphs that naturally occur are,
for example, uh, scientific authors linked to the papers they authored,
actors linked to the movies they appeared in,
users linked to the movies they rated or watched,
um, and so on.
So- or for example,
customers buying products, uh,
is also a bipartite graph where we have a set of customers,
a set of products,
and we link, uh, customer to the product, uh, she purchased.
Now that we have defined a bipartite network,
we can also define the notion of a folded or projected network, where we can create,
for example, author collaboration networks,
or the movie co-rating network.
And the idea is as follows: if I have a bipartite graph,
then I can project this bipartite graph to either to the left side or to the right side.
And when- and when I project it, basically,
I only use the nodes from one side in my projection graph,
and the way I connect the nodes is to say,
I will create a connection between a pair of nodes
if they have at least one neighbor in common.
So if these are authors and these are scientific papers,
then basically, it says,
I will create a co- collaboration or a co-authorship graph where I will
connect a pair of authors if they co-authored at least one paper in common.
So for example, 1, 2,
and 3 co-authored this paper,
so they are all connected with each other.
For example, 3 and 4 did not co-author a paper,
so there is no link between them.
But for example, 5 and 2 co-authored a paper,
so there is a link between them because they co-authored this,
uh, this paper here.
And in analogous way,
you can also create a projection of
this bipartite network to the- to the right-hand side,
and then you will- you would obtain a graph like this.
And as I said, bipartite graphs or multipartite graphs,
if you have multiple types of edges,
are very popular, especially,
if you have two different types of nodes,
like users and products, um,
uh, users and movies, uh,
authors and papers, um,
and so on and so forth.
[NOISE] Another interesting point about graphs is how do we represent them,
um, and representing graphs,
uh, is an interesting question.
One way to represent a graph is to represent it with an adjacency matrix.
So essentially, if for a given,
uh, undirected, for example, graph,
in this case on end nodes, in our case,
4, we will create a square matrix,
where this matrix will be binary.
It will o- only take entries of 0 and 1.
And essentially, an entry of matrix ij will be set to 1 if nodes i and j are connected,
and it will be set to 0 if they are not connected.
So for example, 1 and 2 are connected,
so at entry 1, row 1,
column 2, there is a 1.
And also, because 2 is connected to 1 at row 2,
column 1, we also have a 1.
So this means that adjacency matrices of,
uh, undirected graphs are naturally symmetric.
If the graph is directed,
then the matrix won't be symmetric because 2 links to 1.
We have a 1 here,
but 1 does not link back to 2,
so there is a 0.
Um, and in similar way,
we can then think of node degrees, um, uh,
simply as a summation across a given row or
across a given one column of the graph, uh, adjacency matrix.
So rather than kind of thinking here how many edges are adjacent,
we can just go and sum the- basically,
count the number of ones,
number of other nodes that this given node is connected to.
Um, this is for, um, undirected graphs.
For directed graphs, uh,
in and out degrees will be sums over columns and sums over rows, uh,
of the graph adjacency matrix,
as- as I illustrate here, uh,
with this, um, illustration.
One important consequence of a real-world network is that they are extremely sparse.
So this means if you would look at the adjacency matrix,
series on adjacency matrix of a real-world network where basically for every, um, row I,
column J, if there is an edge,
we put a dot and otherwise the cell is empty, uh,
you get these types of super sparse matrices where,
where there are large parts of the matrix that are empty, that are white.
Um, and this has important consequences for properties
of these matrices because they are extremely, uh, sparse.
To show you an example, right?
Uh, if you have a network on n nodes,
nodes, then the maximum degree of a node,
the number of connections a node has is n minus one
because you can connect to every oth- in principle,
connect to every other node in the network.
So for example, if you are a human and you think about human social network, uh,
the maximum degree that you could have,
the maximum number of friends you could have is every other human in the world.
However, nobody has seven billion friends, right?
Our number of friendships is much, much smaller.
So this means that, let's say the human social network is extremely sparse,
and it turns out that a lot of other,
uh, different types of networks,
you know, power-grids, uh, Internet connection,
science collaborations, email graphs,
uh, and so on and so forth are extremely sparse.
They have average degree that these, you know,
around 10 maybe up to, up to 100.
So, uh, what is the consequence?
The consequence is that the underlying adjacency matrices,
um, are extremely sparse.
So we would never represent the matrix as a dense matrix,
but we've always represent it as a sparse matrix.
There are two other ways to represent graphs.
One is simply to represent it as a edge list,
simply as a list of edges.
Uh, this is a representation that is quite popular, um,
in deep learning frameworks because we can simply
represent it as a two-dimensional matrix.
The problem of this representation is that it is very
hard to do any kind of graph manipulation or
any kind of analysis of the graph because even
computing a degree of a given node is non-trivial,
uh, in this case.
A much, uh, better, uh,
representation for a graph analysis and manipulation is the notion of adjacency list.
Um, and adjacency lists are good because they are easier to
work with if for large and sparse networks.
And adjacency list simply allows us to
quickly retrieve al- all the neighbors of a given node.
So you can think of it, that for every node,
you simply store a list of its neighbors.
So a list of nodes that the,
that the- a given node is connected to.
If the graph is undirected,
you could store, uh, neighbors.
If the graph is connected,
you could store both the outgoing neighbors,
as well as, uh, incoming neighbors based on the direction of the edge.
And the last important thing I want to mention here is that of course,
these graph can- can have attached attributes to them.
So nodes address, as well as
entire graphs can have attributes or properties attached to them.
So for example, an edge can have a weight.
How strong is the relationship?
Perhaps it can have my ranking.
It can have a type.
It can have a sign whether this is a friend-based relationship or whether it's animosity,
a full distrust, let say based relationships.
Um, and edges can have di- many different types of properties,
like if it's a phone call, it's,
it's duration, for example.
Nodes can have properties in- if these are people,
it could be age, gender,
interests, location, and so on.
If a node is a, is a chemical,
perhaps it is chemical mass,
chemical formula and other properties of the- of
the chemical could be represented as attributes of the node.
And of course, also entire graphs can have features or, uh,
attributes based on, uh,
the properties of the underlying object that the graphical structure is modeling.
So what this means is that the graphs you will be considering
are not just the topology nodes and edges,
but it is also the attributes,
uh, attached to them.
Um, as I mentioned,
some of these properties can actually be
represented directly in the adjacency matrix as well.
So for example, properties of edges like
weights can simply be represented in the adjacency matrix, right?
Rather than having adjacency matrix to be binary,
we can now have adjacency matrix to have real values where
the strength of the connection corresponds simply to the value,
uh, in that entry.
So two and four are more strongly linked,
so the value is four,
while for example, one and three are linked with
a weak connection that has weight only 0.5.
Um, as a- um,
another important thing is that when we create the graphs is that we also
can think about nodes having self-loops.
Um, for example, here,
node four has a self-loop, uh,
and now the degree of node four equals to three.
Um, self-loops are simply correspond to
the entries on the diagonal of the adjacency matrix.
And in some cases,
we may actually create a multi-graph where we
allow multiple edges between a pair of nodes.
Sometimes we can, we can think of a multi-graph as
a weighted graph where the entry on the matrix counts the number of edges,
but sometimes you want to represent every edge individually,
separately because these edges might have different properties,
um, and different, um, attributes.
Both, um, the self-loops,
as well as multi-graphs occur quite frequently in nature.
Uh, for example, if you think about phonecalls transactions,
there can be multiple transactions between a pair of nodes
and we can accurately represent this as a multi-graph.
Um, as we have these graphs, I,
I also want to talk about the notion of connectivity,
in a sense, whether the graph is connected or disconnected.
And graph is connected if any pair of nodes in, uh, in this, uh,
graph can be, can be connected via a path along the edges of the graph.
So for example, this particular graph is
connected while this other graph is not connected,
it has three connected components.
This is one connected component, second connected component,
then a third connected component,
the node h, which is an isolated node.
This is the notion of connectivity for undirected graphs, uh,
and what is interesting in this notion is,
that when we, um,
have graphs that are,
for example, disconnect it and look at what is
the structure of the underlying adjacency matrix,
we will have these block diagonal structure, where, basically,
if this is a graph that is composed of two components, then we will have,
um, um, block diagonal structure where the edges only go between the,
um, nodes inside the same, um, connected component,
and there is no edges in the off-diagonal part,
which would mean that there is no edge between,
uh, red and blue,
uh, part of the graph.
The notion of connectivity also generalizes to directed graphs.
Here, we are talking about two types of connectivity,
strong and weak connectivity.
A weakly connected directed graph is simply a graph that is connected,
uh, in- if we ignore the directions of the edges.
A strongly connected graph, um,
or a graph is strongly connected if for every pair of
nodes there exists a directed path between them.
So, um, this means that there has to exist a directed path from, for example,
from node A to node B,
as well as from node B back to, uh,
node A if the graph is strongly connected.
What this also means is that we can talk about notion of
strongly connected components where strongly connected components are,
uh, sets of nodes in the graph, uh,
such that every node, uh,
in that set can visit each other via the- via a directed path.
So for example, in this case here,
nodes, uh, A, B,
and C form a strongly connected component because they are on a cycle.
So we ca- any- from any node we can visit, uh, any other node.
Uh, the example here shows, uh,
directed graph with two strongly connected component,
again, two cycles on, um three nodes.
So this concludes the discussion of the- er- the graph representations,
um, that- and ways how we can create graphs from real data.
Um, in this lecture,
we first talked about machine-learning with graphs and various applications in use cases.
We talked about node level, edge level,
and graph level machine-learning prediction tasks.
And then we discussed the choice of a graph representation in terms of directed,
undirected graphs, bipartite graphs,
weighted, uh, unweighted graphs,
adjacency matrices, as well as some definitions from graph theory,
like the connectivity, um, of graphs,
weak connectivity, strong connectivity,
as well as the notion of node degree.
Um, thank you very much.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 02.1 - Traditional Feature-based Methods Node.txt
Welcome to the class. Today we are going to talk about traditional methods for,
uh, machine learning in graphs.
Um, and in particular,
what we are going to investigate is, uh,
different levels of tasks,
uh, that we can have in the graph.
In particular, we can think about the node-level prediction tasks.
We can think about the link-level or edge-level prediction tasks
that consider pairs of nodes and tries to predict whether the pair is connected or not.
And we can think about graph-level prediction,
where we wanna make a prediction for an entire graph.
For example, for an entire uh,
molecule or for a- for an entire uh, piece of code.
The traditional machine learning pipeline um, eh,
is all about designing proper uh, features.
And here we are going to think of two types of features.
We are going to assume that nodes already
have some types of attributes associated with them.
So this would mean, for example, if this is a protein,
protein interaction network uh,
proteins have different um,
chemical structure, have different chemical properties and we can think of these
as attributes attached to the nodes uh, of the network.
At the same time, what we also wanna do is we wanna be able to
create additional features that will describe how um,
this particular node is uh,
positioned in the rest of the network,
and what is its local network structure.
And these additional features that describe the topology of the network of the graph uh,
will allow us to make more accurate predictions.
So this means that we will always be thinking about two types of uh, features,
structural features, as well as features
describing the attributes and properties uh, of the nodes.
So the goal in uh,
in what we wanna do today is especially focused
on structural features that will describe um,
the structure of a link in the broader surrounding of the network,
that will describe the structure of the um,
network neighborhood around a given node of interest,
as well as features that are going to describe the structure of the entire uh,
graph so that we can then feed these features
into machine learning models uh, to make predictions.
Traditionally um, in traditional machine learning pipelines, we have two steps.
In the first step, we are going to take our data points, nodes, links,
entire graphs um, represent them um,
with vectors of features.
And then on top of that, we are going then to train a classical machine learning uh,
a classifier or a model, for example,
a random forest, perhaps a support vector machine uh,
a feed-forward neural network um,
something of that sort.
So that then in the future,
we are able to apply this model where a new node,
link, or graph uh, appears.
Uh, we can obtain its features um,
and make a prediction.
So this is the setting uh,
generally in which we are going to um, operate today.
So in this lecture,
we are going to focus as I said,
on feature design, where we are going to use effective features uh,
over the graphs, which will be the key to obtain good predictive performance,
because you wanna capture the structure,
the relational structure of the network.
Um, and traditional machine learning pipelines use hand-designed, handcrafted features.
And today's lecture will be all about these handcrafted features.
And we are going to split the lecture into three parts.
First, we are going to talk about features that describe
individual nodes and we can use them for node-level prediction,
then we are going to move and talk about features that can des- describe a pair of nodes.
And you can think of these as features for link-level prediction,
and then we're also going to talk about features and
approaches that describe topology structure of
entire graphs so that different graphs can be compared um, and uh, classified.
And for simplicity, we are going to- to- today focus on uh, undirected graphs.
So the goal will be,
how do we make predictions for a set of objects of interest where
the design choice will be that our feature vector will be a d-dimensional vector?
Uh, objects that we are interested in will be nodes,
edges, sets of nodes uh,
meaning entire graphs um,
and the objective function we'll be thinking about is,
what are the labels uh,
we are trying to predict?
So the way we can think of this is that we are given a graph as a set of vertices,
as a set of edges,
and we wanna learn a function that for example,
for every node will give- will give us a real valued prediction um, which for example,
would be useful if we're trying to predict uh,
edge of uh, every node in our, uh social network.
And the question is, how can we learn this function
f that is going to make uh, these uh, predictions?
So first, we are going to talk about
node-level tasks and features that describe individual nodes.
The way we are thinking of this is that we are thinking of
this in what is known as semi-supervised uh,
case, where we are given a network,
we are given a couple of nodes that are labeled with different colors, for example,
in this case, and the goal is to predict the colors of un- uh, un-uncolored uh, nodes.
And if you look at this example here,
so given the red and green nodes,
we wanna color uh, the gray nodes.
And you know, if you stir a bit at this,
the rule here is that um,
green nodes should have at least two a- edges adjacent to them,
while red nodes have at least one edge uh,
have e- exactly one edge at um, uh, connected to them.
So if we are now able to describe um,
the node degree of every node um,
as a- as a structural feature uh, in this graph,
then we will be able to learn the model that in this uh,
simple case, correctly colors uh,
the nodes uh, of the graph.
So we need features that will describe this particular topological uh, pattern.
So the goal is to characterize the structure o- um,
of the network around a given node,
as well as in some sense,
the position, the location of the node in the broader network context.
And we are going to talk about four different uh,
approaches that allow us to do this.
First, we can always use the degree of the node as a characterization of um,
uh, structure of the network around the node,
then we can think about the importance,
the position of the node through the notion of uh,
node centrality measures, and we are going to review a few.
Then we will talk about um,
characterizing the local network structure.
Not only how many uh, uh,
edges a given node has,
but also what is the structure of the node around it?
First, we are going to talk about clustering coefficient,
and then we are going to generalize this to the concept known uh, graphlets.
So first, let's talk about the node degree.
We have already introduced it.
There is nothing special,
but it is a very useful feature,
and many times it is um,
quite important where basically we will say the-
we capture the- s- the structure of the node uh,
um, v in the network with the number of edges that this node has.
Um, and uh, you know,
the drawback of this is that it treats uh,
all the neighbors equally.
But in- and in the sense, for example,
nodes with the same degree are indistinguishable even though if they may be in uh,
different parts of the network.
So for example, you know,
the node C and the node E have the same degree,
so our classifier uh,
won't be able to distinguish them or perhaps node A, uh,
H, E, uh, uh, F and G also have all degree one.
So they will all have the same feature value,
so the- our simple uh, machine learning model would uh,
that would only use the node degree as a feature would we only-
we'd- would be able to predict the same value uh,
or would be forced to predict the same value for all these uh,
different nodes because they have the same degree.
So um, to generalize bi- a bit this very simple notion,
we can then start thinking about uh, you know,
node degree only counts neighbors of the node and uh,
without capturing, let's say their importance or who they really are.
So the node centrality measures try to uh,
capture or characterize this notion of how important is the node in- in the graph?
And there are many different ways how we can capture um,
or model this notion of importance.
I'm quickly going to introduce um, Eigenvector Centrality um,
which we are going to further uh,
work on and extend to the uh,
seminal PageRank algorithm uh, later uh.
In the- in the course,
I'm going to talk about between the centrality that will tell us how- uh,
how important connector a given node is,
as well as closeness centrality that we'll try to capture
how close to the center of the network a given node is.
Um, and of course, there are many other, um,
measures of, uh, centrality or importance.
So first, let's define what is an eigenvector centrality.
We say that node v is as important if it
is su- surrounded by important neighboring nodes u.
So the idea then is that we say that the importance of a given node v is simply, um,
normalized divided by 1 over Lambda,
sum over the importances of its neighbors,
uh, uh, in the network.
So the idea is, the more important my friends are,
the higher my own importance, uh, is.
And if you-if you look at this,
um, and you've write it down,
you can write this in terms of, uh,
a simple, uh, uh,
matrix equation where basically, Lambda is this,
uh, uh, positive constant like a normalizing factor,
c is the vector of our centrality measures,
A is now, uh, graph adjacency matrix,
and c is again that vector of centrality measures.
And if you write this in this type of, uh, forum,
then you see that this is a simple, um,
eigenvector, eigenvalue, uh, equation.
So what this means is that, um,
the solution to this, uh, uh,
problem, uh, here- to this equation here, um,
is, um, um, this is solved by the, uh,
la- uh, by the given eigenvalue and the associated eigenvector.
And what people take as, uh, uh,
measure of nodes centrality is- is
the eigenvector that is associated with the largest, um, eigenvalue.
So in this case, if eigen- largest eigenvalue is Lambda max, um, be- uh,
by PerronFrobenius theorem, uh,
because we are thinking of the graph is undirected,
it is always positive, uh, and unique.
Then, uh, the associated leading eigenvector c_max
is usually used as a centrality score for the nodes in the network.
And again, um, uh,
the intuition is that the importance of a node is the sum,
the normalized sum of the importances of the nodes that it links to.
So it is not about how many connections you have but it is, uh,
how, um, who these connections point to and how important,
uh, are those people.
So this is the notion of, uh,
nodes centrality captured by the eigenvector, uh, centrality.
A different type of centrality that has
a very different intuition and captures a different aspect of the,
uh, nodes position in the network is- is what is called betweenness centrality.
And betweenness centrality says that node is important if it lies on many shortest paths,
um, between, uh, other pairs of nodes.
So the idea is if a node is an important connector, an important bridge,
an important kind of transit hub,
then it has a high importance.
Um, the way we compute, um,
betweenness centrality is to say betweenness centrality of
a given node v is a summation over pairs of nodes, uh,
s and t. And we count how many shortest paths between s and t,
uh, pass through the node, uh, v,
and we normalize that by the total number of shortest paths,
um, of the same length between, uh,
s and t. So essentially, um,
the more shorter paths a given node,
uh, appears on, uh,
the more important it is.
So it means that this kind of measures how good a connector or how good of a transit,
uh, point a given node is.
So if we look at this example that- that we have here, for example,
between a centrality of these,
uh, nodes that are,
uh, on the- uh, uh, on the edge of the network like A, B,
and E is zero,
but the- the betweenness centrality of node,
uh, uh, C equals to three,
because the shortest path from A to B pass through C,
a shortest path from, uh,
A to D, uh, passes through C,
and a shortest path between,
uh, A and E, again,
passes through C. So these are the three shortest paths that pass through the node C,
so it's between a centrality equals to three.
By a similar argument,
the betweenness centrality of the node D will be the same, equals to three.
Here are the corresponding shortest paths between
different pairs of nodes that actually pass through, uh,
this node D. Now that we talked about how important transit hub a given, uh,
node is captured by the betweenness centrality,
the third type of centrality that,
again, captures a different aspect of
the position of the node is called closeness centrality.
And this notion of centrality importance says that a node is important if it
has small shortest path- path lengths to oth- all other nodes in the network.
So essentially, the more center you are,
the shorter the path to everyone else,
um, uh, is, the more important you are.
The way, um, uh, we operationalize this is to
say that the closeness centrality of a given node v is one
over the sum of the shortest path length
between the node of interest three and any other node,
uh, u, uh, in the network.
So, uh, to give an example, right?
Uh, the idea here is that the- the-
the smaller this- the- the more in the center you are,
the smaller the summation will be,
so one over a smaller number will be a big number.
And if somebody is on the, let's say,
very far on the edge of the network and needs a lot of, uh, uh, uh,
long paths to reach other nodes in the network,
then its betweenness centrality will be low
because the sum of the shortest path lengths, uh, will be high.
Um, in this case,
for example, you know, the- uh,
the closest centrality of node a equals 1/8 because,
um, it has, uh,
to node B, it has the shortest path of length two, to node C,
it has a shortest path of length one, to D,
it is two, and to E is three, so it's 1/8.
While for example, the node D that is a bit more in
the center of the network has a length two,
shortest path to node A and length one,
shortest paths to all other nodes in the network.
So this is one over five,
so it's betweenness centrali- oh,
sorry, its clo- node centrality,
closeness centrality, uh, is higher.
And then now, I'm going to shift gears a bit.
I was talking about centralities in terms of how- how important,
uh, what is the position,
uh, of the node in the network.
Now, we are going to start talk- we are going back to think about the node degree and uh,
the local structure, uh, around the node.
And when I say local structure,
it me- really means that for a given node,
we only look, uh, in its immediate vicinity, uh,
and decide, uh, on the, uh,
pro- on the- and characterize the properties of the network around it.
And, uh, classical measure of this is called clustering coefficient.
And the clustering coefficients measures how connected one's neighbors are.
So how connected the friends of node v are.
And the way we, uh, define this is to say,
clustering coefficient of node, uh, v,
is the number of edges among the,
uh, neighboring nodes of, uh,
v divided by the degree of v, uh, choose 2.
So this is the number- this is, um,
k choose 2 measures how many pairs can you select out of,
uh, uh, k different objects.
So this is saying, how many- how many node pairs there exists in your neighborhood?
So how many potential edges are there in the net- in- in your neighborhood?
Um, and, um, uh,
D says, how many edges actually occur?
So this metric is naturally between zero and one
where zero would mean that none of your friends,
not on- none of your connections know each other, and,
uh, one would mean that all your friends are also friends with each other.
So, uh, here's an example.
Imagine this simple graph on five nodes and we have our,
uh, red node, uh, v,
to be the, uh, node of interest.
So for example, in this case,
node v has clustering coefficient, um, of one, uh,
because all of its, uh,
four friends are also,
uh, connected, uh, with each other.
So here, um, the clustering is one.
In this particular case, for example,
the clustering is, uh, uh, 0.5.
The reason being that, um,
out of, um, six, uh,
possible connections between the, uh,
four neighboring nodes of node v,
there are only, uh,
three of them are connected.
While here in the last example,
the clustering is zero because, um, out of,
uh, all four, uh, neighbors of, uh,
node v, none of them are connected, uh, with each other.
The observation that is interesting that then leads us to generalize, uh,
this notion to the not- of clustering coefficient to the notion of graphlets,
the observation is that clustering coefficient basically
counts the number of triangles in the ego-network of the node.
So let me explain what I mean by that.
First, ego-network of a given node is simply
a network that is induced by the no- node itself and its neighbors.
So it's basically degree 1 neighborhood network around a given node.
And then what I mean by counts triangles?
I mean that now, if I have this ego-network of a node,
I can count how many triples of nodes are connected.
And in this particular, uh,
use case where, um, um,
uh, the clustering coefficient of this node is 0.5,
it means that I find three triangles, um,
in the network, uh, neighborhood, in the,
um, ego-network of my node of interest.
So this means that clustering coefficient is really counting these triangles,
which in social networks are very important because in social networks,
a friend of my friend is also a friend with me.
So social networks naturally evolve by triangle closing where basically,
the intuition is if somebody has two friends in common, then more or la- uh,
sooner or later these two friends will be
introduced by this node v and there will be a link, uh, forming-
Here, so social networks tend to have a lot of triangles syndrome and,
uh, clustering coefficient is a very important metric.
So now with this, the question is,
could we generalize this notion of triangle accounting, uh,
to more interesting structures and- and count, uh,
the number of pre-specified graphs in the neighborhood of a given node.
And this is exactly what the concept of graphlet, uh, captures.
So the last, um, uh,
way to characterize the structure of the net, uh,
of the network around the given node will be through this concept of
graphlets that render them just to- to- to count triangles.
Also, counts other types of structures, uh, around the node.
So let me define that.
So graphlet is a rooted connected non isomorphic, um, subgraph.
So what do I mean by this?
For example, here are all,
um, possible graphlets, um,
that- that have, um,
uh, um, different number,
uh, of nodes which start with,
uh, a graphlet on two nodes.
So it's basically nodes connected by an edge.
There are, uh, three possible graphlets on three nodes and,
uh, there is, uh,
and then here are the graphlets of 4-nodes.
And these are the graphlets on, uh, five nodes.
So now let me explain what we are looking at.
So for example, if you look at the graphlets on three nodes, it is either,
uh, a chain on three,
um, nodes, audits or triangle, right?
Its all three nodes are connected.
These are all possible connected graphs on three nodes.
Now why do I say there are, uh,
uh, three graphlets not two?
Uh, the reason for that,
is that the position of the node of interest also matters.
So here, for example,
you can be at this position.
And then the question is in how many graphs like this do you participate in.
Or you can be at this other, uh,
position here- position number two,
and it's basically saying,
how many pairs of friends do you have?
And then this- this,
in the case of a triangle,
all this position are isomorphic,
they're equivalent, so there is,
uh, only one of them,
so this is the, uh, position in which you may- you can be.
Now, similarly, if you look at now at, uh,
four node graphlets, there is,
uh, um, many more of them, right?
Um, uh, they- they look the following, right?
You again have a chain on four nodes and you have two positions on the chain.
You are either in the edge or you are one- one away
from the edge- from- if you go from the other end, it is just symmetric.
Here in the second,
this kind of star graphlet you can either be, um,
the satellite on the edge of the star or you can be
the center on- of the star in a- in a square.
All the positions are isomorphic,
so you can be just part of the square.
Here's another interesting, um,
example where, um, you test three different positions.
You can be- you can be here- you can be here,
or you can be at position 10,
which is i-isomorphic to the- to the other side in this kind of square,
v dot diagonal, again,
you have, uh, two different positions.
And in this last fully connected graph on four nodes,
uh, all nodes are equivalent.
So there is- all- all positions are equivalent, so there is only one.
So what this shows is that, um,
if you say how many graphlets are there on five nodes,
uh, there is, uh,
73 of them, right?
Labeled from one, uh,
all the way to 73 because it's different graphs as well as,
uh, positions in these graphs.
So now that we know what the graphlets are,
we can define what is called Graphlet Degree Vector,
which is basically a graphlet-base,
uh, based features, uh, for nodes.
And graphlet degree counts, um,
the number of times a given graphlet appears,
uh, uh, rooted at that given node.
So the way you can think of this is, degree counts,
the number of edges that the node touches, uh,
clustering coefficient counts the number of triangles
that are node touches or participates in and,
uh, graphlet degree vector counts the number of
graphlets that- that a node, uh, participates in.
So to give you an example, um, uh,
a Graphlet Degree Vector is then simply
a count vector of graphlets rooted at that given node.
So to, uh, to give an example,
consider this particular graph here,
and we are interested in the node v. Then
here is a set of graphlets on two and three nodes.
This is our universe of graphlets.
We are only look- going to look at
the graphlets all the way up to size of three nodes and node v there.
Um, and then, uh, you know,
what are the- what are now the graphlets instances, for example,
the- the graphlets of type a,
these node v participates in two of them, right?
It's- one is here and the other one is there, right?
So this means this one and that one.
Then the graphlet of type b node v participates in one of them, right?
This is, um, this is the graphlet here, right, uh, b.
And then you say, how about how many graphlets of type d does, uh,
node, um, sorry, of type c,
does node, uh, v participate in.
And it doesn't participate in any because,
uh, here it is, but these two nodes are also connected.
So, um, because graphlets are induced,
this edge appears here as well.
So for d we get zero,
sorry, for c we get zero.
How about for d?
D is a path of- on two nodes.
If you look at it, there are two instances of d. Uh,
here is one and here is the other as, uh, illustrated here.
So graphlet, uh, degree vector for node v would be two, one, zero, two.
So two, one, zero, two.
And this now characterizes the local neighborhood structure, uh,
around the given node of interest based on the frequencies of these graphlets that,
uh, the node v, uh, participates in.
So, um, if we consider graphlets from,
uh, two to five no- nodes,
we can now describe every node in the network with a vector that
has 73 dimensions or 73- 73 coordinates.
Uh, and this is essentially a signature of a node that describes the topology,
uh, of nodes, uh, neighborhood.
Uh, and it captures its interact- interconnections,
um, all the way up to the distance of four- four hops, right?
Because, um, uh, a chain of four edges has five nodes,
so if you are at the edge of the chain,
which means you count how many paths of length four- do,
uh, uh, lead out of that node.
So it characterizes the network up to the distance, uh, of four.
And, uh, Graphlet Degree Vector provides a measure of nodes,
local network topology and comparing vectors now of- of two nodes,
proves, uh, provides a more detailed measure of local topological similarity.
Then for example, just looking at node degree or clustering coefficient.
So this gives me a really fine-grained way to compare the neighborhoods, uh,
the structure of neighborhoods, uh,
of two different nodes, uh,
in perhaps through different, uh, networks.
So to conclude, uh, so far,
we have introduced different ways to obtain node features.
Uh, and they can be categorized based on
importance features like node degree and different centrality measures,
as well as structure-based features where again,
no degrees like the simplest one, then, uh,
that counts edges, clustering, counts triangles.
And the Graphlet Degree Vector is a generalization that counts,
uh, other types of structures that a given node of interest, uh, participates in.
So to summarize, the importance
based features capture the importance of a node in a graph.
We talked about degree centrality,
um, and we talked about different notions of, uh,
centrality closeness, betweenness, um,
as well as, um,
the eigenvector, uh centrality.
And these types of features are using in predicting,
for example, how important or influential are nodes in the graph.
So for example, identifying celebrity users in social networks would be one,
uh, such, uh, example.
Um, the other type of node level features that we talked about,
were structured based-fea- structure-based features that capture
the topological properties of local neighborhood around the node.
We talked about node degree,
clustering coefficient and graphlet degree vector
and these types of features that are very useful,
uh, for predicting a particular nodes role,
uh, in the network.
So for example, if you think about predicting protein function,
then, um, these type of graphlet, uh,
features are very useful because they characterize, uh,
the structure of the, uh,
network, uh, around, given, uh, node.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 02.2 - Traditional Feature-based Methods Link.txt
We continue with our investigation of traditional machine learning approaches uh,
to uh, graph level predictions.
And now we have- we are going to focus on
link pre- prediction tasks and features that capture,
uh, structure of links,
uh, in a given, uh, network.
So the link le- level prediction tasks is the following.
The task is to predict new links based on the existing links in the network.
So this means at test time,
we have to evaluate all node pairs that are not yet linked,
uh, rank them, and then, uh,
proclaim that the top k note pairs, as, um,
predicted by our algorithm,
are the links that are going to occur,
uh, in the network.
And the key here is to design features for a pair of nodes.
And of course, what we can do, um, uh,
as we have seen in the node level, uh,
tasks, we could go and, uh,
let say concatenate, uh,
uh, the features of node number 1,
features of the node number 2,
and train a model on that type of, uh, representation.
However, that would be very, um,
unsatisfactory, because, uh, many times this would, uh,
lose, uh, much of important information about the relationship between the two nodes,
uh, in the network.
So the way we will think of this link prediction task is two-way.
We can formulate it in two different ways.
One way we can formulate it is simply to say,
links in the network are,
let say missing at random,
so we are given a network,
we are going to remove, uh,
at random some number of links and then trying to predict,
uh, back, uh, those links using our,
uh, machine learning algorithm.
That's one type of a formulation.
And then the other type of a formulation is that we are going to predict links over time.
This means that if we have a network that naturally evolves over time, for example,
our citation network, our social network,
or our collaboration network,
then we can say, ah,
we are going to look at a graph between time zero and time zero,
uh, prime, and based on the edges and the structure up to this uh,
the time t 0 prime, uh,
we are going then to output a ranked list L of
links that we predict are going to occur in the future.
Let's say that are going to appear between times T1 and T1 prime.
And the way, uh, we can then evaluate this type of approach is to say ah,
we know that in the future, um,
n new links will appear, let's, uh,
let's rank, uh, the- the potential edges outputed by
our algorithm and let's compare it to the edges
that actually really appeared, uh, in the future.
Uh, this type of formulation is useful or natural
for networks that evolve over time like transaction networks,
like social networks, well, edges, uh, keep,
um- keep adding, while for example,
the links missing at random type formulation is more useful, for example,
for static networks like protein- protein interaction networks,
where we can assume,
even though this assumption is actually heavily violated, that, you know,
biologists are testing kind of at random connections between proteins,
um, and we'd like to infer what other connections in the future, uh,
are going for- for biologists are going to discover, uh, in the future,
or which links should they probe with the,
uh- that lab, uh, experiments.
Of course, in reality,
biologists are not exploring the physical, uh,
protein-protein interaction network, uh, um, at random.
Um, you know, they are heavily influenced by positive results of one another.
So essentially some parts of this network suddenly well explored,
while others are very much, uh, under explored.
So with these two formulations, uh,
let's now start thinking about, um,
how are we going to, uh,
provide a feature descriptor,
uh, for a given, uh, pair of nodes?
So the idea is that for a pair of nodes x, y,
we are going to compute some score,
um, uh, c, uh, x, y.
For example, a score, uh,
could be the number of common neighbors between nodes, uh, X and Y.
And then we are going to sort all pairs x,
y according to the decreasing, uh,
score C, um, and we will predict top end pairs as the new links that are going to appear,
uh, in the network.
And then we can end, uh, the test-time, right,
we can actually go and observe which links actually appear and compare these two lists,
and this way determine how well our approach,
our algorithm, um, is working.
We are going to review, uh,
three different ways how to, uh,
featurize or create a descriptor of the relationship between two nodes in the network.
We are going to talk about distance-based features,
local neighborhood overlap features,
as well as global neighbor- neighborhood overlap, uh, features.
And the goal is that for a given pair of nodes,
we are going to describe the relationship, um,
between the two nodes, uh,
so that from this relationship we can then predict or
learn whether there exists a link between them or not.
So first, uh, we talk about distance-based feature.
Uh, this is very natural.
We can think about
the shortest path distance between the two nodes and characterize it in this way.
So for example, if we have nodes B and H,
then the shortest path length between them, uh, equals two.
So the value of this feature would be equal to two.
However, if you look at this, uh, this does,
uh- what this- what this metric does not capture it, it captures the distance,
but it doesn't measure,
kind of capture the degree of neighborhood overlap or the strength of connection.
Because for example, you can look in this network
nodes B and H actually have two friends in common.
So the- the connection here in some sense is stronger between them.
Then for example, the connection between, uh,
node D and, uh, node, uh,
F, um, because they only have kind of- there is
only one path while here there are two different paths.
So the way we can, um,
try to capture the strength of connection between two nodes would be to ask, okay,
how many neighbors, uh,
do you have in common, right?
What is the number of common friends between a pair of nodes?
And this is captured by the notion of local neighborhood overlap,
which captures the number of neighboring nodes shared between two nodes,
v and, uh- v1 and v2.
Uh, one way to capture this is you simply say,
what is the- what is the number of common neighbors, right?
We take the neighbors of node V1,
take the neighbors of node V2,
and take the intersection of these two sets.
Um, a normalized version of this, uh,
same idea is Jaccard coefficient,
where we take the intersection-
the size of the intersection divided by the size of the union.
The issue with common neighbors is that, of course,
nodes that have higher degree are more likely to have neighbors with others.
While here in the Jaccard coefficient, in some sense,
we are norma- we are trying to normalize, um,
by the degree, uh,
to some degree by saying what is the union of the number of,
um, neighbors of the two nodes.
Uh, and then, uh,
the other type of, uh,
uh, local neighborhood overlap, uh,
metric that is- that actually works quite well in practice is called,
uh, Adamic- Adar index.
And simply what this is saying is,
let's go over the,
um- let's sum over the neighbors that nodes v1 and v2 have in common,
and let's take one over the log, uh, their degree.
So basically, the idea here is that we count how many neighbors,
um, the two nodes have in common,
but the importance of uneven neighbor is,
uh- is low, uh- decreases, uh, with these degrees.
So if you have a lot of,
um, neighbors in common that have low degree,
that is better than if you have a lot of high-
highly connected celebrities as a set of common neighbors.
So this is a- a net- a feature that works really well in a social network.
Of course, the problem with, uh, local, um,
network neighborhood overlap is the limitation is that this, uh,
metric always returns zero if two- two nodes are not- do not have any,
uh, neighbors in common.
So for example, in this case,
if we would want to say what is the neighborhood overlap between nodes A and E,
because they have no neighbors in common,
they are more than, um,
uh, two hops away from each other.
Then the- if only in such cases,
the return- the value of that it will be returned to will always be, zero.
However, in reality, these two nodes may still potentially be connected in the future.
So to fix this problem,
we then define global neighborhood overlap matrix.
That is all of this limitation by only, uh,
focusing on a hop- two hop distances and
two-hop paths between a pairs- pair of nodes and consider,
um, all other distances or the entire graph as well.
So let's now look at global neigh- neighborhood overlap type, uh, metrics.
And the metric we are going to talk about is called Katz index,
and it counts the number of all paths, uh,
of all different lengths between a given pair of nodes.
So now we need to figure out two things here.
First is, how do we compute number of paths of a given length,
uh, between, uh, two, uh, nodes?
This can actually be very elegantly computed by
using powers of the graph adjacency matrix.
So let me give you a quick illustration or a quick proof why this is true.
So the uh, first,
I wanna give you the intuition around the powers of adjacency matrix, right?
The point is that what we are going to show is
that computing number of paths between uh, two nodes um,
reduces down to computing powers of the graph adjacency matrix or
essentially taking the graph adjacency matrix and multiplying it with itself.
So first graph adjacency matrix recall,
it has a value 1 at every entry uv if- if nodes u and v are connected.
Then let's say that p,
uv uh superscript capital K counts the number of paths of
length K between nodes u and v. And our goal is to show that uh,
uh, if we are interested in the number of paths uh,
uh, of length K,
then we have to uh,
compute A to the power of k and that entry uv will tell us the number of pets.
The capital K here is the same as uh,
uh, a small case,
so the Kth power of A measures the number of paths of a given length.
And if you think about it right,
how many paths of length 1 are there between a pair of
nodes that is exactly captured by the graph adjacency matrix, right?
If a pair of nodes is connected,
then there is a value 1,
and if a pair of nodes is not connected,
then there is the value 0.
Now that we know how to compute um,
the number of paths of length 1 between a pair of nodes.
Now we can ask how many- how do we compute
the number of paths of length 2 between a pair of nodes u.
And we are going to do this via the two- two-step procedure.
Uh, and we are going to do this by decompose the path of
length 2 into a path of length 1 plus another path of length 1.
So the idea is that we compute the number of paths of length 1
between each of u's neighbors and v,
and then um, add one to that.
So the idea is the following,
the number of paths between nodes u and v of length 1- of length 2 is
simply a summation over the nodes i that are the neighbors of the starting node u,
um, times the number of paths now from
this neighbor i to the target node v. And this will
now give us the number of paths of length 2 between u and v. And now what you can see,
you can see a substitute here back, the adjacency matrix.
So all these is a sum over i,
u- A _ui times A_iv.
So if you see this,
this is simply the product of matrices uh,
of made- of adjacency matrix A_ iu itself.
So this is now uh,
the entry uv of the adjacency matrix A uh, squared.
Um, this is uh,
now by basically by induction,
we can keep repeating this and get a higher powers that count paths of longer lengths um,
as- as this is uh, increasing.
Another way to look at this,
here is a visual proof is that uh,
what is A squared?
A squared is A multiplied by itself,
so when we are interested in a given,
let's say entry, here these are entry,
these are neighbors of Node 1.
These are um, now the,
the number of paths of length 1 between one- one's neighbors and node number 2.
So after the multiplication,
the value here will be 1,
which we mean that there is one path of length 2 between node 1 uh, and Node 2.
So this is how powers of adjacency matrix give
account paths of length K between a pair of nodes uh, in the network.
What this means is that now we can define the- we have developed
the first component that will allow us to count- to compute the cuts index,
because it allows us to count the number of paths between a pair of nodes for a given K.
But what we still need to decide is how do we do this for all the path lengths,
from one to infinity.
So to compute the pets, as we said,
we are going to use powers of the adjacency matrix uh,
you know, uh, the adjacency matrix itself tells us powers of length 1,
square of it tells us power of,
uh squared tells us paths of length 2,
and the adjacency matrix raised to
the power l counts the number of paths of length l between a pair of nodes.
NOW, the Katz index goes over from 1 path lengths all the way to infinity.
So the- the Katz index uh,
global neighborhood overlap between nodes v1 and
v2 is simply a summation of l from one to infinity.
We have this Beta raised to the power of l by basically
a disc- discount factor that gives lower- lower importance
to paths of longer lengths and A to b_l counts
the number of paths of length l between nodes of v1 and v2.
And now what is interesting about Katz index is that, um,
um,uh, we can actually compute this particular expression in a closed-form.
And here is- here is the- the formula for the Katz index again,
and basically what uh,
here is a closed form expression that will exactly compute the sum,
and the reason um,
why this is true or y there is inequality is this.
We notice that this is simply uh,
geometric series uh, for matrices,
and for that there exists
a closed form expression that all that it requires us is take the identity matrix
minus Beta times adjacency matrix inverted that and
then again then subtract the identity matrix again.
And the entries of this matrix S will give us
the Katz neighborhood overlap scores for any pair uh, of nodes.
So to summarize, uh, link level features.
Uh, we- we described three types of them.
We talked about distance-based features that users, for example,
shortest path between a pair of nodes and does not capture neighborhood overlaps.
Then we talked about this neighborhood overlap metrics like common neighbors, Jaccard,
and the Dmitry data that captures find- in a fine-grained wait,
how many neighbors does a pair of nodes have in common?
But the problem with this is that nodes that are more than two hops apart,
nodes that have no neighbors in common,
the metric will return value 0.
So the global neighborhood overlap type metrics,
for example, like Katz, uh,
uses global graph structure to give us a score for a pair of nodes and Katz index counts
the number of pets of all lands between a pair of
nodes where these paths are discounted um,
exponentially with their length.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 02.3 - Traditional Feature-based Methods Graph.txt
So far we discussed node and edge level features,
uh, for a prediction in graphs.
Now, we are going to discuss graph-level features and
graph kernels that are going to- to allow us to make predictions for entire graphs.
So the goal is that we want one
features that characterize the structure of an entire graph.
So for example, if you have a graph like I have here,
we can think about just in words,
how would we describe the structure of this graph,
that it seems it has, kind of,
two loosely connected parts that there are quite a few edges, uh,
ins- between the nodes in each part and that there is
only one edge between these two different parts.
So the question is,
how do we create the feature description- descriptor that will allow us to characterize,
uh, the structure like I just, uh, explained?
And the way we are going to do this,
is we are going to use kernel methods.
And kernel methods are widely used for
traditional machine learning in, uh, graph-level prediction.
And the idea is to design a kernel rather than a feature vector.
So let me tell you what is a kernel and give you a brief introduction.
So a kernel between graphs G,
and G', uh, returns a real value,
and measures a similarity between these two graphs,
or in general, measure similarity between different data points.
Uh, kernel matrix is then a matrix where
simply measures the similarity between all pairs of data points,
or all pairs of graphs.
And for a kernel to be a valid kern- kernel this ma- eh,
kernel matrix, uh, has to be positive semi-definite.
Which means it has to have positive eigenvalues,
for exam- and- and- as a consequence,
it has to be, symet- it is a symmetric, uh, ma- matrix.
And then what is also an important property of kernels,
is that there exist a feature representation, Phi,
such that the kernel between two graphs is simply a feature representation,
uh, wa-, uh, of the first graph dot product
with the feature representation of the second graph, right?
So Phi of G is a vector,
and Phi of G is a- is another vector,
and the value of the kernel is simply a dot product of this vector representation,
uh, of the two, uh, graphs.
Um, and what is sometimes nice in kernels,
is that this feature representation, Phi,
doesn't even need to- to be explicitly created for us to be able to compute the value,
uh, of the kernel.
And once the kernel is defined,
then off-the-shelf machine learning models,
such as kernel support vector machines,
uh, can be used to make, uh, predictions.
So in this le- in this part of the lecture,
we are going to discuss different, uh,
graph kernels, which will allow us to measure similarity between two graphs.
In particular, we are going to discuss
the graphlet kernel as well as Weisfeiler-Lehman kernel.
There are oth- also other kernels that are proposed in the literature,
uh, but this is beyond the scope of the lecture.
For example, random-walk kernel,
shortest-path kernel, uh, and many, uh, others.
And generally, these kernels provide a very competitive performance in graph-level tasks.
So what is the key idea behind kernels?
The key idea in the goal of kernels is to define a feature vector,
Phi of a given graph,
G. And the- the idea is that,
we are going to think of this feature vector, Phi,
as a bag-of-words type representation of a graph.
So what is bag of words?
When we have text documents,
one way how we can represent that text document,
is to simply to represent it as a bag of words.
Where basically, we would say,
for every word we keep a count of how often that word appears in the document.
So we can think of, let's say,
words sorted alphabetically, and then,
you know, at position, i,
of this bag-of-words representation,
we will have the frequency,
the number of occurrences of word,
i, in the document.
So in our- in the same way,
and naive extension of this idea to graphs would be to regard nodes as words.
However, the problem is that since both- since graphs can have very different structure,
but the same number of nodes,
we would get the same feature vector,
or the same representation for two very different graphs, right?
So if we re- regard nodes as words,
then this graph has four nodes,
this graphs has four nodes,
so their representation would be the same.
So we need a different candidate for- for the-
for the word in this kind of bag-of-words representation.
To be, for example, a bit more expressive,
we could have what we could call,
uh, degree kernel, where we could say,
w- how are we going to represent a graph?
We are going to represent it as a bag-of-node degrees, right?
So we say, "Aha,
we have one node of degree 1,
we have three nodes of degree 2,
and we have 0 nodes of degree, uh, 3."
In the same way,
for example, uh, here,
we could be asking,
how many nodes, uh,
of different degrees do we have here?
We have 0 nodes of degree, um, 1,
we have two nodes, uh, of degree 2,
and two nodes, uh, of degree, um, 3.
So, um, and this means that now we would, er,
obtain different feature representations for these,
uh, different, uh, graphs,
and that would allow us to distinguish these different, uh, graphs.
And now, both the graphlets kernel as well as the Weisfeiler-Lehman kernel,
use this idea of bag-of-something representation of a graph where- where the star,
this something is more sophisticated than node degree.
So let's, uh, first talk about the graphlets kernel.
The idea is that writing 1,
I represented the graph as a count of the number of different graphlets in the graph.
Here, I wanna make,
uh, um important point;
the definition of graphlets for a graphlet kernel,
is a bit different than the definition of a graphlet in the node-level features.
And there are two important differences that graphlets in
the node-level features do not need to be connected,
um, and, um also that they are not, uh, uh, rooted.
So graphlets, uh, in this- in the, eh,
graphlets kernel are not rooted,
and don't have to be connected.
And to give you an example,
let me, uh, show you, uh, the next slide.
So for example, if you have a list of graphlets that we are interested
in little g_1 up to the little g_n_k,
let's say these are graphlets of size k,
then let say for k equals 3,
there are four different graphlets, right?
There are four different con- graphs on three nodes,
and directed, fully connected two edges,
one edge, and no edges.
So this is the definition of graphlets in the graph kernel.
And for example, for k equals 4,
that are 11 different graphlets,
fully connected graph all the way to the graph on four nodes without any connections.
And now, uh, given a graph,
we can simply represent it as count of the number of structures,
um, er, different graphlets that appear, uh, in the graph.
So for example, given a graph,
and the graphr- graphlet list,
we define the graphlet count vector f,
simply as the number of instances of a given graphlet that appears,
uh, in our graph of interest.
For example, if these G is our graph of interest,
then in this graph,
there resides one triangle,
there resides three different parts of land 2,
there reside six different edges with an isolated nodes, and there, uh,
exist no, uh, triplet, uh, of nodes,
uh, that are, uh, that are not connected,
uh, in this graph.
So the graphlet feature vector in this case, uh,
would be here, would have ready 1,
3, 6, uh, and 0.
Now, given two graphs,
we can define the graphlet kernel simply as the dot product between the graphlet, uh,
count vector of the first graph times,
uh, the graphlet count vector of the second graph.
Um, this is a good idea,
but actually, there is a slight problem.
The problem is that graphs G1 and G2 may have different sizes,
so the row counts will be very,
uh, different of, uh,
graphlets in different graphs.
So a common solution people apply is to normalize this feature vector representation,
uh, for the graph.
So this means that, um,
the- the graphlet, uh,
vector representation for a given graph is simply the can- the count of
individual graphlets divided by the total number of graphlets that appear in the graph.
So if the- this essentially
normalizes for the size and the density of the underlying graph,
and then the graphlet kernel is defined as the dot product between these,
um, uh, feature vector representations of graphs,
uh, uh, h that capture,
uh, the frequency or the proportion of- of our given graphlet,
um, in a- in a graph.
There is an important limitation of the graphlet graph kernel.
And the limitation is that counting graphlets is very expens- expensive.
Counting a k-size graphlet in a graph with n nodes, uh,
by enumeration takes time or the n raised to the power k. So,
um, this means that counting graphlets of
size k is polynomial in the number of nodes in the graph,
but it is exponential in the graphlet size.
Um, and this is unavoidable in the worse-case
since sub-graph isomorisic- isomorphism judging whether,
uh, a sub-graph is a- is a, uh,
isomorphic to another, uh,
graph, is, uh, NP-hard.
Um, and, uh, there are faster algorithms if,
uh, graphs node, uh,
node degree is bounded by d,
then there exist a fa- faster algorithm to count the graphlets of size k. However,
the issue still remains that counting
these discrete structures in a graph is very time consuming, um, very expensive.
So we can only count graphlets up to,
uh, you know, uh,
a handful of nodes.
And then the- and then the exponential complexity takes over and we cannot count,
uh, graphlets, uh, that are,
uh, larger than that.
Um, so the question is,
how do we design a more efficient graph kernel?
Um, and Weisfeiler-Lehman graph kernel, uh, achieves this goal.
The goal here is to design an efficient graph feature descriptor Phi of G, uh,
where the idea is that we wanna use a neighborhood structure to iteratively enrich,
uh, node, uh, vocabulary.
And, um, we generalize a version of node degrees since node degrees are
one hot- one-hop neighborhood information to multi-hop neighborhood information.
And the algorithm that achieves this is, uh, uh,
called the Weisfeiler-Lehman graph isomorphism test,
or also known as color refinement.
So, uh, let me explain, uh, this next.
So the idea is that we are given a graph G with a set of nodes V,
and we're going to assign an initial color,
um, c^0, so this is an initial color to each node.
And then we are going to iteratively, er,
aggregate or hash colors from the neighbors to invent new colors.
So the way to think of this, uh,
the new color for a given node v will be a hashed value of its own color, um,
from the previous time step and a concatenation
of colors coming from the neighbors u of the node v of interest,
where hash is basically a hash functions that
maps different inputs into different, uh, colors.
And after k steps of this color refinement,
um, um, c, uh,
capital v of, uh,
capital K of v summarizes the structure, uh,
of the graph, uh, at the level of,
uh, K-hop, uh, neighborhood.
So let me, um, give you an example, uh, and explain.
So for example, here I have two, uh,
graphs that have very similar structure but are just slightly, uh, different.
The difference is, uh, this, uh,
edge and here, um, the- the, uh,
the diagonal edge, the triangle closing edge,
um, um, is, uh, different.
So first we are going to assign initial colors to nodes.
So every node gets the same color,
every node gets a color of one.
Now we are going to aggregate neighboring colors.
For example, this particular node aggregate colors 1,1,
1, um, and, uh,
adds it to it- to itself,
while this particular node up here aggregates colors from its neighbors,
one and one, uh, and it is here.
And the same process, uh,
happens in this second,
uh, graphs- graph as well.
Now that, um, we have collected the colors,
uh, we go, uh,
and hash them, right?
So we apply a hash- hash function that takes
nodes' own color plus the colors from neighbors and produces new colors.
And let's say here the hash function for the first combination returns one,
then two, then three, uh, four, and five.
So now we color the graphs,
uh, based on these new refined colors, right?
So this is the coloring of the first graph,
and this is the coloring of the second graph based on the hash values
of the- of the aggregated colors from the first step.
Now we take these two graphs and,
again, apply the same color aggregation scheme, right?
So for example, this node, uh,
with color 4 aggregates colors from its neighbors,
so aggregates the 3, 4, and 5.
So we have 3, 4, and 5 here, while, for example,
this node here of color 2 aggregates from its neighbor,
uh, that is colored 5,
so it gets 2, 5.
And again, for this graph,
the same process happens.
Now, again, we take, um, uh, this,
uh, aggregated colors, um, and we hash them.
And let's say our hash function, uh,
assigns different, uh, new colors, uh,
to, uh, to this,
uh, colors that are,
uh, combined aggregated from the previous timesteps.
So now we can take this, uh, original, uh,
aggregated colored graph and,
uh, relabel the colors based on the hash value.
So 4,345, uh, 4, um,
er, where is, uh, er, uh,
34- uh, 345- um, is, um,
layer hashes into color 10s,
so we replace a color 10, uh, here.
And we could keep iterating this and we would come up, uh, with, uh,
more and more, uh, refinement,
uh, of the, uh,
uh, colors of the graph.
So now that we have run this color refinement for a,
uh, a fixed number of steps,
let say k iterations,
the Weisfeiler-Lehman, uh, kernel counts number of nodes with a given color.
So in our case,
we run- we run this three times,
so we have 13 different colors.. And now
the feature description for a given graph is simply the count,
the number of nodes of a given color, right?
In the first iteration,
uh, all the nodes were colored,
um- all six nodes were colored the same one- uh, the same way.
Um, so there is six instances of color 1.
Then, um, after we iter- um,
agg- aggregated the colors and refined them, you know,
there were two nodes of color 2,
one node of color 3,
two nodes of color 4, um, and so on.
So here is now the feature description in terms of color counts, uh, for, uh,
for, uh, different colors for the first graph and different colors,
uh, for the second graph.
So now that we have the feature descriptions,
the Weisfeiler-Lehman graph kernel would simply take the dot product between these two,
uh, uh, feature descriptors and return a value.
So for example, in our case,
the si- the, uh, Weisfeiler-Lehman, uh,
kernel similarity between the two graphs is the dot product between the,
uh, feature descriptors, uh, here.
These are the two, uh,
feature descriptors and we compute the dot product,
we would get a value of, uh, 49.
So WL kernel is very popular and very strong,
uh, gives strong performance,
and it is also computationally efficient because the time complexity of
this color refinement at each step is linear in the size of the graph.
It is linear in the number of edges because all that
every node has to do is collect the colors, uh, from its, uh,
neighboring nodes and- and produce- and apply
a simple hash function to- to come up with a new,
um, uh, with a new, uh, color.
When computing the kernel value,
many colors, uh, appeared in two graphs need to be tracked.
So the number of colors will be at most the number of nodes,
uh, in the network.
So this again won't be too- too large.
And counting the colors again takes linear time because it's just a sweep over the nodes.
So the- the total complexity, uh,
of computing the Weisfeiler-Lehman graph kernel between a pair of, uh,
graphs is simply linear in the number of edges in the two graphs.
So this means this is extremely, uh,
fast and actually works,
uh, really well in practice.
So to summarize the graph level features that we have discussed,
first we talked about, uh,
the notion of graph kernels,
where basically graph is represented as a bag of graphlets or a bag of, uh, colors.
Um, and, uh, when we represent the graph as a graph- uh,
as a bag of graphlets,
this is extremely- this is very expensive representation because counting the graphlets,
uh, takes time exponential in the size of the graph.
At the same time, Weisfeiler-Lehman, uh,
kernel is based on this case step color refinement algorithm that
enriches and produces new node colors that are aggregated from the,
um, colors of the immediate neighbors of the node.
And as multiple rounds of this color refinement are run,
the node kind of gathers color information from farther away,
uh, parts of the network.
So here we represent the graph as a bag of colors.
This is computationally efficient.
The time is linear in the size of the graph, um,
and it is also closely related to graph neural networks that we are going to study,
uh, later in this course.
So, um, Weisfeiler-Lehman is a really, uh,
good way to measure similarity, um,
between graphs, and in many cases,
it is, uh, very hard to beat.
So this concludes the today lecture where we talked about, um,
three different, uh, approaches to traditional,
uh, graph, uh, level, um, machine learning.
We talked about, um, handcrafted features for node-level prediction,
uh, in terms of node degree,
centrality, clustering, coefficient, and graphlets.
We talked about link-level or edge-level features,
distance-based, as well as local and global neighborhood overlap.
And then last we'd talk about how do we characterize the structure of the entire graph.
We talked about graph kernels, uh,
and in particular about graphlet kernel and the WL,
meaning Weisfeiler-Lehman graph kernel.
So this concludes our discussion of traditional machine learning approaches, uh,
to graphs and how do we create feature vectors from nodes, links, um,
and graphs, um, in a- in a scalable and interesting way. Uh, thank you very much.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 03.1 - Node Embeddings.txt
This is Lecture 3 of our class and we are going to talk today about node embeddings.
So the way we think of this is the following.
Um, the inter- what we talked
on last week was about traditional machine learning in graphs,
where the idea was that given an input graph,
we are going to extract some node link or graph level
features that basically describe the topological structure,
uh, of the network,
either around the node,
around a particular link or the entire graph.
And then we can take that topological information,
compare, um, er, uh, um,
combine it with the attribute-based information to
then train a classical machine learning model
like a support vector machine or a logistic regression,
uh, to be able to make predictions.
So, um, in this sense, right,
the way we are thinking of this is that we are given an input graph here.
We are then, uh,
creating structure- structured features or structural features,
uh, of this graph so that then we can apply
our learning algorithm and make, uh, prediction.
And generally most of the effort goes here into the feature engineering,
uh, where, you know, we are as,
uh, engineers, humans, scientists,
we are trying to figure out how to best describe,
uh, this particular, um,
network so that, uh,
it would be most useful, uh,
for, uh, downstream prediction task.
Um, and, uh, the question then becomes,
uh, can we do this automatically?
Can we kind of get away from, uh, feature engineer?
So the idea behind graph representation learning is that we wanna
alleviate this need to do manual feature engineering every single time, every time for,
uh, every different task,
and we wanna kind of automatically learn the features,
the structure of the network,
um, in- that we are interested in.
And this is what is called, uh,
representation learning so that no manual, uh,
feature engineering is, uh, necessary, uh, anymore.
So the idea will be to do
efficient task-independent feature learning for machine learning with the graphs.
Um, the idea is that for example,
if we are doing this at the level of individual nodes,
that for every node,
we wanna learn how to map this node in a d-dimensional
space ha- and represent it as a vector of d numbers.
And we will call this vector of d numbers as feature representation,
or we will call it, um, an embeding.
And the goal will be that this, uh, mapping, um,
happens automatically and that this vector
captures the structure of the underlying network that,
uh, we are, uh, interested in,
uh, analyzing or making predictions over.
So why would you wanna do this?
Why create these embeddings?
Right. The task is to map nodes into an- into an embedding space.
Um, and the idea is that similarity, uh,
of the embeddings between nodes indicates their similarity in the network.
Uh, for example, you know,
if bo- nodes that are close to each other in the network,
perhaps they should be embedded close together in the embedding space.
Um, and the goal of this is that kind of en-
automatically encodes the network, uh, structure information.
Um, and then, you know,
it can be used for many kinds of different downstream prediction tasks.
For example, you can do any kind of node classification, link prediction,
graph classification, you can do anomaly detection,
you can do clustering,
a lot of different things.
So to give you an example, uh,
here is- here is a plot from a- a paper that came up with
this idea back in 2014, fe- 2015.
The method is called DeepWalk.
Um, and they take this, uh, small, uh,
small network that you see here,
and then they show how the embedding of nodes would look like in two-dimensions.
And- and here the nodes are,
uh, colored by different colors.
Uh, they have different numbers.
And here in the, um, in this example, uh,
you can also see how, um, uh,
how different nodes get mapped into different parts of the embedding space.
For example, all these light blue nodes end up here,
the violet nodes, uh,
from this part of the network end up here,
you know, the green nodes are here,
the bottom two nodes here,
get kind of set, uh,
uh on a different pa- uh,
in a different place.
And basically what you see is that in some sense,
this visualization of the network and
the underlying embedding correspond to each other quite well in two-dimensional space.
And of course, this is a small network.
It's a small kind of toy- toy network,
but you can get an idea about, uh,
how this would look like in,
uh, uh- in, uh,
more interesting, uh, larger,
uh- in larger dimensions.
So that's basically the,
uh- that's basically the, uh, idea.
So what I wanna now do is to tell you about how do we formulate this as a task, uh,
how do we view it in this, uh,
encoder, decoder, uh, view or a definition?
And then what kind of practical methods, um,
exist there, uh for us to be able, uh, to do this.
So the way we are going to do this, um,
is that we are going to represent, uh,
a graph, as a- as a- with an adjacency matrix.
Um, and we are going to think of this,
um, in terms of its adjacency matrix,
and we are not going to assume any feature, uh, uh,
represe- features or attributes,
uh, on the nodes, uh, of the network.
So we are just going to- to think of this as a- as a- as a set of,
um, as a- as an adjacency matrix that we wanna- that we wanna analyze.
Um, we are going to have a graph, as I showed here,
and the corresponding adjacency matrix A.
And for simplicity, we are going to think of these as undirected graphs.
So the goal is to encode nodes so that similarity in the embedding space- uh,
similarity in the embedding space,
you can think of it as distance or as a dot product,
as an inner product of the coordinates of two nodes
approximates the similarity in the graph space, right?
So the idea will be that in- or in the original network,
I wanna to take the nodes,
I wanna map them into the embedding space.
I'm going to use the letter Z to denote the coordinates,
uh, of that- of that embedding,
uh, of a given node.
Um, and the idea is that, you know,
some notion of similarity here
corresponds to some notion of similarity in the embedding space.
And the goal is to learn this encoder that encodes
the original network as a set of, uh, node embeddings.
So the goal is to- to define the similarity in the original network, um,
and to map nodes into the coordinates in the embedding space such that, uh,
similarity of their embeddings corresponds to the similarity in the network.
Uh, and as a similarity metric in the embedding space, uh,
people usually, uh, select, um, dot product.
And dot product is simply the angle,
uh, between the two vectors, right?
So when you do the dot product,
it's the cosine of the- of the angle.
So if the two points are close together or in the same direction from the origin,
they have, um, um,
high, uh, uh, dot product.
And if they are orthogonal,
so there is kind of a 90-degree angle, uh,
then- then they are as- as dissimilar as
possible because the dot product will be, uh, zero.
So that's the idea. So now what do we need to
define is we need to define this notion of, uh,
ori- similarity in the original network and we need to define then
an objective function that will connect the similarity with the, uh, embeddings.
And this is really what we are going to do ,uh, in this lecture.
So, uh, to summarize a bit, right?
Encoder maps nodes, uh, to embeddings.
We need to define a node similarity function,
a measure of similarity in the original network.
And then the decoder, right,
maps from the embeddings to the similarity score.
Uh, and then we can optimize the parameters such that
the decoded similarity corresponds as closely as
possible to the underlying definition of the network similarity.
Where here we're using a very simple decoder,
as I said, just the dot-product.
So, uh, encoder will map notes into low-dimensional vectors.
So encoder of a given node will simply be the coordinates or the embedding of that node.
Um, we talked about how we are going to define the similarity
in the embedding space in terms of the decoder,
in terms of the dot product.
Um, and as I said, uh,
the embeddings will be in some d-dimensional space.
You can think of d, you know, between,
let's say 64 up to about 1,000,
this is usually how- how,
uh, how many dimensions people, uh, choose,
but of course, it depends a bit on the size of the network,
uh, and other factors as well.
Um, and then as I said,
the similarity function specifies how the relationship in
the- in the vector space map to the relationship in the,
uh, original ,uh, in the original network.
And this is what I'm trying to ,uh,
show an example of, uh, here.
So the simplest encoding approach is that an encoder is just an embedding-lookup.
So what- what do I mean by this that- is that an encoded- an
encoding of a given node is simply a vector of numbers.
And this is just a lookup in some big matrix.
So what I mean by this is that our goal will be to learn this matrix Z,
whose dimensionalities is d,
the embedding dimension times the number of nodes,
uh, in the network.
So this means that for every node we will have a column
that is reserved to store the embedding for that node.
And this is what we are going to learn,
this is what we are going to estimate.
And then in this kind of notation,
you can think of v simply as an indicator vector that has all zeros,
except the value of one in the column
indicating the- the ID of that node v. And- and what this
will do pictorially is that basically you can think of
Z as this matrix that has one column per node,
um, and the column store- a given column stores the embedding of that given node.
So the size of this matrix will be number of nodes times the embedding dimension.
And people now who are, for example,
thinking about large graphs may already have a question.
You know, won't these to be a lot of parameters to estimate?
Because the number of parameters in this model is basically the number of entries, uh,
of this matrix, and this matrix gets very large because
it dep- the size of the matrix depends on the number of nodes in the network.
So if you want to do a network or one billion nodes,
then the dimensionality of this matrix would be one billion times,
let's say thousandth, uh,
and embedding dimension and that's- that's,
uh, that's a lot of parameters.
So these methods won't necessarily be most scalable, you can scale them,
let's say up to millions or a million nodes or, uh,
something like that if you- if you really try,
but they will be slow because for every node
we essentially have to estimate the parameters.
Basically for every node we have to estimate its embedding- embedding vector,
which is described by the d-numbers d-parameters,
d-coordinates that we have to estimate.
So, um, but this means that once we have estimated this embeddings,
getting them is very easy.
Is just, uh, lookup in this matrix where everything is stored.
So this means, as I said,
each node is assigned a unique embedding vector.
And the goal of our methods will be to directly optimize or ,uh,
learn the embedding of each node separately in some sense.
Um, and this means that, uh,
there are many methods that will allow us to do this.
In particular, we are going to look at two methods.
One is called, uh,
DeepWalk and the other one is called node2vec.
So let me- let me summarize.
In this view we talked about an encoder ,uh, decoder, uh,
framework where we have what we call a shallow encoder because it's
just an embedding-lookup the parameters to optimize ,um, are- are,
uh, very simple, it is just this embedding matrix Z. Um,
and for every node we want to identify the embedding z_u.
And v are going to cover in the future lectures is we are
going to cover deep encoders like graph neural networks that- that,
uh, are a very different approach to computing, uh, node embeddings.
In terms of a decoder,
decoder for us would be something very similar- simple.
It'd be simple- simply based on the node similarity based on the dot product.
And our objective function that we are going to try to
learn is to maximize the dot product
of node pairs that are similar according to our node similarity function.
So then the question is,
how do we define the similarity, right?
I've been talking about it,
but I've never really defined it.
And really this is how these methods are going to differ between each other,
is how do they define the node similarity notion?
Um, and you could ask a lot of different ways how- how to do this, right?
You could chay- say,
"Should two nodes have similar embedding if they are perhaps linked by an edge?"
Perhaps they share many neighbors in common,
perhaps they have something else in common or they are in similar part of
the network or the structure of the network around them, uh, look similar.
And the idea that allow- that- that started all this area of
learning node embeddings was that we are going to def- define a similarity,
um, of nodes based on random walks.
And we are going to ,uh,
optimize node embedding for this random-walk similarity measure.
So, uh, let me explain what- what I mean by that.
So, uh, it is important to know that this method
is what is called unsupervised or self-supervised,
in a way that when we are learning the node embeddings,
we are not utilizing any node labels.
Um, will only be basically trying to
learn embedding so that they capture some notion of network similarity,
but they don't need to capture the- the notion of labels of the nodes.
Uh, and we are also not- not utilizing any node features
or node attributes in a sense that if nodes are humans,
perhaps, you know, their interest,
location, gender, age would be attached to the node.
So we are not using any data,
any information attached to every node or attached to every link.
And the goal here is to directly estimate a set of coordinates of node so
that some aspect of the network structure is preserved.
And in- in this sense,
these embeddings will be task-independent because they are
not trained on a given prediction task, um,
or a given specific, you know,
labelings of the nodes or are given specific subset of links,
it is trained just given the network itself.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 03.2 - Random Walk Approaches for Node Embeddings.txt
So we are going to talk about random walk approaches,
uh, to node embeddings.
Um, and the idea here will be the following.
Uh, we are going to learn a vector z for every node
and this would be an embedding of the node and this is what we aim to find.
We are then going to, um,
also define a probability that, uh, basically will- uh,
will be the pre- predicted probability of how similar a given node u,
um, is uh, to some node, uh,
v. And given that we are going to use random walks to define this similarity,
this would be the probab- proba- uh,
predicted probability of visiting node v,
one random walks starting from node u.
Then we are also goin- need to, um,
nonlinear functions, uh, that will be used to,
uh, define or to produce these probabilities.
First, I'm going to define the notion of a softmax function,
which- which is- uh,
which returns a vector of k real values,
uh, um, and the- and these values sum to one.
So essentially, given a set of numbers z,
the- the softmax of that vector will be
a probability distribution over those values and the more likely that,
um, that- number is the maximum in the vector,
the higher the probability,
uh, it will have.
And essentially the way you can think of it, I take this, uh,
value z and I exponentiate them and then I normalize everything to sum to one.
So the idea is if the largest value, um,
in- in this vector z,
when I expo- exponentiate it,
it will be even larger than everything else.
So most probability mass,
um, will be concentrated on that value.
This is why this is called softmax because it's
a kind of a soft version of a maximum function.
Um, and then we are also going to define this notion of a sigmoid which is an
S shaped function that turns real values into,
uh, a range of- of 01,
um, and softmax is defined as 1 over 1 plus e to the minus x. Um,
and this is a nice way how to take something that lives on, uh,
minus infinity to plus infinity and kind of squish it,
uh, to, uh, value 01.
So that's- those are two important functions,
uh, I will- we will use.
Now let me define the notion of a random walk.
So a random walk is simply,
um, a process on top of the graph,
where we sa- let say start at some node and then out
of the outgoing neighbors of that node,
in this case it will be 1, 3, and 5,
we pick one at random and we move to it and this is one step of random- random walk.
Now we are in this,
uh, node 5, again,
we have four different ways in which we can go.
We can return back to four.
We can go to 8, 6 or 7 and we pick one of them at random and move there.
Um, and this process continues,
let say for a- for a- in this case for a fixed,
uh, number of steps.
So the way you can think of this is that we basically simulated this, uh,
random walk over- over this graph and let's say,
um, over our fixed, uh,
number of steps where the random walk can traverse the same edge multiple times,
can return, can go back and forth,
do, uh, whatever the random walk,
uh, wants to do.
All right. And this random walk is a seq- sequence of nodes visited this way,
uh, on a graph across the- across the edges.
So now how are we going to- to define
this notion of similarity and these probabilities that we talked about?
What we are going to do is to say we want to learn these
coordinate z such that the product of two,
uh, nodes u and v,
um, is similar or equals is, uh,
approximates the probability that u and v co-occur on a random walk, uh, over the graph.
So here is- here is the idea, right?
First we will need to estimate the probability of visiting
node v on a random walk starting,
uh, at some node u using some, let's say,
a random walk strategy R. I'm going to define
this notion of random walks strategy, uh, later,
but for now just think it's a simple random walk where we pick one of the, uh,
uh, neighbors uniformly at random and we move to it.
And then we wanna optimize the embeddings,
uh, in such a way to encode this random walk statistics.
Basically we want the- the cosine of the angle between the two vectors,
this is the dot product to be proportional or similar to the probability that,
uh, u and v are visited, uh,
uh, on the same random, uh, walk.
So why random walks?
We want to use random walks because they are expressive, they are flexible.
It gives us a flexible stochastic definition of node similarity that
incorporates both kind of local as well as higher order neighbor with information, right?
And the idea is that if a random walk, uh,
starting from node u visits v, um,
with high probability that u and v are similar, uh, uh,
they have kind of similar network neighborhood they are close together with each other,
there might be multiple paths between them and so on.
Um, and what is interesting is that this is in some sense also
efficient because we do not need to consider all the node pairs when- when training.
We only need to consider pairs that co-occur in random walks.
[BACKGROUND] So the supervised,
uh, feature learning, uh,
will work the following.
The intuition is that we find embedding of nodes in
d-dimensional space that preserves similarity.
Uh, the idea is that we want to learn node embedding such that nearby nodes in
the network are clo- are- are embedded close together in the embedding space.
Um, and given a node u the question is,
how do we define nearby?
And we are going to have these definition and,
uh, sub r of u, where, uh,
basically this is n labels the neighborhood,
uh, of u obtained by some random walk strategy or r, right?
So for a given node uh,
u we need to define what is the neighborhood?
And in our case, neighborhood will simply be a sequence of nodes that this,
um, uh, that the random walk starting at u has visited.
So now how are we setting this up as an optimization problem?
Given the graph, uh,
on nodes, uh, V and an edge set E,
our goal is to learn a mapping from the nodes, uh,
to their embeddings and we are going to maximize the following,
uh, maximum likelihood objective, right?
Our goal will be to find this function,
this mapping so basically find the coordinates z of the nodes such that,
uh, the summation over all the nodes, uh,
of log probabilities that given the node u, um, that, uh,
maximizes log probabilities of the nodes that appear in its- uh,
in its local uh, random-walk neighborhood, right?
So we want to basically to sum out- to maximize the sum,
which means we want to make nodes that are, um,
that are visited in the same random walk to be kind of embedded,
uh, close together, right?
So we wanna learn feature representations that are
predictive of the nodes in each, uh, uh, uh,
of the nodes that appear in it's, uh,
random walk neighborhood uh, uh,
N. That's the idea.
So how are we going to do this?
First, we are going to run short fixed length
random walk starting from each node u in the graph using,
uh, some random walks strategy R. Uh,
for each node u,
we are going to collect, uh,
N of u which is a multi set of nodes
visited in random walk starting from node u. Multi-set
meaning that a same node can appear
multiple times in the neighborhood because it may be visited multiple times.
Um, and then we are going to optimize- define
an optimization problem and optimize the embedding, so that, uh,
given node u we wanna be able to predict who are
the nodes that are in its neighborhood and defined again by the random walk.
So we are going to maximize, uh, uh, this,
uh, objective here, uh,
this maximum likelihood objective.
So how can we write this out?
We can write this out the following.
We- we- we write it as sum over all the starting nodes u,
sum over all the nodes that are in the neighborhood of u.
Let's call this nodes v and then we wanna maximize the log probability,
uh, that predicts, uh,
that node v, um,
is in the neighborhood of node U. Um,
and as I said opt- intuition is that we want to optimize embeddings to
maximize the likelihood of a random walk, uh, co-occurrence.
Um, how are we going to do this?
Um, we still need to define this, uh,
probability p and the way we are going to define it is we are going to use
the notion of softmax function that I have introduced a couple of slides ago.
So the idea here will be that what we wanna do is we wanna maximize
the dot product between the node u and node v. So node u is the starting node,
node v is the node,
um, in the neighborhood.
Random walk neighborhood of node, uh,
u we wanna maxima- we wanna to apply softf- softmax.
So this is the exponentiated value of the dot product of the node that is in
the neighborhood divided by sum of
the exponential dot product with all the other nodes in the network, right?
So the idea here is that we wanna assign as much probability mass to- uh,
to these dot product um,
and as little to all other, uh, dot products.
So now to write- to put it all together,
the way we can think of this is- this is- we are trying to optimize this function,
which is a sum over all the nodes for every node,
sum over all the nodes v that are seen on
random walks starting from this node u and then we wanna, uh, uh, uh,
optimize for a minus log probability of these softmax which says,
I want to, uh,
maximize the dot product between the- the starting node u and
the node v that is in the neighborhood and we- and we normalize this over all the nodes,
uh, in the network.
So now, um, you know what does it mean optimizing random walk embeddings,
it means finding this coordinates z, uh,
used here such that this likelihood function is, uh, minimized.
Now, uh, the question is,
how do we do this in practice?
And the problem is that this is very expensive because if you look at this,
we actually have to sum- two nested summations,
uh, over all nodes of the network.
We sum over all nodes in the- of- of the network
here for starting nodes of the random walks.
And here when we are normalizing softmax,
we are normalizing it over all of the nodes of the network again.
So this is a double summation,
which means that it will have complexity order, um, V squared.
So it will be, uh, quadratic in the number of nodes in the network,
and that's prohibitively expensive.
So let me tell you what, uh,
we do to make this, uh, practical.
And the- the issue here is that there is this problem with the softmax that, um,
we need to sum over all the nodes to basically
normalize it back to- to a distribution over the nodes.
So, um, can we approximate this theorem?
And the answer is yes.
And the solution to this is called negative sampling.
And intuitively, the idea is that rather than summing here over all the nodes,
uh, in the network, we are only going to sum over a subset of the nodes.
So essentially, we are going to sample a couple of
negative examples and sum, uh, over them.
So the way the approximation works out is that, um, we, um,
we can- we can view this as an approximation to the, um,
to the- to the softmax function, where we can,
uh, approximate it using,
uh, the following, uh, expression.
We are going to take log sigmoid function of the dot product between u and v. Uh,
this is for the, uh, for the theorem here.
And then we are going to say minus sum of i going from one to k,
so this is our k negative examples, logarithm, again,
of the sigmoid function between the, uh,
st- starting node u,
and the negative, um,
negative sample, uh, i,
where this negative samples,
this negative nodes will be, uh,
sampled at random, but not at ra- at uniform random,
but random in a biased way.
So the idea here is that,
instead of normalizing with respect to all nodes in the network,
we are going to normalize softmax against k random negative samples,
so negative nodes, uh, from the network.
And this negative samples will be carefully chosen.
So how do we choose negative samples?
We- we sample k negative nodes,
each with probabil- probability proportional to its degrees.
So it means that nodes that have higher degree,
uh, will be more likely to be chosen as a negative sample.
Um, there are two considerations for picking k in practice,
which means number of negative samples.
Higher values of k will give me more robust estimate.
Uh, but higher values of K also correspond, uh, to,
uh, to more, uh,
to more sampling again, to higher bias on negative events.
So what people tend to choose in practice is k between 5 and 20.
And if you think about it,
this is a very small number.
If you think network of a million nodes or 100,000 nodes,
rather than summing over 100,000,
uh, nodes, uh, every time here,
you are only summing over, you know,
5-20 nodes in this case.
And this way, your method and your estimation will be far,
um, much, much, much more, uh, efficient.
So, um, now how do we solve this optimization problem?
Uh, I won't go into too much detail,
but these things are today solved with,
uh, stochastic gradient descent.
And I just want to give you a quick introduction to stochastic gradient descent, uh,
two slides that are great lectures, uh,
a lot of really good tutorials on what is stochastic gradient descent,
how does it work and all the theoretical analysis of it.
But essentially, the idea is that if you have a smooth function,
then you can optimizing- optimize it by doing gradient descent,
by basically computing the gradient at- at a given point and then moving, um, uh, for,
uh, as a small step, um,
in- in the direction opposite of the gradient, right?
So this is the- this is the idea here, right?
Your start at some random point.
Um, in our case,
we can initialize embeddings of nodes,
uh, at- with random numbers,
and then we iterate until we converge.
We computed the derivative of
the likelihood function with respect to the embedding of a single node,
and now we find the direction of the derivative of the gradient.
And then we make, uh,
a step in the opposite direction of that gradient,
where, um, uh, this is the learning rate,
which means how big step do we take.
And we can actually even tune the step as we make,
uh, as we go and solve.
Uh, but essentially, this is what gradient descent is.
And in stochastic gradient descent,
we are approximating the gradient in a stochastic way.
So rather than evaluating the gradient over all the examples, we just do it, um,
uh, over, uh, a small batch of examples or over an individual examples.
So what does it mean? Is that rather than, um, uh, evaluating,
uh, the gradient over all the nodes- all the negative nodes, um,
and- or all the neighbors in the neighborhood of a given node,
and then make a- make a step,
we are going to do this only for a, uh,
for a given, uh,
for a given node in the neighborhood.
So basically, the idea is that, you know,
we'll sample node i,
and then for all, uh,
js that in the- in the,
uh, in the neighborhood,
we are going to compute the gradient, uh,
and then make a step and keep iterating this.
Uh, we- of course, we'll get
a stochastic estimates or kind of a random estimate of the gradient.
But we'll be able to up- to make updates much, much faster,
which in practice tends,
uh, to be much, uh, better.
So let me summarize.
We are going to run th-
a short fixed-length random walks starting from each node on the graph.
Uh, for each node u,
we are going to collect, um,
its neighborhood and as a multiset of
nodes visited on the random walks starting from node u.
And then we are going to optimize this embeddings
used- using stochastic gradient descent, which means, uh,
we are going to, uh, uh,
find the coordinate Z that maximize,
uh, this particular expression.
And we are going to efficiently approximate this expression,
uh, using negative sampling,
where we, um, sample negative nodes of each probability proportional to their degree.
And in practice, we sample about 5-20 negative examples,
uh, for, uh, for every node, for every step.
So, um, now, um,
the question that I wanna also talk is,
um, you know, how should we do this random walk?
Right? So far, I only described, uh,
how to optimize the embeddings,
uh, for a given the random walk, um, uh,
R. And we talked about this uniform random walk,
where basically we run fixed-length unbiased random walks starting at each node.
And the idea here is that, um,
there is the issue of this, uh,
type of similarity because in many cases,
it might be to constraint.
So the question is, can we use richer, um, random walks?
Can be made the- can be make random walks more
expressive so that we can tune these embeddings more?
And this is the idea of a method called, uh, node2vec,
where the idea is that we wanna, again,
embed nodes with similar network neighborhoods,
uh, closing the feature space.
Uh, we are going to frame.
The goal is again as, um,
maximum likelihood optimization problem,
uh, independent of the downstream prediction task.
Uh, and the key observation here is that
we have a flexible notion of network neighborhood,
um, which leads to much richer, uh, node embeddings.
And the extension of this simple random walk, uh,
here is that we are going to develop a second-order random
walk R to generate- to generate the network neighborhood,
uh, N, and then we are going to apply the same optimization problem.
So the only difference between deep walk and node2vec is how these, uh,
set of neighboring nodes, um,
is defined and how the random walk is defined.
So the idea is to use flexible, um,
biased random walks that can trade off between the local and global views,
uh, in the network.
Um, and what I mean by local and global,
when you are doing the random walk,
you can think of, uh, for example,
depth-first search, uh, as a way to explore as much of the network
as possible given a given budget of steps starting at node u.
But if you really want to get a good understanding how
the network looks like very locally around node u,
then perhaps you'd want to explore the network more in,
uh, um, breadth-first search, uh, fashion.
So this is really,
um, what this will allow us to do.
It will allow us to trade off or kind of extrapolate between breadth-first search, um,
and depth-first search, uh, type,
um, network exploration, right?
Um, uh, as I said, right, like, um,
in terms of strategies to explore the network neighborhood,
uh, and define the notion o- of, uh,
N from a given starting node,
you- you could imagine you wanna explore very
locally and would give you a very local view of the network,
and this will be just kind of breadth-first search exploration.
What you'd wanna look, perhaps a depth-first search explanation, right?
You wanna have these kind of global, uh,
macroscopic view of the network because you
capture much longer and larger, uh, distances.
All right. And that's essentially the intuition behind node2vec is
that you can- you can explore the network in different ways and you will get,
um, better resolution, uh, you know,
at more microscopic view versus more,
uh, macroscopic view, uh, of the network.
So how are we going to now do this in practice?
How are we going to define this random walk?
We are going to do biased fixed-length random walks are that,
that- so that a given node u generates its neighborhood,
uh, N, uh, of u.
And we are going to have two hyperparameters.
We'll have the return parameter p,
that will say how likely is the random walk maker step back,
backtrack to this- to the previous node.
And then we are going to have another parameter q that
are- we are going to call, uh, in-out parameter,
and it will allow us to trade off between moving outward,
kind of doing breadth-first search,
versus staying inwards, staying close to the starting node in this way,
mimicking, uh, breadth-first search.
And intuitively, we can think of q as
the ratio between the breadth-first and depth-first,
uh, exploration of the network.
To make this a bit more, uh, precise,
this is called a second-order random walk because it remembers where it came from.
Um, and then imagine for example in this- in this case that
the random walk just came from node S_1 to the node W. And now at W,
random walk needs to decide what to do, and there are- you know,
it needs to pick a node,
and there are actually three things that th- the- that the no- walker can do.
It can return back where it came from.
It can stay at the same distance,
um, uh, from, uh,
from where it came as it was before,
so you know, it's one hop,
our W is one hop from S_1,
so S_2 is also one hop from S_1.
So this means you stay at the same distance as from S_1 as you were,
or you can navigate farther out,
meaning navigate to someone- somebody who is at a distance 2 from the previous node S_1.
All right? So because we know where the random walker came,
the random walker needs to decide,
go back, stay at the same orbit,
at the same level, Or move one step further?
And the way we are going to parameterize this is using parameters p and q.
So we- di- if you think of this in terms of an unnormalized probabilities,
then we can think that, you know,
staying at the same distance,
we take this with probability proportional to some constant,
we return with probability 1 over p1,
and then we move farther away with probability one over
q one or proportional with- to- to 1 over q1, all right?
So here p is the return parameter,
and q is walk away type parameter.
So how are we going now to do this in- in practice is essentially,
as the random walker,
let's say goes from S_1 to the W,
now it needs to decide where to go.
We are going to have this unnormalized probability distribution of transitions,
which neighbor of W to navigate to.
We are going to normalize these to sum to one and then
flip a biased coin that will- that will navigate,
that will pick one of these four possible options, right?
Returning back, staying at the same distance, or navigating further.
And now, for example,
if I set a low value of p,
then this first term will be very high and the random walk will most likely return.
Uh, if we, uh,
want to navigate farther away,
we set a low value of q,
which means that S_3 and S_4,
will- will get a lot of, uh, probabilityness.
Um, and that's, uh, that's basically the idea.
And then again, the- the set n will be defined
by the nodes visited by this biased random walk
that is trading off the exploration of farther
out in the neigh- neighborhood versus exploration close,
uh, to the- to the- to the starting node,
um, S_1 in this case.
So that is, um,
that is the- that is the idea.
Um, so how does the algorithm work?
We are going to compute the random walk probabilities first.
Then we are going to simulate, the r, um,
biased random walks of some fixed length l starting from each node u.
And then we are going to, uh,
optimize the, uh, objective function, uh,
the same negative sampling objective function
that I- that I already discussed in DeepWalk,
uh, using stochastic gradient descent.
Um, the beauty in this,
is that there is linear time complexity in the optimization.
Because for every node,
we have a fixed set of random walks.
So it's linear, uh,
in the size o- of the graph.
And the- all these different three steps are also parallelizable.
So can- you can run them- uh, in parallel.
The drawback of this, uh,
no demanding approaches, uh,
is that we need to learn a separate,
uh, embedding, uh, for every node, uh, individually.
So with a bigger networks,
we need to learn, uh,
bigger, uh, embeddings, or more embeddings.
Um, of course, there's- there has been a lot of work, um,
after these, uh, these initial,
uh, papers that have proposed these ideas,
there are different kinds, uh,
of random walks that people kept proposed,
that our alternative optimization schemes, um,
and also different network pre-processing techniques, uh,
that allow us to define different notions, uh, of similarity.
Here are some papers that I linked,
uh, you know, if you are interested,
curious to learn more, um,
please, uh, please read them,
it will be a very good read.
So let me summarize what we have learned so far.
So the core idea was to embed nodes.
So the distances in the embedding space
reflect node similarities in the original network.
Um, and we talked about two different notions of node similarity.
Uh, first one was naive similarity where,
uh, if two node-,
if we could- for example,
do, uh, connect, uh,
make notes, uh, close together if they are simply connected by an edge.
We could, uh, do,
a neighborhood similarity, um,
and today we talked about random walk approaches, uh,
to, uh, node similarity where we said,
all the nodes visited on a random walk from a starting node,
those are, uh, similar to it.
So that's, uh, essentially the idea,
uh, for- uh, for today.
So, uh, now of course the question is,
which method should you use?
Um, and no method wins all cases.
So for example, node2vec performs better on a node classification,
while for example, a link prediction,
some alternative methods may perform better.
Uh, there is a very nice survey,
um, three years ago by Goyal and Ferrara.
That, um, surveyed many of these methods and compare them on a number of different tasks.
Um, er, and generally, you know,
random walk approaches are, uh,
quite efficient because you can simulate,
uh, a limited number of random walks.
They don't necessarily scale to the super big networks,
but they scale to lar- to rel- let say medium-size network.
Um , and, uh, in general, right?
You must choose the definition of node similarity
that best matches, uh, your application.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 03.3 - Embedding Entire Graphs.txt
Now, we are going to talk about embedding entire graphs.
So rather than embedding individual nodes,
we're going to talk about how do you embed,
or find an embedding for an entire graph.
And the method we are going to talk about is based on anonymous, uh, random walks.
So we are going to continue the theme of, uh,
random walks, but this random walks will be anonymous.
So let me explain. All right?
The goal now is to embed a sub-graph or
an entire graph into the embedding space, uh, z. Um,
and, uh, you may wanna do these because you may wanna do for example,
molecule classification to predict which molecules are toxic versus which are non-toxic.
Or you wanna do some kind of graph anomaly detection,
um, and you want to do this in the embedding space.
So the goal is to embed an entire graph or you can think of it
also as embedding a subset of the nodes in the graph.
So first idea, that is very simple and people have tried.
So the idea is that you run standard node embedding,
uh, uh, technique, uh,
like we- what- like we already dis- discussed in terms of node to walk or, uh, deep walk.
And then, just sum up or average, uh,
node embeddings, either in the entire graph,
or in the sub-graph, right?
So the idea is, um, to say,
the embedding of the graph is simply a sum of the embeddings of the nodes in that graph.
And for example, this method was used in 2016, to classify,
uh, molecules, uh, based on the graph structure,
and it was very, um- very successful.
So even though simplistic,
um, uh, works quite well in practice.
An improvement over this initial idea of averaging node embeddings is
to introduce a virtual node to represent an entire graph or a sub-graph,
and then run a standard graph embedding or node embedding technique,
uh, and then think of the- this virtual node as the embedding for the graph.
So let me explain.
Uh, here is the idea, right?
I will create these virtual node.
I will connect it to the set of nodes I want to embed.
Now I can run node to walk on this,
uh- on this, um- on this graph to determine the embedding of this virtual node.
And, uh, now the embedding of the- of the- of the let's say set of nodes s,
is simply the embedding of this, uh, virtual node,
um, in the embedding space as computed by deep walk or,
uh, node to walk.
And of course, if I would want to embed the entire graph, then this,
uh, virtual node would connect to all other nodes in the network.
I'd run, uh, deep walk,
uh, or an node to walk over this,
determine the embedding of the virtual node and
represent the embedding of the entire graph as the embedding,
uh, of the red node.So that's,
um, idea number, uh, 2.
Now, for idea number 3,
we are going to define this notion of anonymous, uh, walks.
And the way we will think of this is that states in
the anonymous walk correspond to the indexes
of the first time when a given node was- was visited,
uh, on the walk.
So for example, here is a- here is a graph,
uh, uh, you know, a small subpart of the graph of interest.
Then here are a few random walks on this graph.
For example, from A we go to B,
C, B, uh, and C,
or another random walk starts at C, goes to D,
goes to B, goes to D,
and back to B.
But then we are not going to represent the random walk as a sequence of nodes it visits,
but a sequence of times when node was first visited.
So for example, these two random walks,
one and two here, get the- get the same representation.
It is one because A was visited at step 1,
then it's two because B was visited at step 2.
Then it's three because node C was visited at step 3,
then we visited B again,
but B was already visited so it doesn't get a new- new,
uh, index, but it gets value 2.
And then we went back to C. So, um,
so again, um, we have, uh, number 3.
And then this other random walk that started at C went to D,
B, D, B, um,
gets actually the same sequence, right?
C gets one, uh,
D gets two, B gets three.
Then we go back to D. So it's again two,
and then we go to B and it's again three.
And then for example, this other random walk now has
a different anonymous representation because we go from A to B to A to B to D, right?
So it's 1, 2, 1, 2, uh,
3 because at time 3 or, uh,
node D was visited as a third node, uh, in the graph.
So, uh, this anonymous walks,
uh, basically this is,
uh- this is- they are agnostic to the identity of the nose,
of the nodes visited.
That's why they're called anonymous.
Um, and this means that node- that random nodes that have visited the different nodes,
but kind of in the same order that get the same anonymous walk representation.
Now, uh, you may wonder how- how does the number of walks,
uh, increase with the length?
The number of possible anonymous walks increases exponentially.
Uh, here's- here's the graphic for example,
for- there are five anonymous walks of length 3. You know, here they are.
You can basically stay at the same node three times.
You can go- you can stay two times at the same node and then navigate to the second node.
You can go from first node to the second,
back to the first.
You can go from first to the second, stay on the second,
or you navigate from node 1 to node 2 to node 3.
And these are the five possible anonymous walks of length 3.
And then, you know, of length 4, uh,
it will be- it will be more,
that would be 15.
And of length 12, that would be,
I know, four million as it shows here.
So how are we going now to embed a graph?
One idea is to simply simulate anonymous walks of length L, uh,
and record their counts and then represent
the graph as a probability distribution over these walks.
So for example, if I pick anonymous walks of length 3,
then we can represent a graph as a five dimensional vector
because there are five different anonymous walks of length 3.
As I explained earlier,
where, uh, you know,
the embedding of the graph, uh,
the ith coordinate of that embedding is simply the probability that anon-, uh,
the probability or the fraction of times anonimou-
anonymous walk of type I has occurred in
the graph G. So now this would basically mean that we are
embedding a graph as a five dimensional representation.
Now if you want higher number of dimensions,
you increase the length, uh,
of the anonymous, uh, walk.
That's the idea for the- for the anonymous walks, uh,
and how you can basically count them and then have a probability distribution over the,
uh, fraction of times each anonymous walk occurs on your graph.
You, uh, uh, said that the dimensionality of the presentation by basically setting,
uh, the- the length of the,
uh, anonymous, uh, walk.
Now, of course the question also becomes,
how- how many random walks do you need?
And that is very nice mathematical formula that
tells you how many anonymous walks you wanna
sample such that your estimates in
the frequency or the probabilities of occurrence are, uh, accurate.
And you can quantify accuracy by two parameters,
Epsilon, um, and, um, and Delta, uh,
where basically we say we want the distribution of these, uh, uh,
probabilities of anonymous walks to have error of no more than Epsilon,
with probability, uh, less than Delta.
You can plug in these two numbers into
the following equation and that will tell you the number of,
uh, anonymous walks you may wanna,
uh, you may need to sample.
So for example, if you consider anonymous walk of length 7,
there is 877 of them.
If you set the Epsilon to 0.1 and Delta to, uh, 0.01,
then you would need to sample about 120,000, uh,
random walks that you can estimate this probability distribution over this,
er, 877, uh, different, uh, anonymous walks.
So that's the idea in terms of anonymous walks.
And, um, we can further, um,
enhance this idea, uh,
to actually learn embeddings of the walks themselves.
So let me explain how we can do this.
So rather than simply representing each walk by the fraction of time it occurs,
we can learn an embedding Z_i of anonymous walk W_i.
And then we can learn also our graph embedding
Z- Z_G together with the anonymous walk, uh, embeddings.
So this means, uh,
for a- for a- we are going to learn- the number of embeddings we're going to learn,
will be the number of anonymous walks plus 1 because
we are learning an embedding for every anonymous walk plus the embedding,
uh, for the graph.
Right? So again, how are we going to learn
these embeddings of graph and the anonymous walks?
We can- we do this in a very similar way to what we did for DeepWalk or node2vec.
We wanna embed walks so that,
uh, walks adjacent to it can be predicted.
So let me explain the idea.
So the idea is that again,
we will have this vector G that describes the graph.
We are going to have the, uh,
and this will be the embedding of the entire graph we are going to learn.
Let's say we start from some no- node 1 and we sample anonymous,
uh, random walks from it.
So then the idea is to learn to predict walks that co-occur in some, uh, uh,
uh, Delta window size, um, um,
around as we are sampling this random walks,
uh, from a given node.
So the idea is that the objective function is we wanna go,
um, over this window size Delta,
where this W_t are random wa- are anonymous random walks that all start at,
uh, at a starting node 1 and,
uh, Z_G is the embedding of the graph.
And now we basically sum these-
these objective over all starting nodes, uh, in the graph.
So the idea is that we will run T different random walks from each, uh,
node u of length l. So now our notion of neighborhood is not a set of nodes,
but it's a set of anonymous,
uh, random walks, uh,
here labeled by W. And then we want to learn to predict walks
that co-occur in a window of size, uh, Delta.
So the idea is that you wanna em- em- estimate
the embedding Z_i on anony- of anonymous walks W_i,
um, and then, uh,
maximize these given objective where basically we go over, uh,
er, all the random walks that we run from the- from the- from the given node.
Uh, this- this is a sequence of random walks that occ- or anonymous walks that occurred.
Our goal is to find the embedding of the graph as well as the embeddings of
the anonymous walks so that we can predict what is the next anonymous walk,
what is the identity of the next anonymous walk that is going, uh,
to occur in these, uh,
in these sampling of anonymous walks.
So essentially this is the same objective function as we used in note to record DeepWalk,
but the difference is the following,
is that when we are defining this notion of neighborhood,
we don't define neighborhood over the nodes that are visited,
but we define it over the, uh,
anonymous walks that occur,
uh, starting from node u.
So here W is an entire anonymous walk,
it's an ID of an anonymous walk.
If you want more details and, uh,
and read more there was- there is a paper link
down here that you can read for further details.
But the idea here is right that- now that we have learned
both the embedding of the node as well as the embedding of the walks,
we can- sorry, as we learn the embedding of the graph,
you can use these, er, er,
Z_G as a- as
a descriptor of the graph and we can use it in downstream prediction task, right?
So to summarize, we obtain the graph embedding, uh, Z_G,
which is learnable after this optimization,
and then we can use Z_G to make predictions for example, for graph classification.
We can use, uh,
the dot product the same way as we were using so far,
or we could use a neural network that takes, er, er,
Z_G as an- as an input into our classifier and tries to predict some label this,
uh, graph G. Uh, both options,
uh, are, er, feasible in this case.
So in this, ah, part of the lecture,
we discussed three ideas about how to embed entire graphs.
First was simply average or sum up embeddings of the nodes in
the graph where the embeddings of the nodes are computed by DeepWalk or, um, node2vec.
The second idea was to create a super-node that spans
the subgraph of interest and that embed that super-node.
And then the idea number 3 was to use this notion of anonymous walk embeddings,
uh, where do we sample anonymous, uh,
walks, um, to represent graph as a fraction of times each anonymous walk occurs,
so that's one idea.
And the second more complex idea was to embed anonymous walks and then,
um, either, for example,
concatenate their embeddings or use these Z- Z_G to define the notion,
uh, of the embedding of the entire graph.
What we are going to, um,
consider in the future is also more, um,
er, different approaches to graph embeddings, and in particular,
many times graphs tend to have this type of community or
cluster structure so it becomes a good question how do we hierarchically aggregate,
um, the- the network, er,
to obtain the embedding,
uh, of the entire graph.
And later in Lecture 8,
I'm going to discuss such a- such an approach that uses,
uh, graph neural networks,
uh, to be able to do this.
So, um, to- to conclude,
to keep kind of towards- moving towards the ends of the lecture,
the next question is,
you know, how do we use these embeddings Z_i of the node for example?
Um, you can use them, for example,
to cluster, uh, the points.
Basically, you take the graph, compute the embeddings,
and then run a clustering algorithm over the embeddings, for example,
the do community detection or any kind of social rolling identification.
Uh, you can use these embeddings for a node classification.
Simply predict the label of node i based off it- it's embedding Z. Um,
and you can also use them for, uh,
link prediction where basically you can think nodes, uh,
i and j and you can concatenate their embeddings and then,
uh, based on the concatenation,
uh, you can make a prediction.
When, um, when you are combining this node embeddings,
you have several options.
As I said, one is simply concatenating them, uh,
another option would be to combine them by doing per coordinate product.
Uh, this is- this is nice for undirected graphs because this operation is commutative.
Um, you can also sum or average them.
Another communi- commutative operation meaning that, you know,
probability of link from i to j is the same as from j to i. Um,
or you could measure some kind of L2 distance and then make a prediction
based on these kind of different aggregations of the two- of the two embeddings.
If you wanna make predictions in directed graphs,
then concatenation is nice because you are able to
output a different probability depending on,
you know, is it from i to j or from j to i?
For graph classification, node embedding,
uh, ah, can be computed via aggregating, uh,
node embeddings or through anonymous walks or through this anonymous walk embedding plus,
uh, the graph, ah,
embedding approach, uh, that we have discussed.
So, uh, to summarize,
today, we discussed, um,
graph representation learning as a way to learn node and graph embeddings, uh,
independent of downstream prediction tasks and without any manual feature engineering.
We had discussed this encoder decoder framework, where encoder,
you simply an embedding lookup and decoder
predicts the score or network-based similarity,
uh, based on the pos- the embeddings of the nodes.
Um, and we defined, uh,
notion of node similarity based on the random walks.
We discussed two methods,
DeepWalk and node2vec that use the same optimization problem,
the same negative sampling approach, uh,
but DeepWalk uses a very simple random walk,
basically a first-order random walk while node2vec
uses a second-order random walk that can- where you can kind of,
uh, fine tune the way the network is explored.
Um, and then we also discussed extensions, uh,
to, uh, graph embeddings, um,
which means that we can simply aggregate
node embeddings and we also talked about anonymous, er, random walks,
where the anonymous random walk represents the walk not by the identities of the nodes,
but by the time or by the index at
what ti- at what time a given node was first, uh, visited.
So, uh, these are- this is a set of approaches,
um, I wanted to, uh, discuss today.
Um, and thank you very much for watching and,
uh, attending the lecture.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 04.1 - PageRank.txt
It is great to have, uh,
uh, everyone here today, and, uh,
we are going to continue, um,
our investigation of, uh,
machine learning with graphs.
And in particular today we're going to look at how we can represent graph as a matrix.
And, um, how- uh,
we- we will then define the notion of PageRank,
Random Walks and then connect it back to the previous lecture,
when we talked about, uh, node embeddings.
So this is, uh, the plan for, uh, today.
So let's start.
Today, I wanna talk about looking at graph as a matrix.
So we- we are going to investigate the graph analysis and
learning from the matrix linear algebra perspective.
And we are going to treat graph as a matrix,
and this will allow us to, uh,
define node importance via random walks,
and this is the famous PageRank algorithm.
We are then going to extend this into the notion of matrix factorization,
and then we are going to see how node embeddings,
meaning node2vec and DeepWalk are essentially a form of,
uh, implicit, uh, matrix factorization.
Um, and, um, this is the exciting thing here,
as in some sense will bring together graphs,
linear algebra, as well as node embeddings,
uh, that we have, uh,
discussed in the last lecture and show how they are all, uh, closely-related.
So first, let's talk about PageRank,
which is basically the main innovation that let Google Search become, uh, Google Search.
And it was developed by two students here at Stanford Computer Science,
uh, Larry Page and, uh, Sergey Brin.
So let me tell you about, uh,
what- what did they did as PhD students at Stanford.
So first is, we'll- we will talk about
the context of Google and we'll talk about the context of the web,
uh, search and, uh,
thinking about the web as a graph.
So the question is,
what does the web look like,
uh, at the global level, right?
Like how would you represent the web?
What do I mean by the web?
Is I want to represent it as a graph,
where nodes would be web pages and links between
the nodes would be hyperlinks so that you can navigate from one web page to another.
Um, of course, today,
the web has evolved a lot and there is- there is the issue,
you know, what is a node?
We have a lot of dynamic pages created on the fly,
and there is also a lot of the dark web,
so a lot of, uh,
places, uh, that are password protected,
that are inaccessible, uh,
by basically us just browsing the web.
But let's think of the web, you know,
as it was in the older days when we had a set of static pages.
You could imagine, here are some, um, static pages,
uh, that ap- that a- that appear at, uh, Stanford University.
And then, you know, the static pages have hyperlinks, uh,
so basically, links that you can click that move you to the next page, right?
So in the early days, this is how- uh,
how basically web was structured with a lot of
static web pages and a lot of these navigational, uh, links.
Today, many links are transactional,
meaning they are used not to navigate from page to page,
but they are used to make transactions like post something,
comment something, like something,
buy something, and then you get moved to the next page.
But let's today focus on these navigational,
uh, set of links.
Um, and this means that we can now represent the web as
this type of directed graph of web pages and,
uh, hyperlinks, uh, between them.
And the- there are many other places or
other types of information that you can represent in the same way.
For example, you can take citations where nodes would be, uh, papers,
and a paper is citing another paper,
which means that is a directed link from the source paper to the paper it cites,
and this is called citation networks.
And, uh, also you could take let say,
references in- in Encyclopedia like Wikipedia,
and represent them as a graph of how one concept is related or links to another concept.
So the, you know, the Wikipedia as we know it,
um, is a perfect example, uh, of this.
So now the question is,
what does the- what does the web look like?
What is kind of the- the map of the web?
And, uh, how do we- and- can analyze and understand the web,
uh, as a directed graph?
So essentially, we are going to represent this as- as- as-
as a set of no- nodes based vertices,
uh, web pages and a set of directed hyperlinks, uh, between them.
So, um, the important question from the viewpoint of let say,
web searches to understand what nodes on
the web are more important than then the others, right?
Like, um, some unknown domain name might be a priori less important,
less trustworthy than, uh, well-established domain name,
well-established website that a lot of other,
uh, web pages link.
So, you know, thispersondoesnotexist.com versus stanford.edu,
they might have very different a priori, uh, importance.
And because there is this large diversity in the web-graph connectivity,
the question is, could I somehow rank
the pages using the web-graph link structure in a sense,
which ones are more important, more popular,
more trustworthy, and which ones are less important, less trustworthy,
in a- in a sense that then when I,
let say, rank search results,
I can use this as a signal into what to put on the top and what to put on the bottom?
That is essentially the- uh,
the idea and the goal,
how this was developed,
uh, you know, uh, 20 years ago.
So, um, what are we going to discuss today is a set of
link analysis algorithms or approaches to link analysis,
uh, where the goal is to compute importance of nodes in a graph.
So, uh, we are going to talk about PageRank,
then an extension of it called personalized PageRank,
and then a further kind of extensional variant called Random Walk with Restarts.
But essentially, all these three different approaches algorithms under the hood
are the same thing and they only differ in
one slight variation that I'm going to explain.
So we are going to talk about the PageRank algorithm,
the personalized PageRank algorithm,
as well as, uh, a Random Walk with Restarts, uh, as it is called.
So, um, first we- our goal is to,
uh, compute the importance of a web page on the web.
And one way to do this would be to say,
the idea would be that, uh,
we think of links, uh, as votes.
So basically, a page is more important if it has more, uh, links.
And then you could say, is it incoming links or outgoing links, right?
Incoming links are kind of harder to fake
because other people on the web have to link to you,
and outgoing links are easier to kind
of fake because you can just generate them on your website.
So you would say, perhaps,
let's use links-in-links as votes.
So you could say, "Aha, I know stanford.edu has, you know,
23,000 in-links and thispersondoesnotexist.com has one in-link,
so perhaps stanford.edu is more important."
And then in the- in the next iteration of this type of thinking, you can say, "Oh,
but all in-links are not equal,
like an in-link from an important page should count more."
And this is now recursive because you say the- the- the confidence,
the- the amount of a vote I receive from someone depends on their importance.
And that same- that same web page will say, "Aha,
my important depends on the incom- on
the incoming votes of the importances of the pages that link to me."
So kind of, this is now a recursive question because
every no- importance of every node depends on
the importance of other nodes that link to it.
So, um, this is the interesting,
uh, approach carries this recursive nature of this.
So let's- uh, let's work it out mathematically and figure out how, uh, we can do this.
So the idea is that we wanna, uh,
that the- a vote, a link from an important page is worth more.
So the way we formalize this is to say that
each link's vote is proportional to the importance of the source page,
all right, because these are directed links.
And so, if a page i,
ah, has an importance, ah,
let's say r_i, ah,
and it has d_i out-links,
each of its targets,
each of its endpoints, ah,
each link gets, ah, ah,
r_i divided by d_i, ah, fraction of votes.
So essentially, this means every page has some, uh,
importance and then its importance gets equally split along its out-links.
So in this example, for example,
k takes its importance because it has four out-links.
It splits it four ways,
and 1/4 of it flows along this edge.
And then, node i here has, ah,
three, ah, out-links, so, ah, its,
ah, its importance r_i gets split three ways and 1/3 flows over this link.
So the, let's say,
importance of this node r_j is now the sum of the in-links,
ah, that it receives.
So is- so it's r_i divided by 3 plus r_k divided by 4,
because these are the two in-links.
And then, similarly, now r_j,
um, node j, takes its importance,
and it- because it has three out-links,
it divides it three ways and sends it forward to the light blue nodes, right?
And this is essentially the idea here.
So, ah, your importance,
you collect it from the people that points to you.
And then, you take your importance,
you split it, um,
equally among everyone you point to and pass it on to them.
So that's the- that's the, uh,
idea of this flow model of, ah, PageRank.
So, what this means is that a page is
important if it is pointed to by other important pages.
Um, and we can define the notion of a rank r_j of a page or of node j
simply as a- as a summation over- over the nodes i that point to j,
ah, importance of each node i divided by- by its out-degree, ah,
d. And- and this way, um,
we can now written it out, ah,
in terms of this type of, uh, summation.
To give you an example,
if I have these, you know, small, uh,
small example of the web graph, uh,
back in the old days when web was very small, and there were just,
you know, three web pages on the web,
and perhaps this is the hyperlink structure of that, uh,
web graph, um, then you could say,
Aha, here is how I can write this out, right?
Importance of y is half of the- half of the, uh,
importance of y, because of the self-loop,
uh, plus 1/2 of the importance of a,
because a has two out-links,
one to y and one to m. The importance of a is, ah, you know,
it's importance of m,
because m has one out-link and it's, uh, uh,
half importance of y,
because y has two out-links,
one to a and one self-loop.
And then, importance of m is simply half importance of a,
because again, a has two, uh, out-links, right?
So now, uh, you can say, Aha,
I have three unknowns and I have three equations,
of why don't I solve this using,
uh, uh, Gaussian elimination?
Basically, why don't I solve this as a system of equations?
And in principle, yes,
you could- you could do this.
You need the fourth constraint that you could say, Oh,
these importances have to sum to, uh,
to be equal to one,
and you could use a large-scale equation solver,
but this is kind of a bad idea.
Nobody does it this way because it's not scalable.
So there exists a much more elegant way to solve this system of equations,
and I'm going to show you, uh, how to do that.
So, uh, to be able to do that,
we are now not- we will stop looking at the graph as these set of nodes and edges,
but we are going to represent it as a matrix.
And we will define this notion of stochastic adjacency matrix M,
where if a page j has,
uh, out-degree d_j, then,
you know, in the entries of the- of the- of the matrix,
so if i poi-, i- if j points to i,
then the entry i_j of the matrix M will have the value of 1 over d_j.
So this means this matrix is what is called column stochastic, right?
Every column represents two, uh,
out-links of the node,
um, and, uh, each n,
number of non-zero entries,
is the number of out-links of that node j,
and the value is 1 over,
uh, d su- d_j.
So it means that these entries,
because there is d of them- d_j of them,
and each one has value 1 over d_j,
they will exactly say, uh,
be, um, say, uh,
uh, summed up to have value one.
So this is why this is called column stochastic,
because every column of this matrix sums up to one.
So it means every column you can think of it as
a probability distribution over the neighboring nodes, right?
So this is now the definition of matrix,
uh, m. So now,
we are also going to define this notion of
a rank vector that will have one entry per page,
and the- the entry i of this vector r will be the importance score of page i.
All right, so, ah, basically,
matrix M has size n by n number of nodes,
and then ve- vector r has also,
uh, the size number of nodes,
where for every node there is an entry telling-
telling or storing how important is that node.
Um, and one other thing we will do is we are going to say that
the entries of the vector r have to sum to one.
So again, the way you can think of this is that this is a probability distribution of,
uh, over the nodes, uh, in the network.
And then what is interesting is that the equations I- I showed, uh,
on the previous slide can actually now be written in
this very simple matrix form that r equals M times r, right?
So basically, you say, Aha, you know,
entry of, uh, of, uh,
j is simply a sum, um,
over the- over the nodes i that,
uh, point to the,
uh, to the node j.
And in the matrix form, this is how, uh,
you would save it- say it because you basically go over the- over
the row and sum of the importances of the nodes, uh, that points to you.
So, um, you know,
these two representations are equivalent.
We can write it as a- as an equation,
or we can write it in this, uh, matrix form.
So now, let me just give you an example how this would look like.
Here is my previous, you know,
example of the web graph on three nodes.
Here is my system of equations that I had before.
And now, this is my, er,
stochastic adjacency matrix, um,
meaning that you notice that every column now sums to one.
Um, you, um, and then the way I could write the- the- the system of
equations is simply to say r equals M times r. And if you- if you see this, for example,
y is, you know, 1/2- 1/2, uh, uh,
of r_y plus 1/2 of,
um, uh, r_a, which is exactly what this equation says, right?
So basically, these two representations, uh, are equivalent.
So now, I want to kind of stop this mathematical discussion and give you some intuition.
And the int- intuition here will be
the connection back to random walks that we talked about,
uh, in the last lecture.
So, let me make a connection to random walks and see how
this mathematical formulation of this Gaussian system of equation,
um, how- how is that really corresponds to random walks.
And this is so fascinating,
because it's mathematically kind of the same thing,
algorithmically, the same thing,
but we can think about it in so many different ways and we can use
so many different intuitions about the same kind of underlying,
uh, object or underlying thing.
So let's, uh, let's kind of, uh, uh,
pause a bit and think about, uh,
this hypothetical case of a random surfer, uh, on the web.
So the idea is a random surfer would be a- would be
a person who basically randomly navigates the web pages of the web.
So this is, you know, somebody or this surfer who at,
you know, at some time t is one of the pages on the web.
And then because this person is, let's say, I know,
so bored, they decide to randomly surf the web.
What this means is that when they are at page i,
they- then they at next time stay- they decide to pick an out-link of this page i, uh,
uniformly at random and- and click it and this means they
navigate now to the new page, uh, j, right?
So the idea is, again, I'm, uh, uh,
re- I have this random walk, this random surfer,
that whenever the surfer arrives to a page,
picks one of its outgoing links uniformly at random and navigates, uh, to the target.
And this process, ah,
you know, repeats indefinitely.
It kind of runs, ah, infinitely long.
So let's try to ask where is the web surfer, right?
So this random surfer surfs the web,
at what node is this web surfer?
So let's say that p of t is a vector
whose ith coordinate is the probability that a surfer is at
page i at time t. So what this is
means is that p of t is a probability distribution over pages.
It tells me with what probability is a s- a web surfer at
that given page at time t. So now,
let's work it out and ask, Okay,
so how- how likely- how can I think about this page j, right?
This page j, the way we formulated it in the, um, in the,
ah, in our, uh,
PageRank algorithm is to say that there are these, uh, pages i.
Each page i has importance.
And then, you know,
1 over d out-degree of this page i gets sent to the,
uh, to the page, uh, j.
And this is the key to be able to
make the correspondence between the random walk and these,
uh, flow-based equations that I talked about.
So let me tell you how to think about this.
So we want to compute where is the surfer at time t plus 1.
And basically, the way we know where is surfer at time t plus 1 is the following, right?
Is, um, the surfer follows links uniformly at random.
So this means that,
um, wherever is the surfer at time t,
the surfer will- will pick, uh,
out-links uniformly at random and navigate over them.
So the way you can say, Aha,
how likely is surfer at time t plus 1 out-link j, the, ah,
the answer to that is it is the- the likelihood that the surfer was what,
at node i_1 and then, you know,
picked a random link out of i_1 to arrive to j plus the probability they were at, uh,
i_2 and picked a random out-link that led them to j,
or they were at i_3, and again,
they were lucky to pick the right out-link to get them to j.
So the way you can write this out is exactly the following.
You say this is the stochastic, um, um,
ah, adjacency matrix that we talked about.
This is the probability vector where the- where
the random walker was at the previous time step,
and this gives you now the probability distribution of where the walker,
ah, will be in the next time step.
Right? So, um, you notice how now we can
basically say the- the random walker started somewhere,
we multiply it with matrix M,
and we get the probability distribution
where the walker is going to be in the next time step.
So now, let's suppose that this random walk process reaches a steady state.
What do I mean by that?
Is, um, that the- that
this probability distribution of where the random walker is, converges.
So I'm saying M times probability- the random walker,
ah, probability distribution of the random walker at time t equals now, right?
To probability where- probability distribution where-
over where the random walker is going to be at the next time step.
And let's just assume that this is actually equal to where they- where they used to be.
And technically p of t is called a stationary distribution of a random walk, right?
So random walker walks around so long that kind of time doesn't really matter,
and this distribution of where the random walker is kind of stabilizes,
uh, over the graph.
And, ah, now, um,
why is this interesting?
Because in our original,
uh, PageRank vector that we talked about,
we said r equals M times r. And notice what I've wrote here is basically,
p of t equals M times p of t. So now,
because we have this correspondence,
this means that r is a stationary distribution of the random walk.
So basically, it means that this flow
based equations can be interpreted based on the flow,
or can also be interpreted as this intuition that there
is a random walker walking around infinitely long over the graph,
so that after some time, basically,
it doesn't matter where the walker- random walker started,
but it's really all about this distribution of where the random walker is,
is going to converge to this stationary, uh, distribution.
And in order for you to compute the stationary distribution,
is the same problem as it was before to solve that system of equations or
to solve this recursive equation of r equals m times r. Right?
So, ah, this is the interesting, ah,
correspondence that I wanted,
ah, to talk about, right?
And now, this is very interesting because this,
ah, also links back to the lecture number 2,
when we talked about eigenvector centrality, and we said,
let adjacency matrix a be,
um, an adjacency matrix of an undirected graph.
As an example is here.
And we said, ah, eigenvectors of- eigenvectors, ah,
centrality of a, um,
of a- of a graph with adjacency matrix A,
simply satisfies this equation.
We said A, ah,
we said Lambda times c equals,
ah, A times c, right?
Where, ah, c is a vector and Lambda is of value.
And solutions to these equations, um,
where we have a scalar and a vector, um,
are called, um, eigenvector eigenvalue, ah, equation. So right?
So c in this case would be an eigenvector,
and lambda would be an eigenvalue.
So what this means is that the eigenvector centrality that
we talked about is- is- has very similar structure to the,
ah, to the PageRank equation, right?
Here we are using this matrix M,
that is stochastic matrix,
but we have basically the same equation, ah,
than what we had in the eigenvector centrality.
The only difference is that we have this, ah,
factor Lambda, and that we have this,
um, adjacency matrix A,
rather than the stochastic matrix M. So that's, ah,
the connection now back to, ah,
lecture 2, which is also, ah, super fascinating.
So now, lets keep, uh, working.
How are we going to solve this, right?
So if I write this r equals M times r,
and I include, ah, you know,
implicitly basically a constant one here,
then notice that this is also an eigenvalue eigenvector problem, right?
Now, ah, r is an eigenvector that corresponds to the eigenvalue, ah,
one of these stochastic, ah, matrix, ah,
M. So this is- this is very interesting, right?
Because this rank vector r, as I said,
is the eigenvector of the stochastic adjacency matrix M,
with the corresponding eigenvalue one, right?
This is the Lambda constant from the previous,
uh, slide, implicitly one.
Um, and now, you know,
what is the intuition of the- of the eigenvector?
So the idea is that imagine you start from some vector,
ah u, and you wanna compute this, uh,
product of M times u,
times M, times M, times M,
times M. So basically you are just taking u and
keep multiplying it with M over and over and over again.
Um, so the question then becomes- right what this will
do if u is the starting distribution of the random walker,
then the question is, what is
the long-term distribution of where the surfer is going to be in my network?
And if you are multiplying with M,
essentially you are making the surfer, uh,
keeps surfing around, uh, longer and longer.
Um, and just to- to tell you, right?
Like this is also interesting because it connects to the notion of ka- Katz,
uh, Katz, uh, centrality,
when we talked about this,
or Katz similarity in the- in the lecture 2,
because there we also said that if you take an adjacency matrix and power it,
it counts the number of paths between a pair of nodes, all right?
So here in some sense we are also, ah,
evolving and making this random walker walk longer and longer,
uh, one step longer whenever we multiply with the matrix m. So what did we learn so far?
We learned that PageRank is a limiting distribution,
a stationary distribution of this random walk process,
that it corresponds to the principal eigenvector of the stochastic, uh,
adjacency matrix M. Principal eigenvector means eigenvector
associated with the eigenvalue 1 of the- of the matrix.
Um, and, ah, also notice if r is the limit of this,
ah, ah, product of M, ah, you know,
of number of multiplications of matrix M and u,
then r satisfies this flow equation that we have,
uh, that we have written up here that, you know,
1 times r equals M times r. So what
this means is that our vector r is the principal eigenvector,
uh, of matrix M and that it corresponds to eigenvalue 1.
So, um, now, we brought three different things together.
We brought things- we brought together the random walk equation,
we've brought together this flow based equation that I've wrote here,
and we brought together this concept of
eigenvectors and eigenvalues from, uh, linear algebra.
And it's super fascinating that this very different intuitions all converge, uh,
in- in the same- in the same spot and they are
just different interpretation of the same underlying mathematics.
So now, what I wanna say is,
how can we solve for this vector r?
How do we determine vector r?
And it turns out there is a very elegant,
uh, and powerful, uh,
algorithm to do this.
It's called power iteration.
Um, and it's- the approach is amazingly simple and amazingly new scalable.
So, um, let me summarize,
um, what we learned so far.
We defined this notion of PageRank that measures importance of nodes in a graph using,
uh, link structure of the graph.
We talked today about directed graphs.
Ah, PageRank models are random web surfer using this stochastic adjacency matrix M,
and this random website for basically,
uh, resides at the page,
picks, uh, uh, out link at random and makes a transition.
And then we said,
how we- how could we compute the stationary distribution of
this random walk process of this random surfer process on the graph?
Um, we saw that PageRank solves this equation,
ah, r equals M times r, uh,
where r can be viewed both as the principal eigenvector of M,
as well as the stationary distribution of this random walk process of the graph.
And this is the fascinating thing, right?
It's- it's just an eigenvector,
but it has this very rich interpretation of the random walker,
random surfer, ah, surfing around.
So, um, this is the summary, ah, so far.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 04.2 - PageRank How to Solve.txt
So what I wanna talk next is,
how do we solve the PageRank equation?
How do we actually compute,
uh, the vector r?
The idea here is to use the method called, uh, power iteration.
And what I would like to do is,
we'd like to do the following.
Right? Given a graph on n nodes,
we want to use an iterate- we will use an iterative procedure that will, uh,
update over time our rank vector r. Um,
and the idea will be that we- we start the procedure by assigning
each node some initial random, uh, PageRank score.
And then we are going to repeat our iterative process until this,
uh, vector r stabilizes.
And the way we are going to measure whether it stabilizes,
we'll say, "Here is our previous estimate."
Now, t runs over the iteration of our algorithm,
where we say this is our previous estimate in the vector r. This is
our new estimating vector r. And if the coordinates,
the entries in the vector don't change too much,
they change less than epsilon, then we are done.
And the equation we are going to iterate is written here.
Basically, node j will say,
"My new estimate of my importance is simply,
take the estimate of the nodes i that point to me, uh,
from the previous step, you know,
divide each one by the out-degree of node i,
sum them up, and that is my new importance.
And we are just going to iterate this, um, you know,
a couple of times and it's ah,
it's guaranteed to converge to the,
uh, to this- to the solution,
which is essentially saying this is guaranteed to find
me the leading eigenvector of- of the underlying,
uh, matrix, uh, M. So, um,
the method that do- does this, what I explained,
is called power iteration, where,
um, basically, the way we are going to do this,
to just write it again in the very simple form,
we initialize vector r, you know,
let's call in- call it initialization to be times zero, to be simply,
uh, you know, every node has,
let's say the same importance or you can assign the random importances.
And then we will simply iterate, you know,
new estimate of r equals M times
previous estimate of r. And we are going to iterate this equation, uh,
for long enough until the- the, uh,
ent- the entry wise differences between estimating
the previous round and in the next round when the sum of these differences,
uh, is, uh, less than epsilon.
Again, notice that this equation is exactly this equation, right?
This is just written in two different ways,
but it's exactly the same thing.
Um, one last thing to say is,
you can use the- what is called L_1 norm,
so the sum of the absolute differences here.
You could also use, let's say, Euclidean norm.
So sum of the squares of absolute differences, uh, if you like.
Um, and generally, it takes about 50 iterations.
You have to compute about 50, uh,
50- you have to compute this product 50 times before you
reach this stationary distribution or this, uh, limiting solution.
So basically, you can compute PageRank by a couple of,
uh, matrix vector multiplies, uh, and you are done.
And this is important because, you know,
Google is computing this PageRank every day over the entire web graph,
write off tens of billions of- uh,
of nodes, uh, and I know hundreds of billions of edges.
All right? So this is really, really scalable and you can- you can compute
it on the- on the graph that captures the entire web.
So, uh, to give you an example, again, uh,
power iteration method, you know,
written again, in a different way.
Our matrix M, our set of flow equations.
Uh, and what I'm going to show you here is the iterations of the algorithm,
where we set node importances to be simply, ah,
1/3 at the beginning and now we multiply
it with matrix M. And you know after we multiply once,
here are the new values.
We multiply the second time,
here are the new values and,
you know, the third time,
and as we keep multiplying the- the- uh,
the value of- values of vector r,
then we'll converge to a stationary, uh,
vector so that m equals - r equals N times r. Um,
and the final importances would be 6/15, 6/15, and 3/15.
It means y and a will have importance of 6/15,
and m will have a lower importance of, uh, 3/15.
So, um, this is- this is what PageRank is going to, uh, give us.
So now that we, uh,
have seen these equations and everything seems beautiful,
uh, we need to ask a few questions.
So first question is, does this converge?
Second question is, does it converge to where we want?
And the third question is,
are the results reasonable?
Right? So basically, what I said right now is create the, uh,
graph represented as this matrix M around this uh, uh,
uh, iterative- power iteration procedure,
it will converge in about 50 steps and you will get your,
uh, vector r out of it.
Let's look at this, uh,
a bit more, uh, carefully.
So it turns out that with what I explained so far,
there are two problems.
The first problem is that some pages are what is called dead ends.
They have no out links.
And It turns out that for such web pages,
the importance, the votes kind of leak out.
I will tell you what I mean by that.
And then there is also a second problem called a spid- spider traps,
where all outlinks are within the same group
and the- the spider traps eventually absorb all- all importance.
So let me now give you an example and you will see what's happening.
So, um, first, spider traps.
Uh, here, in this case, right,
we have a links to b,
and then b has a self-loop.
So if you run this, um, er,
power iteration of what an adjacency matrix describing this graph,
what will happen is that in the end, a will have,
um, importance zero, and b will have importance 1.
And if you think of this, why is this happening is
because wherever the random walker starts,
uh, you know, it will traverse this edge and get into b,
and then it is going to- to- to be stuck here in b forever,
so really, you know,
after some number of time,
the random walker is- is in node b with probability 1,
and can never go back to a.
So this is called a spider trap because the random walker gets trapped, and at the end,
you know, this may be, uh, er,
all the importance will be,
uh, kept here in b.
And you can imagine these that, you know,
even if- if you have a super huge graph here, eventually,
the random walker is going traverse over this edge and then
be stuck forever in this, uh, self-loop.
So that's the problem of spider traps.
Um, and then here is the problem of dead ends.
The problem of dead ends is now that node b has no outlink.
And what this means that if you would simply create an adjacency matrix for this graph,
run the power iteration,
it will converge to all zeros.
And intuitively, why is this happening is
that as soon as the random walker gets to node b,
the random walker has nowhere to go so it kind of falls off
the cliff and the- and it gets lost.
Right? And this way,
the- the dead ends kind of this importance doesn't yet sum to one anymore,
but it leaks out, uh, of the graph.
So, um, this is- uh,
these are two problems that we are going to address.
And, um, you know, what is the solution?
The solution is this notion of,
uh, random jumps or teleports.
So the solution for spider traps is that
we are going to change the random walk processor.
So basically saying at every time
the random walker will not only choose a link at random,
but can also decide to teleport itself.
So let me explain what this means.
So we are going to have one parameter beta
that will allow random walker to do one of the two choices.
With probability beta, you know,
the random walker will decide and follow a link at
random the same way as we discussed, um, so far.
But with probability 1 minus beta,
the random walker is going to jump teleport to a random page.
And the common values of beta usually are between 0.8 to 0.9.
So it means that, you know,
if a random walker gets stuc- stuck in a spider trap,
it will stay here for a few steps,
but it's- eventually, it will- it will- it will be able to teleport out, right?
Because with some smaller probability,
the random walker will say,
"Let me just randomly teleport to a random page."
So it means that out of every webpage,
out of every node, there is a way for you to teleport yourself somewhere else.
So basically, randomly jump somewhere else.
And this is, um,
how now spider traps are no longer the problem because you don't get,
er, trapped, you can always, uh, jump out.
You can always teleport, you know,
the [inaudible] can all you - always kind of give you up.
That is kind of the idea.
Um, how about the dead ends?
The- the- the way you do this is also with teleports.
Essentially, what you say,
if you come to a dead end,
if you come to node m and there's nowhere for you to go,
what you- what you do is you simply teleport with probability 1.0, right?
So, uh, you know,
why were the dead- dead end is the problem?
Dead ends were the problem because m has no outlinks,
so our column of this, uh, matrix M,
the column stochastic adjacency matrix is not- is the- the- the column stochasticity is,
uh, violated because column for node m does not sum to 1 because m has no outlinks.
So what do we do is,
we fix this by basically saying,
when you arrive to node m,
you can- you can randomly teleport wherever you want,
you can jump to any node.
So this means in some sense,
now- now m is connected to all other nodes in the network,
including itself, and, um, you know,
the random worker can choose any of these links with equal probability.
And this, uh, solves the problem of dead ends.
It essentially eliminates them.
So why do teleports solve the problem, right?
What are- why are dead ends and spider traps a problem,
and why do teleports solve both of them, right?
Spider traps, in some sense,
are not a mathematical problem,
in a sense that the eigenve- the eigenvector is still well defined,
the power iteration is going to- to converge,
everything is fine, uh, mathematically.
But the problem is that the PageRank score is not what we want, right?
We don't want to say there is one page on the web that is important,
uh, has all the importance,
and everyone else is zero important, right?
So the solution here is to add teleports.
This means the random walker never gets,
uh, trapped in a spider trap, um,
and it will be able to teleport itself out in a finite number of steps,
which means that all the nodes on the web will now have some importance.
So this basically is,
um, solves us, uh, this particular issue.
So spider traps are not a mathematical problem,
but a problem that PageRank,
uh, value is not- becomes not what we want.
And then dead ends are a problem mathematically
because our matrix M is not column stochastic anymore, uh,
and our initial assumptions are not met, so power iteration,
as a method, does not- does not, uh,
does not work, uh, and does not converge.
So the solution here is to make the column-
the matrix column stochastic by always teleporting when there is nowhere to go, right?
Whenever you come to a node without any outlinks,
you always randomly teleport,
um, and this is now means that basically,
the same solution of teleports both give-
gives us PageRank the way we want it to define intuitively,
and also fixes the underlying mathematical problems
that all these concepts that I discussed, um, are well-defined.
So what is the final solution or the Google solution to this problem?
Um, the solution is that at each step,
the random walker has two options;
you know, it flips a coin,
and with some probability Beta,
it's going to follow an outlink- outlink at random,
and with the remaining probability,
it's going to jump to a random page.
So the way now our PageRank equation that was defined by,
um, uh, Sergey and Brin, um,
or Page and Brin, uh,
back, uh, in, uh,
1998, is the following: we say the importance of node j
equals the Beta times the importances of node i that,
uh, point to it, right?
Divided by their outdegrees plus 1 minus Beta,
1 over N. So the way you can think of this is to say,
if a random if a- how likely is a random walker likely to be at node j right now?
It- with probability Beta,
it decided to follow on, uh,
an outlink, and this means it was,
at node i, with what- with some probability r_i,
and it decided to follow an out- outlink towards,
uh, node j, following, you know,
picking the right outlink out of the d_i outlink has the probability 1 over d_i,
so that's what's happening here.
And then we say, oh, and also,
the random walker could come to the node j,
um, by basically teleporting,
1 minus Beta is probability of teleporting.
Now, how likely is the random walker to land at node j?
Node j is just one out of N nodes,
so the probability that it landed at specific node j is 1 over N. And uh, this is essentially,
uh, the PageRank, uh,
equation and iteration one can run.
Uh, just note that this formulation here assumes M has no dead ends.
The way you can do is you can pre-process matrix M to remove all the dead ends,
um, and or- or explicitly follow random teleports with probability 1.0 out of dead-ends.
So that's how you can, uh, fix this.
But you can see again, this is very fast and very simple, uh, to iterate.
So I just gave you the equation in this,
um, the, uh, flow-based formulation in some sense.
You can also write it in a matrix form,
where you say my new matrix, uh, uh, G, right?
So this should be G equals Beta times the stochastic matrix M plus 1 minus Beta,
um, times the, uh,
the matrix that has all the entries, uh,
1 over N. So this is the random teleportation, uh, matrix,
and this is the- the transition matrix over the edges of the graph.
Um, and then you have this, again,
recursive equation that r equals G times r,
and you can iterate this, uh,
power i- power iteration would still work, um,
and if you ask what should be the Beta value that I- that I set, in practice,
we take Beta to be between 0.8, uh, and 0.9,
which means that you- the random walker takes
about five steps on the average before it decides to jump.
Uh, just to be very clear,
the random walk is just the intuition,
and we'd never simulate the random walk, right?
In the previous lecture,
we actually said, "Let's simulate the random walk."
Here, we don't simulate the random walk,
but, uh, in some sense,
we think of it as being run infinitely long,
and then we say- we show that actually,
we can compute this infinitely long random walk
by basically solving this recursive equation by basically
computing the- the leading eigenvector of this graph-transformed matrix,
uh, that I call,
uh, G, uh, here.
So the random walk is just an intuition because we never truly,
um, we never truly, uh, simulated.
So to show you how this works, here is my, uh,
little graph on three nodes, uh,
here's the matrix M. Notice that, uh,
the node m is a- is a spider trap,
so what do I do now is I add also these,
um, random teleport, uh, links,
so I have this matrix, uh, uh,
1 over N. Let's say that my Beta is 0.8,
so now, my new stochastic transition matrix G is written here, right?
It's 0.8 times the, uh,
matrix of link transitions plus point to the,
uh, matrix of random jumps, where basically,
you can think of this that every column says,
if a no- if a random surfer is at a given node,
then this is the probability distribution where the random surfer is going to jump.
And if you add these two together,
you get a new, um,
transition matrix now that includes both traversing over the links of the graph,
as well as randomly jumping.
Uh, here is how you can think of this in terms of transition probabilities.
These are now, in some sense,
transition probabilities of a random walker,
random surfer, um, and then you can multiply,
uh, r with G multiple times.
Here is the r_0, and now,
we are multiplying it over and over and over again, and you know,
after some number of iterations,
it's going to converge,
and it converges to 7 over 33,
5 over 33, and 21 over 33.
So it means that node m in this graph will be the most important,
followed by y, followed by a, right?
And the why m is so important is because it's kind of a spider trap,
but we also are able to teleport out.
Now, if intuitively, this is, uh, you know,
node m kind of collects too much importance,
you can increase, uh, value of beta,
and the importance of node m is going to, uh, decrease.
And just to show you an idea how this-
how this looks like in- in a bit more interesting graph,
this is a graph where node size corresponds to it's PageRank weight,
and also, there is a number that tells you what's the PageRank score of the node?
What do you notice?
For example, why is PageRank so cool?
It's cool because, for example, first,
notice all nodes have non-zero importance.
So even this nodes here,
um, that have no inlinks,
they still have some importance because a random jumper can always jump to them.
Another thing to notice is that for example,
node B has a lot of inlinks,
and it- so that's why it has high importance, right?
Notice that, for example, node E has, you know,
it has five inlinks, six inlinks,
and, uh, node B also has six inlinks.
But because node E gets most of the inlinks from this unimportant pages,
its importance is, you know,
eighth, versus B, who is 38,
so B is much more important because it gets, uh,
inlinks from these other nodes that have higher importance than this,
uh, little blue nodes.
Um, another thing to notice is,
for example, node C has only one inlink,
but because it gets it from this super important node B,
its importance is also very, very high, right?
You see, for example,
also that, uh, here,
node E has some,
uh, s- uh, you know, some importance,
uh, D has less,
uh, F has less, uh,
they both have the same importance, D and F,
because they both get one inlink from node E. So notice how these, uh,
importances are very nuanced,
and they take a lot of different considerations into account that all make sense,
in a sense of I want a lot of inlinks,
I want inlinks from, uh,
important nodes, even if I have one inlink,
but some were the very important links to me,
that means I am very important,
um, and so on- so on and so forth.
So, uh, this is why this notion of PageRank is so,
so useful, and also,
there is a lot of mathematical beauty,
um, uh, behind its, uh,
its definition, and we can efficiently compute it,
uh, for very large-scale, uh, graphs.
So to summarize, we talked about how do we solve the PageRank, uh, scores.
We solve them by iterating this, uh,
equation, r equals G times r, um,
and this can be efficiently computed using
power iteration of the stochastic matrix G, um,
and adding uniform teleportation solves both the issues with dead ends,
as well as the issue with spider traps

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 04.3 - Random Walk with Restarts.txt
So we are going to talk about random walk with restarts and personalized PageRank.
And this will be extensions of our initial,
uh, PageRank idea that we have just discussed.
So let me give you an example how this would be very useful.
So we are going to talk about a problem of recommendations.
Imagine you- you have a set of users, customers,
uh, and a set of items,
uh, perhaps products, movies.
And then you create interactions between users and items by
basically saying a given user perhaps purchased a given item,
or a given user watched a given movie, right?
So this is now our bipartite graph representation,
uh, here of this user to item relation.
And then what we want to do is somehow measure similarity or proximity on graphs.
Why is this useful?
This is useful because if you are a online store or if you are Netflix,
you want to ask yourself,
what items should I recommend to a user who,
you know, purchased an item Q.
And the idea would be that if you know two items, P and Q,
are purchased by a lot of, uh, similar users,
a lot of other users have, let say,
bought or enjoyed the same- the same item,
the same movie, then- then whenever a user is looking at item Q,
we should also recommend, uh,
item P. So now,
how are we going to quantify this notion of proximity or
relatedness of different items in this graph.
So the question is, for example,
if I have this graph as I show here,
and I have items, you know,
A and A prime and B and B prime.
The question is which two are more related?
So you could do- one thing to do would be to say,
you know, let's measure shortest path.
So A has a shorter path than B to B-prime,
so, you know, A and A prime are more related.
However, the issue becomes,
is that- that you could then say, oh,
but if I have another example,
let's say this one where I have C and C prime.
And now C and C prime have to users that
bo- that both of let say purchased these two items,
then C and C prime intuitively are more related,
are at closer proximity than A and A prime, right?
So now, the question is,
how would I develop a metric that would allow me to kind of say,
hi, it's kind of the shortest path,
but it's also about how many different, uh,
neighbors you have in
common and how many different paths allow you to go from one, uh, to another.
And they- the idea here is that, er,
PageRank is going to solve this because
in this third example, you know, if you would just say,
let's count common neighbors, then let say, uh,
C and C prime are related as D and D prime.
And again, this is- um,
this is perhaps intuitively not what we want because, er,
you know item D,
this user has enjoyed a lot of different items as well.
This other user has enjoyed a lot of different items there.
So this relationship is less strong than the relationship here because here,
it's really two items,
two items that- that's all- that's all there is.
So, you know, how could we capture this
mathematically algorithmically to be able to run it on networks?
And, uh, this is where the notion of extension of PageRank happens.
Um, so PageRank tells me
the importance of a node on the graph and ranks nodes by importance.
And it has this notion of a teleport where we discuss that- that,
um, a random surfer teleports uniformly over any node in the graph.
So now, we will have- we will first define a notion of what is
called personalized PageRank, where basically,
the only difference with the original PageRank is that whenever we
teleport or whenever the random walker teleports,
it doesn't teleport anywhere in the graph,
but it only teleports,
jumps back to a subset of nodes S. Okay?
So basically, we say, you know,
there is a set of nodes S that are interesting to the user.
So whenever the random walker teleports,
it teleports back to that subset S and not to,
uh, every node in the graph.
And then in terms of, uh,
you know, er, proximity in graphs,
you can now take this notion of
a teleport set S and you can shrink it even further and say,
what if S is a single node?
So it means that the random walker can walk,
but whenever it decides to teleport,
it always jumps back to the starting point
S. And this is what is called a random walk with restart, where basically,
you always teleport back to the starting node S.
So essentially, PageRank, personalized PageRank,
and random walk with restarts are the same algorithm with one important difference,
that in PageRank, teleport set S is all of the nodes of the network,
all having equal probability.
In personalized PageRank, the teleport set S is a subset of nodes,
so you only can jump to the subset.
And in a random walker with restart,
the teleportation set S is
a simple node- is simply one node and that's the starting node,
our, you know, query node item,
uh, Q, uh, from the previous slide.
So let me now talk more about random walks with restarts.
So the idea here is that every node has some importance,
and the importance gets evenly split among all edges,
uh, and pushed to the neighbors.
And this is essentially the same as what we were discussing in,
uh, page- in the original PageRank formulation.
So in our case, we are going to say let's have a set of query nodes.
Um, uh, this is basically the set S. And let's
now physically simulate the random walk over this graph, right?
We will make a step at random neighbor,
um, and record the visit to that neighbor.
So we are going to increase the visit count of that neighbor.
And with some probability alpha,
we are going to restart the walk,
which basically means we are going to jump back to
any of the query nodes and restart the walk.
And then the nodes with the highest query- highest visit count
will have the highest proximity to the- uh,
to the query- to the nodes in the query nodes, uh, set.
So this is essentially the idea.
So let me now show you graphically, right?
So we have this bipartite graph.
Imagine my query nodes set Q is simply one node here.
Then we are going to simulate,
really, like a random walk that basically says,
I'm at Q. I pick one of its,
uh, links at random,
and I move to the user.
Now, I am at the user.
I pick one of the links at random, uh, move to,
uh- to the- to the other side and I increase the visit count, uh, one here.
And now I get to decide do I restart,
meaning go back to Q,
or do I continue walking by picking one of- one link,
um, to go to the user,
pick another link to go back,
and increase the visit count?
And again, ask myself do I want to restart or do want to continue walking?
So the pseudocode is written here and it's really what I just say.
It's basically, you know, pick a random neighbor for- for- for,
uh, start at a- at a query,
pick a random user,
uh, pick a random item,
increase the revisit count of the item,
uh, pick a biased coin.
If the coin says, uh, let restart,
you'll simply, uh, jump back to the query nodes.
You can jump, uh, uniformly at random to any of them,
or if they have different weights,
you can sample them, uh, by weight.
And that is- that is this notion of a random walk, uh, with, uh, restart.
And if you do this, then you will have the query item and then you will also get
this visit counts and- and the idea is that items that are more, uh,
related, that are closer in the graphs,
will have higher visit counts because it means
that the random walker will visit them more often,
which means you have more common neighbor,
more paths lead from one to the other,
these paths are short so that, uh,
the random walker does not decide to restart,
uh, and so on and so forth.
And this allows us to measure proximity in graphs very efficiently.
And here, we are measuring it by actually, uh,
un- kind of simulating this random walk physically.
But you could also compute this using
the power iteration where you would represent this bipartite graph with a matrix, uh,
M, you would then start with, uh, um,
rank vector, um, uh,
to be- to have a given value.
You would then, uh,
transfer them to the stochastic adjacency matrix with teleportation,
uh, matrix, and then round power iteration on top of it.
And it would, um, converge to the same- uh,
to the same set of, uh,
uh, node importance as we- as we show
here by basically running this quick, uh, simulation.
Um, so what are the benefits of this approach?
Um, this is a good solution because it
measures similarity by considering a lot of different,
um, things that are important, right?
It considers how many connections or how many paths are between a pair of nodes.
Um, what is the strength of those connections?
Are these connections direct or are they indirect?
They also- it also considers the- the degree of the nodes on the path.
Because, uh, the more edges it has,
the more- the more likely we- for the random walker,
is to kind of walk away and don't go to the node.
Let's say that- that we are interested in.
So in all these cases, um,
this is a very- uh,
has a lot of properties that we want.
It's very simple to implement,
it's very scalable, and,
uh, works, uh, really well.
So let me summarize this part of the lecture.
So basically, here, we talked about extensions of PageRank.
We talked about classical PageRank where the random walker teleports to any node.
So, you know, if I have a graph with 10 nodes,
then its teleport set S. You can think
of it is- it includes all the nodes and each node has,
uh, equal probability of the random walker landing there.
This is called PageRank.
Then the personalized PageRank,
sometimes also called topic-specific PageRank,
is basically, the only difference is that now
the teleport vector only has a couple of- of non-zero elements.
And this now means that whenever a random walker decides to jump,
you know, 50 percent of the times,
it will jump to this node,
10 percent to this node,
20 percent to this one and that one.
So that's, uh, what is called personalized PageRank.
And then random walk with restarts is again PageRank.
But here, the teleportation vector is a single node.
So whenever the-the surfer decides to
teleport it always teleports to the- to one, uh, single node.
But mathematically, all these formulations are the same,
the same power iteration can solve them.
Uh, we can also solve, for example,
especially the random walk with restarts by actually simulating the random walk,
which in some cases, might be- might be,
um, faster, but it is approximate.
Um, and the same algorithm works,
only thing is how do we define the set S,
the teleportation, uh, set.
So to summarize, uh,
a graph can naturally be represented as a matrix.
We then define the random walk process over, er, the graph.
We have this notion of a random surfer moving across links,
uh, with- er, together with having a-a way to teleport,
uh, out of every node.
This defined- allowed us to define this stochastic adjacency matrix M that
essentially tells us with what probability
the random surfer is going to navigate to each edge.
And then we define the notion of PageRank,
which is a limiting distribution of a- of the surfer location.
Um, and this limiting distribution of the surfer location represents node importance.
And then another beautiful thing happened is that we showed that this
limiting distributions- distribution can be computed or
corresponds to the leading eigenvector of
the transform adjacency matrix M. So it
basically means that by computing the eigenvector of M,
we are computing the limiting distribution of this, uh,
random surfer, and we are also computing this solution to the system of equations,
of, uh, flow equations where the importance of a node is,
you know, some of the importances of the other nodes that point to it.
So all these three different, uh, intuitions.
So the linear algebra, eigenvector eigenvalue,
the Random Walk intuition,
and these links as votes intuition are of the same thing.
They all boil down to the same optimization problem,
to the same algorithm,
to the same formulation that is solved with
this iterative approach called power iteration.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 04.4 - Matrix Factorization and Node Embeddings.txt
So this now concludes,
uh, the first two parts of the lecture.
And what I wanna do in the, uh,
remaining, uh, few minutes,
10 minutes or so, I wanna talk about
connection to know the embeddings and matrix factorization.
So, uh, let me refresh you what we talked,
uh, what we talked, uh, on Tuesday.
So we talked about embeddings and we talked
about how we have this embedding matrix z that, you know,
the number of rows is the e- embedding dimension,
and the number of columns is the number of nodes,
uh, in the graph.
And this means that every column of this matrix z
will store an embedding for that given node.
And our objective was in this encoder-decoder framework is that we wanna maximize,
um, the- the- the dot product between pairs of nodes,
uh, that are similar.
So if two nodes are similar, uh,
then their dot product of their embeddings of
their columns in this matrix z, uh, has to be high.
And that was the- that was the idea.
Then of course, how did you define this notion of similarity?
We said two nodes are similar if they co-appear in the same random walk,
uh, starting at the given node.
So now, you know,
how- how can we,
uh, think about this more broadly, right?
You could say we define the notion of
similarity through random walks in the previous lecture.
What if we define an even simpler notion of, uh, similarity?
What if we say, two nodes are similar if they are connected by an edge.
Then what this means is we say, "Oh,
I want to approximate this matrix,
say like entry in the mai- UV in the matrix,
say this is either 0 or 1 by this dot product?"
Like I'm saying, if two nodes are connect- u and v are connected,
then I want their dot product to be 1.
And if they are not connected,
I want their dot product to be 0, right?
Um, of course, like if I write it this way at the level of the single entry,
if I write it in the- in the matrix form,
this- I'm writing basically saying,
this is Z trans- Z transposed times Z equals A.
So this is now caused matrix factorization because I take my matrix A and I factorize it,
I represent it as a product of,
um- of, uh, two matrices.
Um, Z- Z transposed and Z.
So essentially I'm saying I take my adjacency matrix, uh, A.
Here is, for example, one entry of it.
I want- because there is an edge, I want, you know,
the- the- the dot product of this row and that column to be value 1.
For example, for, uh, for this entry,
I would want the product of this,
uh, row and this particular column to be 0.
So this means that we take matrix A and factorize it as a product of Z transpose times Z.
Of course, because embedding dimension D number of rows in Z,
Z is much, much smaller than the number of, uh- uh, nodes, right?
So the matrix is very wide,
but not too- too deep.
This means that this exact approximation saying
A equals Z transpose times Z is generally not possible, right?
We don't have enough representation power
to really capture each edge but perfectly.
So we can learn this martix Z approximately.
So what we can see is,
let's find the matrix Z such that, you know, the, uh,
Z transpose times Z, uh,
the values of it are as similar to the values of A as possible.
How do I- how do I measure similarity now between
values is to use what is called Frobenius norm,
which is simply take an entry in A,
subtract an entry in this,
uh- in this matrix Z transpose times Z,
um, and take the square value of it and sum it up.
So Frobenius norm is simply, uh,
a sum of the square differences, uh,
between corresponding entries into, uh- into matrices.
Right? So this is now very similar to what we were
also doing in the previous, uh, lecture, right?
In the node embeddings lecture.
In the node embeddings lecture,
we were not using the L2 norm to- to define
the discrepancy between A and its factorization,
we used the softmax,
uh, uh, function instead of the L2 norm.
But the goal of approximating A with Z transpose times Z was the same.
So the conclusion is that the inner product decoder with note similarity defined by
edge- edge connectivity is equivalent to
the factorization of adjacency matrix, uh, A, right?
So we say we wanna directly approximate A by
the embeddings of the nodes such that if two nodes are linked,
then I want their dot product to be equal to 1.
And if they are not linked,
I want their dot product to be equal to 0.
So that's the, uh- the simplest way to define node similarity.
Now, in random walk-based similarity,
it turns out that when we have this more,
um, nuanced, more complex definition of, uh,
similarity, then, um, it is still- the entire process is still equivalent to
matrix factorization of just- of a more complex or transformed, uh, adjacency matrix.
So, um, here's the equation,
let me explain it, uh, on the next slide, what does it mean.
So it means that what we are really trying to do is
we are trying to factorize this particular,
um, transformed graph adjacency matrix.
So how is the the- uh,
what- what is the transformation?
The transformation is- here is the, um, adjacency matrix.
Here is the, um, one over the- the diagonal matrix,
these are the node degrees.
Um, this is now, uh,
raised to the power r,
where this- r goes between 1 and T where, uh,
capital T is the context window length is actually the length of the random walks that,
uh, we are simulating in,
uh- in DeepWalk or, uh, node2vec.
Um, volume of G is simply the sum of the entries of the adjacency matrix,
we see- which is twice the number of edges.
And the log b is a factor that corresponds to
the number of negative samples we are using in the optimization problem.
So basically, what this means is that you can compute,
um- in this case,
deep walk either by what we talked last time by simulating random walks
and defining the gradients and doing gradient descent or taking the adjacency matrix,
A, of your graph,
transforming it according to this equation where
we both take into account the lands of the random walks,
as well as the number of negative samples we use.
And if we factorize this, uh,
transform matrix, what I mean by this is if we go and, uh,
replace this A here with the transform matrix and then we try to solve,
uh, this- this equation,
uh, then the, uh, the solution to it,
this matrix Z, will be exactly the same as what,
uh- what we- what, uh,
our approach that we discussed in the previous lecture arrived to.
So, um, basically that is a very- very nice paper.
Um, if you wanna know more about this called network embedding as
matrix factorization that basically unifies DeepWalk LINE,
um, and a lot of other algorithms including node2vec in this,
uh, one mathematical, uh, framework.
So, um, as I said,
this random walk-based similarity can also be thought as, um, matrix factorization.
The equation I show here is for DeepWalk.
Um, you can derive similar type of matrix transformation for, uh, node2vec.
Just the matrix is even more complex because the random walk process of node2vec is,
uh- is more complex,
is more, uh- more nuanced.
So to conclude the lecture today,
I wanna talk a bit about the limitations and kind of
motivate what are we going to talk about next week.
So the limitations of node embeddings via this matrix factorization or this random walks,
uh, like- like I discussed in terms of, um,
node2vec, DeepWalk for multidimensional embeddings,
you can even think of PageRank as a single-dimensional embedding is that,
um, we cannot obtain embeddings for nodes not in the training set.
So this means if our graph is evolving,
if new nodes are appearing over time, then, uh,
the nodes that are not in the graph, uh,
at the time when we are computing embeddings won't have an embedding.
So if a newly added node 5,
for example, arrives, let's say, at the test time,
lets say- or is a new user in the social network,
we can- we cannot compute its embedding,
we have to redo everything from scratch.
We have to recompute all embeddings of all nodes in the network.
So this is very limiting.
The second important thing is that this, uh,
embeddings cannot capture structural similarity.
Uh, the reason being is, for example,
if I have the graph like I show here and I consider nodes,
uh, 1 and 11.
Even though they are in very different parts of the graph,
their local network structure, uh,
looks- looks quite similar.
Um, and DeepWalk and node2vec will come up with
very different embeddings for 11 and- and 1 because,
you know, 11 is neighbor with 12 and 13,
while 1 is neighbor with 2 and 3.
So this means that they- these types of embeddings
won't be able to capture this notion of local structural similarity,
but it will- more be able to capture who- who- what are the identities of the neighbors,
uh, next, uh- next to a given starting node.
Of course, if you were to define these over anonymous walks,
then, um, that would cap- allow us to capture the structure because, uh,
2 and 3, um, would- would- basically the identities of the nodes,
um, uh, are forgotten, and then we would be able to,
uh, solve this, uh, problem.
And then the last limitation I wanna talk about is- is that
this approaches cannot utilize node edge in graph level features,
meaning, you know, feature vectors attached to nodes,
uh, edges and graphs cannot naturally be incorporated in this framework.
Right? We basically create node embedding separately
from the features that these nodes might have.
So for example, you know, I use it in
a social network may have a set of properties, features,
attributes where are protein has a set of properties,
features that you would want to use them in creating, uh, embeddings.
And really, what are we going to talk next is,
um, you know, what are the solutions to these limitations?
The solution for- to these limitations is deep representation learning and,
uh, graph neural networks.
And in the next week,
we are going to move, uh,
to the topic of, uh,
graph neural networks that will allow us to
resolve these limitations that I have just, uh,
discussed, and will allow us to fuse the feature
information together with the structured information.
So to summarize today's lecture,
we talked about PageRank that measures important soft nodes in a graph.
Uh, we talked about it in three different ways,
we talked about it in terms of flow formulation,
in terms of links, and nodes.
We talked about it in terms of
random walk and stationary distribution of a random walk process.
And we also talked about it from the linear algebra point of view, uh,
by basically computing the eigenvector, uh,
to the particularly transformed,
uh, graph adjacency matrix.
Then we talked about, um,
extensions of PageRank particular random walk
with restarts and personalized PageRank where
basically the difference is in terms of changing the teleportation set.
Um, and then the last part of the lecture,
we talked about node embeddings based on random walks and
how they can be rep- expressed as a form of matrix factorization.
So this means that viewing graphs as matrices is a-plays
a key role in all of the above algorithms
where we can think of this in many different ways.
But mathematically at the end,
it's all about mat- matrix representation of the graph, factorizing this matrix,
computing eigenvectors, eigenvalues, and extracting connectivity information,
uh, out of it with the tools of, uh, linear algebra.
So, um, thank you very much,
everyone, for the- for the lecture.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 05.1 - Message passing and Node Classification.txt
Thank you so much everyone for attending,
exciting to be here and to talk about,
uh, the next topic in this class.
Today we are going to discuss, uh,
message passing and node classification.
Um, and this is an intermediate topic that we are going to use, um,
uh, so that we can then move to,
uh, graph, uh, neural networks.
Um, so let's talk about how,
uh, we are going to think about this.
So the idea for today is that we are given a network with labels on some nodes.
And the question is, how do we assign labels to all other nodes in the network?
Um, so the example is that in a network,
some nodes are, let's say,
fraudsters or un- untrustworthy nodes,
and some other nodes are trusted, fully trusted.
The question becomes, how do we find other fraudsters or trustworthy nodes,
uh, in the network?
Um, and we have already, for example,
discussed node embeddings method,
um, in lecture 3.
And we could say, let's use this node embedding methods to simply build
a classifier that predicts which node is trusted,
and which node is, uh, not trusted.
Uh, however, today, we are going to think about this a bit differently.
We are going to de- think about this in what is
called semi-supervised node classification,
where we will be given both, uh,
some nodes that are labeled,
let's say labeled with the green and the red color,
and some other nodes that are unlabeled and they will be all part of the same network.
So the question will be,
how do we predict labels of the unlabeled node?
Uh, and this is called,
uh, semi-supervised node classification,
semi-supervised because we are both given the supervised signal, so the labels,
as well as the unsupervised signal,
as well as the non-labels,
uh, all at the same time.
And the idea is that we wanna do this
in what is called a message passing framework, right?
We would like to- basically,
given one big network partially labeled,
we'd like to infer the labels,
uh, of the unlabeled nodes.
Uh, and we are going to do this by doing what is called message passing over the network.
Uh, and the intuition here will be that we wanna exploit,
uh, correlations that exist in the network.
So what I mean by correlations,
I mean that nodes that share labels tend to be connected, right?
So we will have this notion of collective classification,
where basically the idea will be that nodes are going to, uh, um,
update what they believe is their own labels based on the labels of the neighbors,
uh, in the network.
And here, conceptually, this is also similar to what we were discussing, uh, in, uh,
pager link lecture, where the pager link score has- was updated,
uh, based on the scores,
uh, of the neighbors.
But here we are not going to update the score,
we're going to update the belief,
the prediction about what is the label about a given node.
And we are going to talk about three classical techniques.
One is called relational classification,
the other one is iterative classification,
and then last we are going to talk about belief propagation.
And all these methods,
are kind of a bit old school methods,
but they give us a lot of intuition for what we are going to talk in
the next few weeks when we are going to focus
on deep learning on graphs and in particular,
uh, graph neural networks.
So today, kind of as a starter topic into that,
we are going to talk about, um, uh,
these three topics of, uh, collective classification.
So when I said correlations exist in networks,
what I mean by this is that individual behaviors
are often correlated in the network structure.
And correlation means that nearby nodes tend to have the same color,
tend to have the same label.
They tend to be- uh- uh,
belong to the same class.
Um, and there are two reasons, specially, for example,
in social networks, but also in other types of networks,
for why this might be happening.
First is this notion of homophily,
where basically individual characteristics, um,
uh, um mean that people of similar characteristics tend to link each other.
This is notion of homophily from social science.
And then there is also this notion of influence,
where the idea is that, uh,
social connections influence our own characteristics or our own behaviors.
So let me kind of give you a bit of motivation from the social science point of view,
why these correlations exist in networks and why network data,
um, is so useful.
So homophily is defined as the tendency of
individuals to associate or bond with similar others.
And one way to think of this is to say,
birds of feather flock together, so right?
People that are similar tend to bond,
they tend to link with each other.
And this phenomenon has been observed in a vast array of different,
uh, social networks studies, uh,
on a variety of attributes in terms of age,
gender, organization, a little social status, um,
any kind of preferences,
political preferences, food preferences, and so on.
Um, and one example would be, for example,
that researchers who focus, uh,
on the same research area are more li- more likely to collaborate with each other.
Or researchers who focus on the same area are more likely to know each other
or be friends with each other naturally because they attend the same conference,
they interact with each other, and, uh,
connections between them, uh, get formed.
So this is in terms of, um, homophily.
And to give you an example,
this is an online social network,
uh, from a high school, uh, where,
uh, nodes are people,
uh, edges are friendships,
and color denotes their interests in terms of sports and arts.
And what you notice immediately from this visualization is that there
seems to be kind of four groups, uh, of people.
And it seems that they are very much grouped
based on this node color or based on interests, right?
Is that green nodes tend to link with each other and, um,
and ye- uh, yellow nodes,
uh, tend to leak- link, uh, with each other.
So it means that people with same interests are more
likely or more closely connected due to this,
um, effect of or this phenomena called homophily.
Another, um, phenomena, or another, uh,
force that creates these correlations in networks is kind of the other way around, right?
If in homophily we say people have characteristics
and people with similar characteristics tend to link to each other,
the notion of social influence kind of flips the a- the,
uh, the- the arrow in some sense, right?
So it says that social connections can
influence the individual characteristics of a person, right?
So for example, if I recommend my musical preferences to my friends and I'm, you know,
very, very, uh, persistent,
perhaps one of them will- will grow to like my favorite genres, my favorite music.
So this means that now this person just became more similar to me, right?
It means we're very connected.
And I influence them to kind of change their behavior,
to change their preference so that the two of us are more similar.
Which- which one explanation would be is, you know,
this makes our bond even stronger,
even easier to maintain.
So here it was the social connection that
affected or influenced the individual characteristic.
And both of these phenomena are very,
very common and very strong in, um, social networks.
Um, and the correlations also exist in many other types of networks.
And this is really the main intuition that we
are going to exploit in today's, uh, lecture.
So the question will be,
how do we leverage this notion of correlation across the edges of the network,
observe the networks, uh,
to help to predict node labels, right?
When I say correlation, I mean nodes that are connected tend to have the same label,
tend to have the same preferences.
So the question is,
given this partially labeled network, you know, green,
let's call that a positive class a- a- a label 1,
and red will be what I'll call our negative class,
and let's label it with label 0.
And the gray nodes are the nodes that don't have the color yet.
And the question is, how would I come up with an algorithm to learn,
uh, or to predict the colors of, um, gray nodes?
So the motivation is that similar nodes are
typically close together or directly connected in the network.
So the principal we are going to, uh,
use is also known as guilt by association,
in a sense that if I'm connected to a node we'd label X,
then I'm likely to have that label X as well.
And that's this notion of correlation I was saying about, right?
So if you could say about, let's say,
um, the malicious and benign web pages, you could say,
malicious web- webpages tend to link to one another to increase visibility,
uh, and look credible,
and rank higher in search engines.
So we find out that one web page is malicious,
then perhaps other pages that link towards it also tend to be,
uh, malicious. All right?
Um, that's intuition.
So the way we are going to think of this is that we are going to, uh,
determine the classification label of a node v in the network,
that it will depend on two factors.
It will depend on the properties,
features of the node V. And it will also depend on the labels,
um, of the neighbors of- of the node v of interest.
Uh, and of course, because the v's label depends on the labels of, um,
nodes in the neighborhood,
those labels will also depend on the features of those nodes in the neighborhood.
So- so kind of- um,
this means that also the label of node v will depend on the features of the nodes,
uh, in its, uh, neighborhood.
So here is how we are thinking about this, uh, graphically.
Given a graph and a few,
uh, labeled nodes, um,
right we want to find labeled class of, ah, remaining nodes.
When I say label, in this case it will be positive or negative,
it will be, um,
green or it will be red.
Um, and the main assumption,
the main modeling assumption,
the main inductive bias in this, in our approach,
will be to assume that there is some degree of homophily in the network.
So that basically these, uh,
labels tend to cluster,
meaning that nodes of the same label tend to link to each other.
So to give you an example task,
let A be an adjacency matrix over n nodes.
This is basically captures the structure of the graph.
This adjacency matrix can be unweighted,
can be also weighted,
all methods generalize to weighted graphs as well,
can be undirected or directed.
All the methods generalize to both types of graphs.
And we will use the Y as a vector of node labels, right?
So we'll say Y_v equals 1, if the, um,
node V belongs to class 1,
to the green color,
and Y_v equals 0.
If the node V belongs to the class 0,
meaning it is labeled with a red color.
And of course there may be also other unlabelled nodes
that- that need to be- whose label needs to be predicted,
whose labeling needs to be classified.
And right now, they- we don't know, ah, the label.
And the goal is predict which labeled nodes are likely to be of class
1 and which ones are likely to be of class 0.
So that's the idea of what we wanna do.
Um, there are many examples of this notion of collective classification, right?
You can think about, I wanna do
document classification and documents linked to each other.
I wanna do link prediction in, in, in graphs.
And the links will depend on the properties,
labels of the neighbors.
Um, even like you can take certain other domains where you have
optical character recognition and you can represent that as a graph and say, you know,
the label of what character I am also
depends on the labels what are the characters around me,
meaning that I can,
I-I know how to form let's say English,
English words and you know what,
some random character, sequence of characters is very unlikely.
So, you know, whether I'm letter A or letter
G will depends what my neighbors here in the,
let's say the line graph think about.
So there is a lot of different cases where you can basically want to
make prediction about one object based on the relationships of the object to its,
uh, nearby, uh, objects in terms of nodes,
images, letters in OCR, part-of-speech tagging.
In many other cases,
knowing what- what- what are the labels of
the nodes around you helps you determine your own label.
That's essentially the idea.
So collective classification that we are going to talk about today is going to have,
um, three different parts, right?
So the intuition we are going to have is that you wanna simultaneously classify, ah,
linked nodes using address and propagate information across the edges of the network.
And this will be our probabilistic framework.
Where do we will be making what is called a Markov assumption.
And a Markov assumption means that the label of a node,
um, only depends on the labels of its neighbors, right?
So this is a first-order Markov assumption because we only
assume that a label depends on the label of the neighbors.
And we don't- for example,
assume that the label depends on the label of neighbors, of neighbors, right.
Like degree 2 neighborhood.
We only look at the degree 1 neighborhood.
And this notion of collective classification,
the reason why we use this experiment is because we are- we will be
altogether classifying all the nodes on the graph because every,
every nodes labeled depends on other nodes labeled.
So we are going to iteratively,
ah, reclassify, reclassify nodes.
Nodes are going to update the belief for a prediction about
the labels until the process will converge.
And in order for us to do this kind of collective iterative classification,
we will need three types of, ah, classifiers.
We'll have these local classifier that assigns the initial label to the node,
we'll then have what we call a relational classifier that
captures between- correlations between nodes and you will basically say,
aha, what are the labels of other nodes in
the network of the neighbors of the node of interest.
And then we'll have this notion of collective inference,
where we will be- where we will be propagating these correlations,
these beliefs over the network until the labels wi- will
converge to some stable state or until some fixed number of iterations,
um, will be achieved.
So what are these three pieces that we need to define?
First, we have this notion of our local classifier that
will assign initial labels to unlabeled nodes.
So this is used for initial label assignment.
And it will predict label of node based on its attributes or features.
Um, it is just a standard classification task where given a set of,
ah, given features of a node,
we wanna predict it's labeled.
And these does not use the network information yet.
So this is applied only once at the beginning to give initial labels to the gray nodes.
Then we will define this notion of
a relational classifier that will capture the correlations between the nodes.
So what does this mean?
Is that we learn another predictor that will predict a label of
one node based on the labels or attributes of other nodes in its neighborhood.
And this is where the network information will be used
because this relational classifier will say what is,
what is given nodes label based on the labels of the nodes that are connected to it.
And this is where the network information is used.
And then we won't only apply this relational classifier once,
but we are going to apply it in rounds.
So we'll- we will have this collective inference stretch,
where we are going to keep updating
the predictions based on the updated predictions on the neighbors, right?
So we are going to apply a relational classifier to each node iteratively
and iterate until the inconsistency between neighboring nodes is minimize,
meaning network structure is going to affect the predictions and
these predictions are going to converge and the predictions are going to stabilize.
And usually we will either run this, ah,
iteration until it stabilizes or until some maximum number of iterations, ah, is reached.
And I will give you specific examples,
ah, what I mean by that.
So the problem setting is how do we predict labels Y_v of unlabeled node V?
Here denoted in Grey color.
Each node V will have a feature vector F_v. Labels of some nodes will be given to us.
Ah, you know, we'll use label 1 for green nodes and labeled 0 for red nodes.
Ah, and the task is find the probability that a given node, um, is,
let's say positive is green based on the features it has,
as well as the network structure and the colors,
ah, of the nodes around it.
So that's the problem we are trying to solve.
And we are going to solve this by propagating the beliefs,
the propagating the information, ah,
across the underlying network structure in an iterative way.
So what's the overview of what is coming?
We are going to focus on this notion of semi-supervised node classification.
Semi-supervised in a sense that we are given
both labeled and unlabeled data at the same time,
we are given a partially labeled network.
Ah, we are going to use this notion- this intuition of the notion of homophily,
that similar nodes are typically close together or directly connected in the network.
And we are going to talk about three techniques,
about the relational classification,
iterative classification, and then last,
I'm going to talk about belief propagation.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 05.2 - Relational and Iterative Classification.txt
So first, we are going to talk about
relational classification and iterative classification.
So let me explain to you, uh,
the ideas, uh, for this part of the lecture.
So first, we wanna talk about what is, uh, relation classification,
then we'll talk about iterative classification, and then last,
I'm going to talk about, uh,
belief propagation as I- as I said, uh, earlier.
So, uh, probabilistic relational classifier,
the idea is the following.
Uh, the class probability, um,
Y_v of node v is a weighted average of class probabilities of its neighbors, right?
So this means that for labeled nodes we are going to
fix the class label to be the ground-truth label,
the label we are given, and then for, um,
unlabeled nodes, we are just initialize the belief that, let's say,
they are- uh, they have the color green to be,
let say, 0.5, so something, uh, uniform.
And then nodes are going to update their belief about what color they are about,
based on the- on the colors of the,
eh, nodes, uh, in the network, uh, around them.
And this is going to iterate until it, uh, converges.
And just to note,
here we are go- only going to use node labels and network structure,
but we have not yet going to use node attributes.
So here, this works even if nodes only have colors and there are no,
uh, attributes, uh, attached to the nodes,
or no features attached to the nodes.
So how are we going to do this, uh, update?
We are going to, uh,
update this formula that I- that I show- uh,
that I show here, and the idea is the following.
We say that probability that my node of interest v is of class C,
is simply, um, 1 over the sum over, let's say,
the weights of the edges, um, uh,
that- that are adjacent to our node v. So this is basically, uh,
counts the degree or the in-degree of node v. Um,
and then we say, now let's sum up over this same,
uh, set of edges.
Here is the weight of every edge,
uh times the probability that
the neighbor beho- belongs to the same class C. So basically,
here we are summing all the neighboring nodes,
u of node v. We ask what is the weight of the edge times the probability,
the likelihood that node u belongs to class C?
So basically what this is saying,
is that the node v says the- the likelihood that I belong to a given class C,
is the average likelihood that
my neighbors belong to that same class C. Uh, and that's it, right?
So in some sense here,
a node is going to update its belief for a prediction about its own label,
about the beliefs or predictions about the labels of its nearby nodes.
And here we are making the most basic assumption,
which is that the two nodes share a label
if they are connected in the- in the network, which is here.
And, you know, we can now think of A_uv as a zero, one,
kind of as an unweighted graph,
or we can think of this as a weight,
um, and then, you know, now different nodes,
u have different influence on the label of v,
because of the co- different connection strength.
And this is just a normalization, so that this, uh,
weight that summation, uh,
is between, uh, zero and one.
There are a few just important things to note,
is that convergence of this formula is not guaranteed.
Um, and also notice as I said,
this model cannot use node featured information.
It is just using the labels of the nodes and also uses the network information.
How does it use the network information?
It uses it because these summations go over
the edges or over the neighbors, uh, in the network.
That's how network information is used.
So let me give you now one example how one could go and,
uh, implement the simple method.
First, you know, we are going to, uh,
initialize all labeled nodes, uh,
with their, uh, true labels,
and we are going to fix this.
So this probabilities of green and red nodes remain fixed.
So maybe it says, uh,
node 7 belongs to green class with probability 1,
while the node, uh,
2 belongs to green class with probability 0, because it's red.
Right? Um, and then all the unlabeled nodes here,
we will assign their, uh,
probability to belonging to,
uh, to class, uh,
green, to the positive class to be,
uh, to be 0.5., Right?
Nodes are unde- undecided which class they wanna, uh, belong to.
So now what we are going to do,
is we're going to update the- thi- this probabilities P of the, uh, gray nodes.
So let me show you how we are going to do this.
Um, of course, when we will be doing this,
we need to come up with some order in which we are going to update the nodes.
And in our case, let's assume that our order is exactly based on node ID,
so it's 1, 2, 3, 4, 5, and so on.
So first, we are going to update node 3.
And node 3 says the following.
It says, "Aha, I have, uh, three neighbors.
Um, you know, um,
and, uh, how I- how am I going, uh, to do this?
I have, um, you know, uh,
the- the- two neighbors of me have the probability zero of belonging to the green class,
and this- uh, and I have one gray- gray member that has 0.5.
So this is 0, 0 plus 0.5,
and because I have three neighbors and my graph is unweighted,
so this is divided by 3.
So now the- the node number 3 has a probability of belonging to green class,
uh, o- of 0.17."
Now there is- you know,
now node 4 wakes up and we're going to update its own,
uh, predicted probability of being green.
And again, it goes over its four neighbors and we sum up at zero,
uh, this is because of this one.
Then we sum up 0.17,
0.5, and 1, and divide it by 4.
So the, um, uh,
node number 4 will say,
"my likelihood or my probability that I am,
um, labeled green is 0.42."
Now, um, now that node 4 has,
uh, updated its label,
then node 5 is going to update its label,
again, summing over its neighbors,
ah, and, you know, it will- it will come up with a belief
that its probability of being green is 0.73.
Then, you know, node 8 is going to update
its label and node 9 is going to update its label.
And after, you know, the first iteration,
updates of all unlabeled nodes, uh,
these are our, uh, beliefs,
our predicted probabilities of them being green.
Again, the- the pre-labeled nodes,
they don't get to update,
they remain fixed, and all the gray nodes gets to update.
And you- for example, node number 9,
thinks it's green because the only neighbor it has is green with probability one,
so nine is also very sure it's- uh, it's green.
Um, while for example,
the pa- the eight is a bit less because it has one node who is not- uh,
neighboring one node, node number 5,
and that node is not super entirely sure about its color yet.
So this is, uh, one iteration.
Now we do the second iteration when now again,
node 3 is going to- to update its by summing 0,
0- 0, 0, um,
and 0.42, and dividing it by 3.
So what is going to happen,
is this probability increases a bit more than, uh,
you know, node 4 updated itself,
five updated, uh, and eight updated.
And nine updated, but its portability didn't change,
so we say node- node 9 has converged.
It converged to its belief and that's how it would stay.
So now you notice how the, uh,
beliefs have, uh, changed across the network.
And now I can run this for another, uh, iteration,
where again, I've update nodes in the order 3,
4, 5, uh, 8.
And as I update them, uh, you know,
the probability's updated a bit again, uh,
but node 8 did not change its, uh,
probability, so we say it converged and we fix it.
Right? So now, I still have three nodes and I will
update this for another iteration, right?
Um, and in the other iteration,
the four- five and three don't change,
so they have also converged.
So now, uh, node 3 beliefs it's of green class with probability 0.16.
Um, five thinks it's 0.86, so quite high.
And then, you know, po- no- node number 4 is undetermined.
It thinks it's a bit- leaning a bit more towards green,
but it's, uh, but it's unclear.
So, um, if I run for one more iteration,
um, then the probabilities,
um, on these classification task will converge.
Here is what- the- the values they converge to,
so what do we conclude?
We conclude that, uh, 9,8,
and 5, uh, belong- uh,
belong to green class.
Uh, node 4 also belongs to green class because it's just above 0.5, but very slightly.
So four is also green.
And three says, "You know,
I have a very low belief that I am of green color,
" so three is of red color, right?
So our predictions now would be that 4- 4, 5, uh, 8,
and belong to, uh, green color,
and three belongs to, uh, red color.
And we did this by basically just doing this, um,
iterative classification by propagating
this label information and nodes updating its belief about its label,
based on the labels of other nodes.
So in this approach,
we used the notion of the network,
but we did not use the notion that nodes have any kind of features or any kind of,
uh- uh, signal attached to them.
So this was in terms of relational classification based on the labels alone.
And you saw how basically the- the probabilities kind of
spread from labeled nodes to unlabeled nodes through these- um,
through this iterative classification,
where nodes were updating, uh,
the probabilities, based on the probabilities of
their nearby nodes or their neighbors in the network.
So now, we are going to look at the iterative, uh, classification,
a different kind of type of approach that will use more,
uh, of the information.
In particular, it's going to use both the node features as well as the labels,
uh, of the nearby nodes.
So this will now combine both the network as well as the,
uh, um, the feature information of the nodes.
So in the relational classifiers that we have just talked about,
they do not use node attributes,
they just use the network structure and the labels.
So now the question is,
how do we label?
How do we take advantage?
How do we harness the information in node attributes?
And the main idea of iterative classification,
is to classify node v based on its attributes f,
as well as the labels z of the neighboring nodes,
uh, N sub v of this,
uh, node v of interest.
So how is this going to work?
On the input, we are given a graph.
Each node will have a feature vector associated with it,
and some nodes will be labeled with, uh, labels y.
The task is to predict labels of unlabeled nodes
and the way we are going to do this is that we are going to train two classifiers.
We are going first to train a base classifier that is going to predict a label of a node,
uh, based on its feature vector alone.
That's the classifier Phi_1.
And then, we are also going to train another classifier that will have two inputs.
It will have inputs about the features of node v
and it is also going to have this additional input,
this vector z, that will su- summarize the labels of v's neighbors.
So this means that now we are making predictions with this classify our Phi_2,
um, using two- two types of input.
One is the feature information, uh,
captured in this feature vector f, and, uh,
also the labels of the name of its neighbors captured in the vector, uh, z.
And this way, this approach will now be using
both the feature information as well as the network information.
So let me tell you now how do we compute these labels summary vector z, uh,
of a- of a given node v. So the idea will be that,
you know, I'm a given node here in the network,
let's say blue and I have, um,
some nodes that believe they are green and other nodes that believe they are red.
So now what I need is to create this- this- this summary vector z, uh,
that will tell me- what are the beliefs,
what are- what does my- what do my neighbors think about their labels.
So for example, there are many ch- choices I can make to determine- to set this,
uh, vector, er, z.
For example, I could simply have it to be a vector,
to be a histogram which would count the number
or the fraction of each label in the neighborhood.
So for example, in this case,
um, for this node, uh,
the blue node we could say a-ha,
I have two neighbors who think they are green,
and I have one neighbor who thinks it's red.
Or another way would be to say,
you know, 66 percent of my,
um, neighbors think they are green,
and 33 think they are red.
And this is what this vector z would capture.
We could also, for example, um,
extend z to say what is the most common label among, uh,
these neighbors, or how many different labels are among these neighbors.
But essentially here the idea is that this vector z
captures how the labels around the node of interest,
uh, v are- are distributed among,
uh, the neighbors of node v in the network.
Um, and there are a lot of different, uh, choices, uh,
how we come up the,
uh- how we can come up with this vector.
Um, so this iterative classifiers are going now to work in two steps.
In the first step or in the first phase,
we are going to classify node labels based on node attributes alone.
So it means using the training data,
we are going to train two classifiers.
For example, using a linear classifier, a neural network,
or support vector machine or a decision tree that given a feature vector of a given node,
it's going to predict its label.
And in phase 1, we are going to apply this classifier Phi_1,
so that now every node in the network has some predicted, uh, label.
And then using the training data,
we are also going to train this classifier Phi_2,
that is going to use two inputs.
It's going to use the feature of the node of interest,
as well as this summary vector z that captures or summarizes the labels of the nodes,
um, in it's network neighborhood.
And this Phi_2 is going to predict the label of the node
of interest v based on its features,
as well as the summary z of its,
uh- of the labels of its neighbors.
And then- right, so we have first applied Phi_1 then we also have trained Phi_2 so
that now we will go into phase 2 where we are going to
iterate and we are going to iterate until convergence,
uh, the following- uh, the following, uh,
iteration where on the, uh,
test set, which means on the unlabeled nodes.
Uh, we are going to use this, uh, first,
the classifier Phi_1 to assign initial, uh, labels.
We are going to compute the label summaries z,
and then we are going to predict the labels with the second classifier, Phi_2.
And we are going now to repeat this process until it
converges in a sense that we are going to update the vector z.
We are going to, uh,
apply the classifier Phi_2 to update the label of the node.
And, uh, now the no- node labels have updated,
we are going to update the label summaries z and again
apply Phi_2 to update the node predi- the- the node predictions, update z,
update the prediction, and keep iterating this until this, uh,
class labels stabilize, converge or some maximum number of iterations is reached.
Um, note that in general or in worse-case,
uh, convergence is not guaranteed,
so we would set some maximum, uh,
number of iterations, you know,
maybe 10, 50, 100, something like that, something like that.
Not too many- not too many.
Um, so that's the idea.
So what I wanna do next is actually show you
this to give you a sense how this work on a simple, uh, toy example.
So the idea here is that we have an input graph,
uh, let's say that we are thinking about web pages.
So we have a graph of how web pages link to each other.
Each node will be a web page.
An edge will be a hyperlink,
uh, between the web pages.
And, uh, uh, we will have a directed edge,
which means that one page points to another page.
And imagine that every node in my network is,
uh, is, uh, is described with the set of features.
In our case, you know,
we could simply assume that these features captures,
uh, what words, what texts a web page is using.
And imagine that the task we wanna do is to predict the topic of the web page.
And what we would like to do in this prediction is use two facts.
First is, of course,
we'd like to use the words of the web page to predict the topic.
And then the second thing we would like to do is we also can
assume that pages that are on similar topics or on the same topic,
they tend to link one another.
So when we make a prediction about the topic of a page,
we don't only rely on the words that the page users itself.
But if you also wanna rely on the labels,
on the topics of the pages that link to this,
uh, page of interest.
So that is, uh, the idea.
Uh, this is what we would like to do.
So let me now, uh,
show you how- how this would work, uh,
in this, uh, example of a web page classification.
So the idea here is that first,
we are going to train this baseline classifier,
this classifier Phi 1,
that is going to- to use and classify pages,
classify nodes based on their attributes.
So just to give you uh,
to explain what- what is happening here,
we have this graph on four nodes.
We will do the following.
It's- the color will denote the ground truth color.
So this is the ground truth topic of the real topic of the web-page.
Um, the- the- then each- each page also has some feature- feature vector,
let's say describing the words on that page.
And let's assume these are the feature vectors of my four pages.
For- the pages also have hyperlinks that are directed and point to each other.
And then for example, based on the labeled data,
we can uh, apply our Phi 1 and Phi 1 will say,
this- this web-page belongs to topic o and you know,
the ground truth is zero,
so this is a correct classification.
But for example, this- this web-page here, uh,
based on it's features alone um,
we will predict that it is ah,
of topic 0, but in reality it is of topic 1.
And the question will be,
can we using kind of network information- could network
information help us to decide that this should really be green topic,
not the red topic as it is predicted based on the features alone.
And then we have this four other pages,
here are their corresponding featured vectors um,
and you know, here the classifier would predict that they're both green um,
while here the classifier would predict that
these two should be red and, perhaps, you know,
one way to think of this is that it's really whether the first feature uh,
is set to one, you're red and if it's set to zero, you are green.
That would be one way to think of the uh,
what the classifier is learning.
So now, in the first step of this or in the first phase, um,
we- we came up with predictions of the labels uh,
based only on the features alone.
So we applied the classifier Phi 1 from the previous- from the previous slide.
So now what we wanna do next is we wanna create our feet labeled summary vectors z.
So for each node v,
we are going to create this vector z that captures the neighborhood labels.
And given that this is a directed graph,
we are going to use a four-dimensional vector to capture the statistics.
Basically, we are going to have two parts to this vector z;
a part I and O. I is about incoming and O is about outgoing neighbor labeling formation.
And we will say that for example,
I of 0 is set to 1 if at least one of the
incoming pages- one of the pages that links to this node of interest is also labeled 0.
And we will use similar definitions for I1, O0, and O1.
So give you an idea.
So this is now same graph as before.
These are the features of the web page,
but now this four more uh,
numbers appear here and let me again explain what do they mean.
So this is now- this four numbers are basically
my vector z that summarizes the labels of the neighbors around it.
And we have I, which is for incoming neighbors,
I wanna say is any of the incoming neighbors uh, of plus zero,
is any of the incoming neighbors of plus one,
because this node here has one neighbor that is green,
we set value one here, right?
It's an incoming neighbor.
And then on the- on the uh, outgoing neighbor side,
this is this edge,
we have one neighbor that believes it's class is zero.
So here it is, right?
So notice that colors correspond to ground truth
and the numbers correspond to predicted labels, right?
So that's why here I have one,
zero because the node of interest has one outgoing edge to our node of
class zero and has zero outgoing edges to nodes of class 1 of the green class.
Similarly, for example, you can look at this node here, uh,
it's feature vector and then the summary of
the incoming edges and the summary of the outgoing edges,
this- this node has here- has one incoming neighbor that is of class zero.
This is this one and has zero incoming edges
of class 1 and in terms of the outgoing edges,
it has zero outgoing edges to class, uh,
to class 0, and it has one outgoing edge to class 1.
So this is now, you know,
the four-dimensional representation, uh,
the vector z, the summary of the labels around this particular node.
Analogously, you can compute it for this node and you can compute it for that node.
So now that you have for every node both the vector f and the vector z,
what you do, you now train,
uh, the second um, uh, classifier.
And again you only train it on the labeled training set.
And this classifier, you train in such a way that it uses
both the information in f as well as the information in z.
So now you are basically training
the classifier Phi 2 that uses both the feature information as well as
the label summary information about what do
the neighbors of a node of interest think about their own labels.
And this now gives me the classifier Phi 2.
So now um, in the second step,
I'm going now to apply my uh,
classifier Phi 2 on the- on the graph of interest on the unlabeled nodes.
So right, so we've- I'm going to use trained node features to apply
the classifier Phi 1 on the unlabeled- unlabeled set and I'm going to predict the labels.
And now- now I'm going to use
trained node feature vectors as well
to- to predict the labels and now given the predicted labels,
I'm going to um,
update the feature descriptors- the feature descriptors in a sec- in a sense of uh,
class summaries around each node.
Now that I have created this vector z,
I can now apply my classifier Phi 2 to update the predicted label of every node.
And if I do this, the labels of certain nodes may change.
And basically the idea is right now that I have used all these data,
meaning the feature vector as well as
the summary vector of the labels around the given node,
I will be able to more robustly predict the label of the node of interest.
And now, right, I- basically the idea is that now I
can go and update the summaries because some labels might have changed.
I update the summary and reapply the Phi 2 predictor- Phi 2 classifier.
And I keep doing this,
basically reclassifying nodes with Phi 2 until the process converges.
Right? So I'll be updating this until- until the label prediction stabilize.
I think because if our prediction for a given node changes,
then its vector z is also going to change.
And if the vector z is going- changes then the- I have to apply
the predictor Phi 2 to update
the belief or the prediction about the label of a given node.
And I keep iterating this until the process stabilized, right?
So here the prediction for this node flipped from 0 to 1.
Now this vectors z have changed, have been updated.
So I have to re-update my classifier Phi 2 again over all these nodes.
And I keep doing this until the process converges meaning until no node labels change,
or until some maximum number of iterations is reached.
And that is essentially the idea of what
we are trying to do here and how do we arrive to the final prediction.
So to summarize, so far,
we have talked about two approaches to collective classification.
First, we talked about relational classification,
where we are iteratively updating probabilities of nodes
belonging to a labeled class based on the labels of its neighbors.
This approach uses the network structure and the labels of the nodes,
but it does not use the features of the nodes.
So then in the second part,
we talked about iterative classification,
where we improve over
the relational classification to
handle attribute or feature information of nodes as well.
And here the idea was that we classify a given node,
v or a given node i,
based on its features,
as well as the labels of its neighbors.
So here, the way we achieve these was to train two classifiers.
The initial classifier that given the features of a node predicts its label.
Now that the nodes have its labels predicted, for every node,
we can now create this summary vector z that
describes the labels of the nodes that are neighbors of a node of interest.
And then that we have that,
we can then use the classifier Phi 2 to re-update the prediction on a given node.
And this classifier Phi 2,
uses both the feature information of the node as
well as the summary vector of the labels of its neighbors,
the summary vector z.
And we then keep applying this classifier Phi 2 over the network until
the entire process converges or until
the entire- the maximum number of iterations is reached.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 05.3 - Collective Classification.txt
So to, uh, talk about the third method, uh,
we discussed today, I'm going now to,
uh, talk about belief propagation.
So this is the final method around collective classification we are going to talk,
um, in this, uh, in this lecture.
And really this will be all about what is called,
uh, no- uh, message passing, right?
We can think of these methods that we have talked about so far,
all in terms of kind of messages,
beliefs being sent over the edges of the network,
a node receiving these messages update- updating
its own belief so that in the next iterations its neighbors are able to- to get this,
uh, new information and update their own, uh, beliefs as well.
So this is, um, basically what, uh,
uh, what is, uh,
the core intuition we are trying to explore today,
which is all about passing information across the neighbors,
either pushing it or receiving it,
and updating the belief about, uh, oneself.
And, uh, the algorithm that- that does this is also known as loopy belief propagation.
Loopy because we will apply it to graphs that may have cycles, um,
and belief propagation because these messages,
these beliefs are going to be passed over the edges,
uh, of the network.
So let me give you a bit of context and where does this,
uh, notion of loopy belief propagation come from.
Belief propagation is a dynamic programming approach
to- about answering the proba- probabilistic queries,
uh, uh, in a graph, right?
By probabilistic queries, I mean, you know,
computing the probability that a given node belongs to a given class.
And it's an iterative process in which
the neighbor- neighboring nodes talk to each other by passing messages to each other.
And, you know, the way you can,
uh, think of this is really like, you know,
if- if there wouldn't be COVID,
we'd be nicely sitting in school and, you know,
you can- you can pass messages between, uh,
one another if you are, you know,
seat close together or if you are connected in the network, right?
So node v will say,
"I, you know, I believe you,
um, that you belong to this class 1 with the following, uh, likelihood."
And then- and then a given node can
then collect these beliefs from its neighbors and say,
"This now makes me more sure,
or less sure, I also belong to class 1."
And when this consensus, uh,
is reached as these messages are being, uh, passed around,
we- we arrive to the optimum,
we arrive to the kind of the final belief about the- the class or label,
uh, of every node in the network.
So let me show you some basics about message passing,
and we are first going to do this on simple graphs.
We're going to do this on a line graph,
then we are going to generalize this to a,
uh, tree-type graph, and then we are going to apply this to general graphs.
And just for simplicity, uh,
let's assume we want to count the number of nodes in
a graph using message passing, right?
So each node can only interact,
meaning it can only pass messages,
with its neighbors, right?
Other nodes it is connected to.
So, um, you know, as I said before,
note that there might be issues when we- if the graphs contains cycles,
but for now let's assume there are no cycles,
or let's ignore the cycles, right?
So the idea is I have a line graph,
I have nodes 1-6 linked in this type of linear structure.
So the task is that we want to compute,
or count, the number of nodes in the graph.
And the way how we can do this using message passing is that we define some ordering,
uh, on the nodes, right?
And this ordering, let's say,
results in a path,
and then edge directions, uh,
we can think of them according to the ordering of the nodes, right?
So, um, and this edge direction
determines the- the order in which messages are being passed.
So let's say we consider the nodes in order 1, 2, 3, 4, 5,
6, so the messages will start at one and be passed to node 6.
And the way these messages will go is that node i will compute the- the message and then,
um, uh, you know, it will- the message will be sent from node i to node i plus 1.
Um, and the idea will be that, um,
as the message passes to the node,
a node gets to look that message,
may transform it, and pass it on.
So if now we want to go back to this task of counting the number of nodes in the graph,
uh, the idea here is,
um, uh, simple, right?
Uh, each node can only interact with its, uh, members,
its neighbors can only pass messages,
um, and the idea is that each node says,
"What- however many nodes are in front of me,
I also add myself to the count and pass on this message,
uh, to the next node."
So this means that each node listens to the messages from its incoming neighbors,
updates the message and passes it forward.Uh,
and we will use the letter M to denote the message, right?
So for example, the way to think of this node 1 will say, "Oh,
there is one of me," and then it will pass this message M to the neighbor, number 2.
Neighbor number 2 is going to,
uh, get the incoming message and say, "Oh,
there is one node in front of me,
but there is me here as well,
so this means there are- there are two nodes so far."
So it will update the belief that there are,
you know, n plus 1 messages, uh,
ahead of it, and then it's going to send
this message forward with- the message will now have value 2.
And then node 3 is going to collect this message with value 2,
update it, it will say, "Oh,
there is me as well," so it will add one plu- to it,
so it will become a three,
and it will send it on.
And this way, you know,
like when- when we arrive to node, uh, six,
we will be counting the number of nodes, uh,
on the path because every node increases the message value by one and passes it on.
So at the end, you know,
the final belief of node 6 will be that there are six nodes, uh, in the graph.
And then the, you know,
we could even like now take this message and pass it back to the beginning node, uh, 1,
but the main idea here is that each node collects a message from its neighbor,
updates it, and passes it forward.
That's the core operation that I wanted to illustrate here.
And you can see how this works nicely on a- on a line graph,
um, with the proper, uh, ordering of the nodes.
Now we can do the same algorithm also on a tree, right?
If you have a graph that has this acyclic tree-like structure,
then we can perform, uh,
message passing not only on a path graph but also on a tree-structured graph.
The important thing here is to propagate or do
the message passing from the leaves to the root of the node- of the- of the tree.
So the idea is, right,
that first, uh, leaves, um, five,
six and seven, um,
will- and, uh, number 2 will say, "Oh,
we are one- I'm one node,
I'm the first node", and will set the message value to one,
and then they will send messages to their,
uh, to their parents, um, right?
Who are- who are, uh,
here, uh, above them.
And then the parents are going to sum up the values of the messages from the children,
add one to it and pass that message on.
And then again recursively,
the next level, you know,
it is going to sum up the- the incoming here,
it will be 2 plus 1 it's 3,
plus 1 for itself,
it's 4 to send it on,
and then this here is going again to sum up 1, uh,
plus 4 is 5 plus itself equals to 6, um,
so and there should be, uh,
six messages here, actually it should be seven,
so I lost a count of one somewhere.
But you get the idea, right?
So the idea is to say,
"I am one descendant,
I send information on.
I'm one descendant, I send it on."
This one sums the messages,
adds one to them, um, and says, "Oh,
there is three of us,
let me send this forward."
Right? So there will be a value of three here.
Um, there will be, um.
Again, uh, similarly, here it will be a value of one,
so that'll be, uh,
1 plus 3 is 4 plus 1 is 5.
So there'll be five here.
There is one descendant here.
So the two together will be 6 plus 1 for this node at the end,
the final belief will be that,
there is seven nodes, uh, in the graph.
Again, the basic information in this algorithm is this local message computation,
where messages come in,
they get collected by the node,
the node collects them, uh,
processes the messages, creates a new message, and sends it on.
So the loopy belief propagation, uh,
algorithm, what it does,
it, um, you know,
if you say what message will be sent from node i to node j,
it will depend on what node i hears from its neighbors, right?
The content of this message here is going to depend on
the incoming messages from its downstream, uh, neighbors.
So each neighbor passes a message to i to, uh,
i will now take these messages up,
uh, collect them, compute them,
update them, create a new message,
and then send this new message to node, uh, j.
And that's essentially what loopy belief propagation, uh, does.
The way we can think of this a bit more formally, uh, is the following.
We are going to have what is called label-label potential matrix.
So here we are going to capture dependencies between,
uh, a node and its neighbor in terms of labels.
And you can think of this as a- as a- one way to say is, uh,
what is the- if node, uh,
i has label y and node j has,
you know, some other label, uh,
y sub j, then, um, um,
the label-label potential matrix,
the entry of that, uh, um, uh,
cell will be the proportion, uh,
or will be proportional to the probability that node j belongs to class, uh,
Y sub j, given that its neighbor i belongs to class,
uh, Y sub, uh, i.
So this means that here,
if homophily is present,
this matrix will have high values on the diagonal.
Meaning, if- if, uh,
j belongs to Class 1,
then i should also belong to Class 1.
But given that we have this label-label potential matrix,
if there are big values, um,
off the diagonal, this will mean that, you know,
if my neighbor is of Class 1,
then I'm likely to be of class 0.
So it also can capture the cases where the, actually,
nodes that are connected are of the opposite labels,
are of the opposite, uh, classes.
So that this label-label potential matrix that tells us if a node j is of one label,
how likely am I of- uh,
of a different or of some,
uh, other type of label, if I'm node i.
Then we are also going to have this, uh, Phi,
which is the prior belief about what is- what should be the label of node i, so right?
So Phi of, uh,
Y_i is proportional to the probability,
prior probability of node i belonging to class Y_i.
And then we have this notion of a message where message goes from node i to node j.
And this means that it is i's message or estimate of
node y being in class Y sub- Y sub j. Um,
and L will be the set of all, uh, class labels.
So to give you now the formula, firstly,
in the initial iteration, in the utilization,
we are going to initialize all messages to have Value 1.
And then we are going to repeat for every node,
uh, the following formula,
where basically we are right now at node i
and we'd like to compute the message that you are going to send to
node j and we are in this message will be i's belief that j belongs to a given class.
So the way we are going to do this is to say, okay,
let's sum over all the states or over all the possible labels.
Let's take the- the belief that i belongs to a given, uh, label.
And, uh, this is the belie- this is the potential that
the j will belong to the label Y sub j, right?
So this is the label-label potential ma- matrix
that I have introduced in the previous slide.
Now, we are going to multiply that with a- with a, um,
prior probability of node i,
uh, belonging to class Y sub i.
And now here, what we are doing is we are going to sum over all the neighbors, uh, uh,
k of node i, um,
omitting the j, j will- is the uh uh,
the node to which we are sending the message.
So we are summing over everyone but the j or multiplying go- everyone by j.
Where now for every neighbor,
uh, k here, these three neighbors, we are asking,
what is your belief about node, um,
Y being in class,
uh, Y sub i, right?
So now basically what this means is that node i
collects beliefs from its downstream neighbors,
um, aggregates the beliefs,
multiplies with the- its pra- its- its belief what its own class should be.
And then- and then applies the label potential, um,
matrix to now send a message to node j about
how i's label should influence j's, uh, label.
And this is the core of, uh,
loopy belief, uh, propagation, right?
And, uh, we can keep iterating this, uh,
equation until we reach,
uh, some convergence, right?
When basically we collect messages,
uh, we transform them,
and send that message onward to the next level, uh, neighbor, right?
And after this approach,
this iteration is going to converge.
Um, we will have the belief about what
is the- what is the likelihood or probability of node i,
belonging to a given class or to a given label,
uh, Y sub, uh, i.
And essentially the way- the way this will look like
is that our belief will be a product of
the prior belief about what's the label of that node
times the messages of what other nodes downstream,
uh, here labeled as j.
Uh, think about what the label of node, uh,
i, uh, is, and this is encoded in these messages.
So this is the idea how belief propagation algorithm works.
Um, here, I call it loopy belief propagation,
because in practice, people tend to apply this algorithm to, um,
graphs that have cycles or loops as well,
even though than any kind of convergence guarantees and this kind of
probabilistic interpretation that I gave here, uh, gets lost.
So, um, if you consider graphs with cycles, right,
there is no- no longer a fixed ordering on- of the nodes which is, er,
which, um, otherwise the fixed ordering exists in terms if the graphs are trees.
But if a graph has a cycle,
you cannot- you cannot, uh,
sort, uh, the nodes, uh,
in- in a- in a- in a nice order.
And basically the idea is that we apply the same algorithm as in the previous slides,
but we start from arbitrary nodes and then follow the edges to update the messages.
So basically, we kind of propagate this in some kind of,
uh, random order, again,
until it converges or until, uh,
some fixed number of, uh,
iterations is, uh, is reached.
So, uh, to give you an idea how this would look like,
for example, on a - on a graph with cycles, um,
the issue becomes that,
if our graph has cycles,
messages from different subgraphs,
from different subbranches, are no longer independent, right?
So it means that for example,
when our graph was a tree, we could say, "Oh,
let's collect information from the left child,
from the right child,
and add it up, and send it to our parents."
So it means that, kind of,
children don't talk to each other,
and these messages really are coming from disjoint - disjoint parts of the tree.
However, if there is a cycle, then this,
uh- this idea of something being disjoint or independent,
is no longer true, right?
Like, for example, when node u would collect messages,
it would collect message from i and k. But really,
these messages are no longer independent because they
both depend on the message they got from j.
So in this sense, it comes- in some sense,
j is talking to u twice,
once through i and once through k. So this creates problems,
uh, uh, in terms of, uh,
theory and in terms of convergence.
Um, but what people tend to do in practice,
and not what works really well in practice,
is that you still run this belief propagation,
even though the graph has cycles, right?
And here what tried to show you is that when- once you are in a cycle, kind of,
the information will- uh,
will- will amplify, um,
artificially because it gets on a cycle.
And this is similar,
if you think about it to- when we talked about PageRank,
and we talked about spider traps, right,
that- where the random walker kind of gets- gets infinitely lost in this- uh,
in this spider trap.
It starts cycling.
And- and the problem with cycles or with loops is that
these messages start cycling, um, again, as well.
So the problem, um,
in this case, if the graph has cycles,
is that the beliefs may not converge.
Um, message is based on initial belief of i and not on separate,
kind of, independent evidence coming from nodes of i.
So the initial belief about i,
uh, which could be incorrect,
is reinforced, uh, let's say,
uh, through the cycle in this case.
Uh, however, as I said, in practice, uh,
loopy belief propagation is still very good heuristic, uh,
for complex graphs because, uh,
complex real world networks tend to be more like trees,
and they tend to have a relatively small number of cycles.
So the cycles, in reality,
are not such a big problem,
as they might be in this kind of,
wore ca- worse-case scenarios.
Um, and to give you an example, right, imagine,
we are doing belief propagation and we have two states.
We have a state, or,
a true or a false.
And now, you know, this node here sends, uh,
a message to the following node and says,
"You know, I think you are true,
with, you know- s- on a- with my belief about you
being true is 2 and my belief about you being false is 1."
So now this node will take this and s- and pass it on, and this is now,
kind of, going to be,
let's say, updated or,
uh, passed on in a- in a cycle.
But when it comes back to this node,
this node is now going to collect messages from both- uh,
from both of incoming, uh,
messages, this message and that message.
So now it will say, "Oh,
my belief that I'm- I am in- uh,
I am true is actually 4.
And my belief that I'm- I'm false is,
let say, only 1, or,
uh, let's say only 2.
And what this means is that now in this cycle,
this is going, kind of,
to an- uh, to amplify.
Um, and the cycle is going to amplify our belief that the state is actually, uh, true.
So this means that messages loop around and around and are more- more and more
convinced that these variables actually have state true and not state false.
And, uh, belief propagation, kind of incorrectly,
will treat these messages as separate evidence,
uh, that the variable T is true.
Um, and the issue is that these messages
are no longer independent because there is a- there is a cycle,
uh, on the graph.
Um, and this can cause- uh,
this can cause problems.
As I said, in practice,
real graphs, uh, tend to look more like trees.
So they don't- uh,
the cycles are not, uh, a problem,
uh, in- uh, in practice, right?
This is, as I said, an extreme example.
Often, in practice, the cycles- the influence of cycles is weak.
Our cycles are long,
or, uh, include, uh,
at least one weak, uh, correlation,
so that the message strength, uh, gets broken.
So, um, what are some advantages of belief propagation?
Advantages are- ea- that it's easy to- to code up,
and it's easy to paralleli- parallelize.
It is, uh, general.
It means that we can apply any graph model with any form of potential.
I showed you this label-label potential matrix,
but you can also, uh,
think of more complex, higher order potentials.
Um, so this is nice because label propagation or belief propagation does not, uh,
consider only homophily anymore,
but can learn more complex patterns,
where labels change, based on the labels of the neighbors, right?
So far, in- in previous methods, we only said,
"My label is- uh,
depends on the label of my neighbors.
So if- whatever my neighbor preference is,
my- why a- whatever my label- my neighbor's label is,
this is also my label."
In belief propagation, the labels can flip because we have this notion of,
uh, label-label, uh, affinity matrix.
Um, the challenge in belief propagation is
that convergence is not guaranteed so we generally don't know when to stop,
um, especially if there are many, uh, closed loops.
So the trick here would be to run a belief propagation for a short,
uh, number of steps.
Um, and of course, these potential functions,
this label-label, uh, potential, uh,
matrix, uh, this needs to be- um,
it requires training, uh,
data analysis to be able to, uh, estimate it.
So to summarize, uh,
we learned how to leverage correlations in graphs to make predictions of nodes.
We talked about three techniques.
Relational classification, where basically we say,
my label is a sum of the labels from my neighbors,
which basically means my label is, kind of,
the label of my neighbors.
Um, this uses the network structure but doesn't use feature information.
Then we talked about iterative classification that use both node feature information,
as well as the summary of the labels captured in the vector z around a given node.
So this approach use both the feature information about the nodes, as well as,
labels of the neighbors, but still,
kind of, would depend on homophily-type principle.
And then we talked about loopy belief propagation.
That- that, um, included this label- uh,
label-label potential matrix, uh, and,
uh, thought about this as collecting messages,
transforming messages, and sending a message to the upstream neighbor, as well.
This process is exact and, uh,
well-defined on, uh, chain graphs and on trees.
But on graphs with cycles,
um, it creates, uh, problems.
However, as I said in practice,
cycles tend to be, uh,
few or tend to have weak connections,
so that in practice, cycles don't, uh,
cause, uh, too much problem,
and loopy belief propagation is
a very strong allegory or a very strong approach for semi-supervised,
uh, labeling of nodes, uh, in the graph.
So, um, with this, uh,
we have finished the lecture for today.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 06.1 - Introduction to Graph Neural Networks.txt
Great. So, uh, welcome back to the class.
Uh, we're starting a very exciting, uh,
new topic, uh, for which we have been building up,
uh, from the beginning of the course.
So today, we are going to talk about, uh,
deep learning for graphs and in particular,
uh, techniques around graph neural networks.
And this should be one of the most central topics of the- of the class.
And we are going to spend the next, uh, two weeks, uh,
discussing this and going, uh,
deeper, uh, into this exciting topic.
So the way we think of this is the following: so far,
we have been talking about node embeddings,
where our intuition was to map nodes of
the graph into d-dimensional embeddings such that,
uh, nodes similar in the graph are embedded close together.
And our goal was to learn this, uh,
function f that takes in a graph and gives us the positions,
the embeddings of individual nodes.
And the way we thought about this is,
we thought about this in this encoder-decoder framework where we said
that we want the similarity of nodes in the network,
uh, denoted as here to be similar or to
match the similarity of the nodes in the embedding space.
And here, by similarity,
we could measure distance.
Uh, there are many types of distances you can- you can quantify.
One type of a distance that we were interested in was cosine distance, a dot products.
So we say that, you know,
the dot product of the embeddings of the nodes
has to match the notion of similarity of them in the network.
So the goal was, that given an input network,
I want to encode the nodes by computing
their embeddings such that if nodes are similar in the network,
they are similar in the embedding space as well.
So their similarity, um, is higher.
And of course, when we talked about this,
we were defining about what does it mean to be
similar and what does it mean, uh, to encode?
And, ah, as I mentioned,
there are two key components in
this node embedding framework that we talked to- in- in the class so far.
So our goal was to make each node to a low-dimensional vector.
And, um, we wanted to encode the embedding of a node in this vector, uh, z_v.
And this is a d-dimensional embedding.
So, you know, a vector of d numbers.
And, uh, we also needed to specify the similarity function that specifies how
the relationships in the vector space
mapped to the relationships in the original network, right?
So we said, um, you know,
this is the similarity defined in terms of the network.
This is the similarity defined, uh,
in terms of the, um, embedding space,
and this similarity computation in the embedding space,
we can call this a dot,
uh, uh, we can call this a decoder.
So our goal was to encode the coordinate so that when we decode them,
they decode the similarity in the network.
Um, so far, we talked about what is called shallow encoding,
which is the simplest approach to, uh,
learning that encoder which is that basically encoder is just an embedding-look- look-up,
which means we said we are going to learn this matrix z,
where every node will have our column,
uh, reserved in this matrix.
And this- the way we are going to decode the- the embedding of a given node will simply
be to basically look at the appropriate column of
this matrix and say here is when it's embedding the store.
So this is what we mean by shallow because basically,
you are just memorizing the embedding of every node.
We are directly learning,
directly determining the embedding of every node.
Um, so what are some of the limitations of the approaches?
Um, like deep walk or nodes to end that we have talked about, uh,
so far that apply this, uh,
shallow approach to- to learning node embeddings.
First is that this is extremely expensive in
terms of the number of parameters that are needed, right?
The number of parameters that the model has,
the number of variables it has to
learn is proportional to the number of nodes in the network.
It is basically d times number of nodes.
This- the reason for this being is that for every node we have
to determine or learn d-parameters,
d-values that determine its embedding.
So, um, this means that, uh,
for huge graphs the parameter space will be- will be giant.
Um, there is no parameter sharing, uh, between the nodes.
Um, in some sense, every node has to determine its own unique embedding.
So there is a lot of computation that we need to do.
Then, um, this is what is called, uh, transductive.
Um, this means that in transductive learning,
we can only make predictions over the examples that we have actually seen,
uh, during the training phase.
So this means, in this case,
if you cannot generate an embedding for
a node that- for a node that was not seen during training.
We cannot transfer embeddings for one graph to another because for every graph,
for every node, we have to directly learn- learn that embedding in the training space.
And then, um, another important,
uh, uh, drawback of this shallow encoding- encoders,
as I said, like a deep walk or, uh,
node correct, is that they do not incorporate node features, right?
Many graphs have features,
properties attached to the nodes of the network.
Um, and these approaches do not naturally, uh, leverage them.
So today, we are going to talk about deep graph encoders.
So we are going to discuss graph neural networks that are examples of deep, um,
graph encoders where the idea is that this encoding of, uh, er, er,
of an embedding of our node v is
a multi-layers of nonlinear transformations based on the graph structure.
So basically now, we'll really think about
deep neural networks and how they are transforming
information through multiple layers of
nonlinear transforms to come up with the final embedding.
And important to note that all these deep encoders
can be combined also with node similarity functions in Lecture 3.
So we could say I want to learn a deep encoder that encodes the similarity function,
uh, for example, the random walk similarity function that we used,
uh, in the previous lectures.
Or in this case,
we can actually directly learn the- the-
the encoder so that it is able to decode the node labels,
which means we can actually directly train these models for, uh,
node classification or any kind of,
uh, graph prediction task.
So intuitively, what we would like to do,
or what we are going to do is we are going to develop
deep neural networks that on the left will- on the input will take graph,
uh, as a structure together with, uh, properties,
features of nodes, uh,
and potentially of edges.
We will send this to the multiple layers of
non-linear transformations in the network so that at the end,
in the output, we get, for example,
node embeddings, we also embed entire sub-graphs.
We can embed the pairs of nodes and make various kinds of predictions, uh, in the end.
And the good thing would be that we are able to train this in an end-to-end fashion.
So from the labels, uh,
on the right all the way down to the,
uh, graph structure, uh, on the left.
Um, and this would be in some sense,
task, uh, agnostic or it will be applicable to many different tasks.
We'll be able to do, uh, node classification,
we'll be able to do, uh, link prediction,
we'll be able to do any kind of clustering community detection,
as well as to measure similarity, um, or,
um, compatibility between different graphs or different sub-networks.
So, uh, this, uh, will really, uh,
allow us to be applied to any of these,
uh, different, uh, tasks.
So why is this interesting or why is it hard or why is it different from,
let's say, classical, uh,
machine learning or classical deep learning?
If you think about the classical deep learning toolbox,
it is designed for simple data types.
Essentially, what tra- traditional or current toolbox is really good at is,
we know how to process fixed-size matrices, right?
So basically, I can resize every image and I represent it as
a fixed-size matrix or as a fixed-size grid graph.
And I can also take,
for example, texts, speech,
and represent- think of it basically as a linear sequence,
as a chain graph.
And we know how to process that, uh, really, really well.
So kind of the claim, uh,
and motivation for this is that
modern deep learning toolbox is designed for simple data types,
meaning sequences, uh, linear sequences,
and, uh, fixed-size grids.
Um, and of course the question is,
how can we generalize that?
How can we apply deep learning representation learning to more complex data types?
And this is where graph neural networks come into play because they allow
us to apply representation learning to
much more complex data types than just the span of two very simple,
uh, uh, data types,
meaning the fixed size grids,
uh, and linear sequences.
And why is this hard?
Why this non-trivial to do?
It's because networks have a lot of complexity, right?
They have arbitrary size and they have complex topological structure, right?
There is no spatial locality like in grids.
Uh, this means there is also no reference point or at no fixed orderings on the nodes,
which means that in a graph there is no top left or bottom right as there is in a grid,
or you know there is no left and right,
as you can define in a text because it's a sequence.
In a graph, there is no notion of a reference point.
There is no notion of direction.
Um, and more interestingly, right,
these graphs are often dynamic and have multiple,
um multi-modal features assigned to the nodes and edges of them.
So this becomes, uh,
very- very interesting because it really, um, expands, uh,
ways in which we can describe the data and in which we
can represent the underlying domain in underlying data.
Because not everything can be represented as
a ma- fixed size matrix or as a linear sequence.
And there are- as I showed in the beginning,
right, in the first lecture,
there is a lot of domains,
a lot of use cases where, um,
proper graphical representation is,
uh, very, very important.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 06.2 - Basics of Deep Learning.txt
So what we are going to, uh,
discuss today is, uh, three topics.
First, I'm going to give a basic overview
of rudimentary deep learning neural network concepts.
Uh, and this will be important so that we all get on the same page.
And then I'm going to spend majority of the time talking about
deep learning for graphs and this concept of graph neural networks.
And then I'm going to discuss two specific, uh, architectures.
One is called graph convolutional networks and the other one is called,
uh, GraphSAGE, uh, to give you some intuition.
And then through, uh,
uh- through then series of the next lectures,
we are then going to go more in- deeper into the theory,
more go- deeper into different applications, different,
uh- different architectures, different design choices, um, and so on.
So we are going to talk about this graph neural networks topic,
uh, for the- for the next,
uh, couple of lectures.
So let's talk about, uh,
basics or do a quick tutorial introduction,
uh, to deep learning and deep neural networks.
So we will think of machine learning,
uh, supervised learning as an optimization problem.
So the idea is that we are given some inputs x,
and the goal is to predict or to produce outputs, uh, y.
And these outputs y we will call labels or classes and so on.
Um, and x can be represented in different ways.
X can be a vector of real numbers.
X can be a sequence,
like a natural language sequence,
sequence of word, sequence of tokens, voices.
It can be matrices,
meaning it can be a fixed size matrix like a-
the images that are already sized to be the same size.
Or it can be also an entire graph,
um, or a node in a graph,
which is what we will be, uh,
interested in, uh, later down,
uh, in the lecture today.
And then, right, the goal is that we want to map this abstract x,
whatever it is into the label y.
So we wanna be able to predict, uh, the label y.
And we are going to, uh,
need to learn this function that makes this mapping.
And we are going to formulate learning of this function as an optimization problem.
So we formulated as an optimization problem in
a sense that we say this function f that we are interested in,
that will take input x and produ- produce output
y would be parameterized by some parameters Theta.
And our goal will be that we will define this notion of a loss,
um, that we will say what is
the discrepancy between the predicted value and the true value?
So there are two important,
uh, things here, right?
Is that first, we are going to say, let's minimize, uh,
over these model parameters, uh,
Theta such that this discrepancy,
this loss is minimized.
So, uh, Theta is a set of parameters,
uh, we want to optimize.
Uh, you know, this could be a scalar value,
a vector, an entire matrix,
or a set of matrices.
Uh, and for example,
if you think in these, uh- uh,
laws-based framework, um, our, um, uh, deep,
uh- shallow encoders like, uh,
DeepWalk and node2vec ,
they're- in their- in those- those cases,
our parameter matrix Theta was really
the embedding matrix z in the shallower encoder case, right?
So- so Theta is just a generic, uh,
symbol to describe the parameters of the model.
And then loss function, as I said,
measures the or quantifies the discrepancy
between the predicted value and the re- and the, uh, true value.
So for example, um,
if you think about a regression,
so predicting real value numbers,
then y is the correct prediction,
f of x is the prediction,
what our modulary terms,
and L2 loss is simply ,
uh,a square, uh, of the distance- of the difference between the two numbers, right?
So we would say, I want to- I want to find the model parameters Theta
so- so- such that the sum of the squares of the differences is as small as possible,
square differences between the true value and the predicted value.
And of course, there are many different types of losses one may want to,
uh, use depending on the problem,
whether it's a regression wets- whether it's like
classification, whether it is, you know,
this, uh, um- whether it is a ranking task,
whether it's a classification task.
Um, and, uh, here's the link where you can,
uh, uh, talk- learn more about various types of losses.
Um, I won't go more into details of this,
but, you know, we mostly work with,
uh, L2 loss, which is used for,
uh, regression most often.
So when we are trying to predict, uh,
real values, uh, or cross entropy loss,
which I'm going to define later,
which is all about classification,
which is like classifying, you know,
single colors, for example.
So now let me give you an example of a loss function, right?
One common, uh, loss function that we are interested in is called a cross entropy.
Um, and let's say that we are talking about multiclass classification,
so we can have multiple color- colors.
So in this case, let say we have five classes- five different colors.
So, uh, this means that we are going to encode the-
the color using what is called one-hot encoding,
which means that we will say, aha,
now what we are trying to predict,
we are trying to predict a vector of dimensionality five, where, you know,
the first they mentioned perhaps corresponds to blue,
second corresponds to red,
third corresponds to green,
fourth corresponds to black and I don't know,
uh, the last corresponds to white, right?
So now I have- and if I given node of interest is- is green,
then, you know, the third entry of this vector is set to one.
So this is now how I encode the, uh- uh,
the, the- the colors in using this what is called one-hot encoding because there's 1,
1 and the rest is zero,
and then I can say, aha,
what I am going to do is I'm going to model this now in some sense,
probability distribution over colors, um, using,
uh, a function f, that will be a softmax of some function g. Um,
and, you know, lecture 3,
we defined the notion of soft max,
which is simply you go over to the ne- over the entries,
you expo- exponentiate them, and,
uh, you make sure that they sum up to one, right?
So for example, in our case,
maybe f of x, uh,
after we put in, uh,
f, uh, de- denote x,
we would produce the set of numbers
where basically you would say 100 probability point, you know,
we think the color is blue,
which points three-eighths and had read 0.40 is green and so on and so forth.
So now, what we wanna do is we want to measure the quality of this prediction, right?
We wanna say, what is the discrepancy between
the predicted probability of being green and the- the item truly being green.
And the way we computed this is no- is- is, uh, denoted, uh,
or called cross entropy loss,
where basically we are seeing,
let's sum up over all the classes.
Um, you know, here,
we have five classes,
so C goes up to five.
We say white is y_i?
What is the true probability of
that class times the lock predicted probability of that plus, right?
So in this sense, what this means is y is the actual and
f of x is the predicted value of the ith glass or the ith color.
And to intuition is the lower the loss,
the closer the pred- prediction is to one-hot, right?
If- if the value here would be one,
then log of one is zero.
So we made the correct prediction.
So now that we have a loss- defined the loss or
a discrepancy over any individual example or an individual data point,
we can then define the notion of a total loss,
which is just a loss summed up over all the training examples.
So it's a total amount of discrepancy between the predicted value and the true value,
summed up over all the training examples, right?
And what we want, right,
we want to find our function f,
our parameters to function f Theta,
so that this total discrepancy between the true values and predicted values is minimized.
And in this case,
we measure the discrepancy, uh, ah, for each,
uh, data point, for each prediction using this notion of a cross entropy loss.
So that's essentially, uh, the idea.
So now that we have defined the notion of loss and we have defined the notion, uh,
of the optimization problem,
basically trying to model parameters that minimize the loss.
The next question is,
how do we optimize this objective function, right?
And a classic way to- to minimize objective functions goes through,
uh, various kinds of more or less advanced notions of gradient descent.
So this notion of a gradient,
notion of a derivative,
the becomes central and most important, right?
So recall that gradient vector, uh,
at the given point is a direction and the rate of fastest,
uh, increase of a function.
So I can say, aha, I have my loss function,
and now I can ask my loss function, I can evaluate it,
um, with respect to my parameters,
and I can- this will tell me if I'm right- if
my parameters have a certain value right now, um,
what is the direction in which,
um, this, uh- this,
uh objective function, this loss function would,
um- would increase the fastest.
And- and that is very important because this means
then that I can think about what is called a directional derivative,
which is a multi- of a multivariable function, like for example,
the loss function, which is um,
where the variable are our model parameters theta.
Basically, it, uh, it basically tells us that alo- at a given point,
along a given vector represents instantaneous rate of change of a function along,
uh, at that vector.
So this means that I can now say given my current parameters,
in which direction should I change them such that the loss will decrease the most,
and that's essentially, uh,
what we are trying to do, right.
We would say we have a current estimate of our parameters,
let's compute the directional derivative of
our loss function surface or up that point where we are,
and then we are going to move into the direction, um,
of the fastest decrease of the loss and hopefully reach some good local,
uh, solution or a global minimum solution.
So gradient evaluated for the given point is the direction or derivative,
um, that gives me the direction of the largest increase, right.
Um, we are not interested in the increase,
we are interested in the decrease,
so we are going to walk in the direction opposite of the gradient.
We are going to walk- walk down,
not walk up, in terms of the gradient, uh, update.
Right. So a way how we think about this is to use
the algorithm called gradient, uh, descent.
This is the most basic version and then everything just kind of,
uh, uses the same intuition,
but it's just an improved over this.
And essentially, what this is saying is let me, uh,
repeatedly update the weights or the parameters of the model
in the opposite direction of the gradients until I converge, right.
So I say I have my current estimate of the grade- of the parameters,
let me evaluate the gradient, uh,
the derivative of the loss function at that, uh,
set of parameters at that point where my parameters currently are.
And then, you know, let me make a step in the direction that is opposite of the gradient.
So that's why I had minus here.
And this constant eta,
this is the learning rate, right?
It says how big of a step I wanna make.
And then this gives me the new updated set of parameters.
And now again, I put them here,
I evaluate the gradient,
and- and, uh, I make an- an update.
So basically in training, we say that, you know,
we optimize this Theta parameters iteratively,
and one iteration is one step of gradient descent.
And as I said, uh, eta here,
is the learning rate which is a hyperparameter that controls the size of a step, right.
Uh, and the idea is that usually at the beginning you could make bigger steps.
But as you get closer to the minimum,
you wanna make smaller steps because you don't wanna overstep.
You don't wanna kind of jump over the value,
you want to slowly descend into the value.
If you think of a function like this, right,
you don't want to kind of jump across.
And an ideal termination condition is when the gradient is zero,
which means you got stuck in some local minimum
where the function is flat so you know you are at the bottom.
In practice, um, we would stop training if
it no longer improves the performance on the validation set.
So rather than stopping when the gradient is zero, in practice,
we have a separate validation set over which we
validate the predictions of the model but we don't use it to compute gradients,
and, um, as soon as our performance on that
validation set stops improving, we stop the training.
That's usually the case,
even though, you know,
it might be still, uh,
possible to keep, uh, optimizing your, uh, objective function.
So the problem with this general gradient descent idea is that
computing the exact gradient requires a pass over the entire data set.
Uh, this is the case because if you remember earlier when I defined the loss,
I said the loss measures the discrepancy between
the data point- the predicted value and the true value,
and the total loss is a sum of the losses over all the training examples.
So now, this means that this- even when you are computing the gradient of the loss,
it means you have to dis- kind of propagate the gradient
inside the sum over all the training examples.
So it mean- which- which means that when you compute the gradient, each discrepancy,
each training example evalu- uh,
and the loss evaluated at the training example will have some contribution to the,
uh, to the total gradients.
So it means that one iteration of gradient descent in this case would
allo- would require to make a pass over the entire, uh, training dataset.
Um, and this is problematic because modern data sets
often have or often contain billions of data points,
um, and this can become very expensive and very slow.
So the solution, the speedup,
is called stochastic gradient descent or SGD.
And the idea is that rather than computing the loss over all the training examples,
we are only going to s- to compute the loss
and the gradient of the loss over what is called a minibatch.
And the minibatch is simply some small subset of the data.
And this is what we will call an x.
So let me now, uh,
define a couple of very important concepts
that you are going to hear over and over again.
So first, we talked about the notion of batch,
which is a subset of the data over which we evaluate the gradient, right?
Rather than evaluating it over the entire training dataset,
we are going to evaluate it on a small subset of the training dataset,
maybe hundreds, maybe thousands examples.
Batch size is the number of data points in the minibatch, right.
Um, so this is, uh, uh, important.
Usually, we like to make batches bigger,
but bigger batches make the- make the optimization slower because for every step,
we need to compute, uh,
over, uh, larger batch size.
Iteration in terms of stochastic gradient descent is then one step of, uh,
stochastic gradient descent where we evaluate the gradient on a given minibatch,
and we call these an iteration.
And then an epoch is basically a full pass over the datasets.
So basically, it means we get processing batches 1, 2, 3,
4, 5 all the way until we exhaust the training dataset.
So if we have, I don't know, a million examples and, uh,
we have, I don't know,
uh, um, 100,000 batches,
each one of size 10,
so basically after we have pre-processed,
uh,100,000 batches, this is one- one epoch.
And then we go to the beginning and start from the,
uh, beginning again, right.
So the number of iterations is equal to the ratio of the dataset size,
uh, and the batch size.
And as I mentioned, if you create these batches, uh, uh,
uniformly at random, then SGD is unbiased estimator of the full gradient, right?
Um, of course, there is no guarantee on the rate of convergence.
And in practice, uh,
this means it requires tuning the learning rate.
And this SGD idea is kind of a common core idea that then many other, um,
optimizers, uh, improve on,
like ada- adagrad, adadelta,
M RMSprop and so on.
Essentially, all use this core idea of selecting the subsets of the- subset of data,
evaluating the gradient over it and making the steps.
Now- now the details, uh,
vary in terms of what data points you select,
how big of a step you make,
how do you decide on the step size,
um, and so on and so forth.
But essentially, this minibatch stochastic gradient descent is the core of,
um, optimization in deep learning.
So now that we have discussed the objective function,
we discussed the notion of a minibatch,
we discussed the notion of a stochastic gradient descent, now,
we need to, uh,
talk about how is- is this actually done?
How are these, um,
gradients, uh, computed, evaluated, right.
Because in the old days, pre-deep learning,
you actually had to write down the model with the set of equations
and then you have to do by hand computed these gradients essentially,
you know, like we did it in high school, uh,
many of you are computing the gradients by hand- by hand on the whiteboard.
So essentially, you would have to compute those gradients by
hand and then code them into your, uh,
software C++, Python, Matlab, whatever,
uh, to be then able to,
uh, run the optimization.
Um, and interestingly, you're writing deep learning.
In deep learning, this prediction functions f can be very complex, right?
It can be this complex multi-layer deep neural networks.
Uh, and what I'm going to show you now is that basically,
the benefit of these deep learning approaches is
that the gradient computation is actually very,
very simple and um,
it comes for free in a sense that as you- as you made more complex models,
you- the complexity of gradient computation
in terms of what you have to do as a- as a programmer,
um, uh, doesn't really affect you.
So the idea is the following, right?
Let's start with a very, uh,
simple function, uh, f, that, uh,
basically take the input x and multiplies it with W. So
our parameters Theta of the model is this, um, is this,
uh, object W. Now if f returns a scalar,
if f returns a single number,
then W should be a vector, right?
X is a vector times a vector gives me a scalar.
So then for example, the gradient,
with respect to, uh, uh,
v of f respect- respect to W, is simply, uh,
vector where- where we differentiate, uh,
f with respect to w_1,
w_2, w_3, which are the components of our, uh,
vector W. And this, um,
gradient is then simply the derivative evaluate at- at a particular,
specific point, uh, W. So basically,
we have to work out what these derivatives are,
then plug in the concrete number for w and, um, get to the value,
and that would be then the gradient of that point W. Now for example,
if f returns a vector,
so f is a more complex function,
then W would be a matrix, right.
We would have matrix times a vector, gives me a vector.
So, uh, in this case,
W would be what is called a weight matrix.
It is also called a Jacobian matrix.
And then the way you would compute the gradient is
exactly the same sum up but now you would take the derivate
with respect to every entry of that W. So with respect to w1 1,
w1 2, w1 3,
and then you know to end of the first level,
and then it'll be W2 1,
2 2 and so on, right.
But essentially it's, uh, it's the same,
so now the gradient would be, uh, the matrix.
Now, we just had this very simple, uh,
predictor that just states the input and multiplies it with the- with the W. But now,
what if we wanna create more complex predictors?
Imagine, uh, just for the sake of the example,
I wanna have this complex predictor, uh, f,
that now first takes input x,
multiplies it with W,
and then multi- w_1 and then multiplies it with w_2.
So this now seems kind of more complex because first,
we are multiplying with
a one weight- weight matrix and then we are multiplying with the second weight matrix.
In this case, parameters of the model are the two,
let say rate, uh, matrices.
And now what we'd like to do is we'd like to compute the derivative,
both with respect to W_1 and W_2.
And what happens here is that we can actually apply the chain rule, right?
The chain rule says if you take a deriva- if you wanna take a derivative of variable,
uh, uh, z with respect to some variable x, but, um, uh,
variable z depends on variable y,
then the way you can do it is you say, aha,
I take z with respect to y,
and then I have to take y and, uh,
they could- derivative with- of it with respect to x.
So basically, this is how I can apply this chain rule to create this, uh,
deriva- partial- these derivatives, um,
and- and chain them together based- based on the dependencies.
So in our case,
if I wanna take my function f and compute the derivative of it with respect to x,
I could first take the function and take a derivative of it with respect to
W_1x and then I think the W_1x and take a derivative of it with respect to, uh, x.
And the notion of, uh,
back-propagation uses the chain rule to propa-
propagate gradients of intermediate steps and,
uh, finally obtain the gradient with respect of
the loss with respect to the model parameters.
Um, and this is very, um, uh,
interesting because it means we can mechanically compute, uh, the gradients.
So let me, uh,
give you an example, right?
So it- we are still working with this simple,
uh, two layer linear network, right?
Here is kind of the neural network representation of this,
but essentially, it takes,
let say, two dimensional on input x,
multiply it- it with W_1.
This is happening here,
and then multiply it with W_2 here,
to get an output, right?
Imagine I have some loss function.
Let's say I have a, uh, L_2 loss,
a squared loss that sim- simply says- was a discrepancy between the predicted value,
uh, and the true value?
And then, um, you know,
I evaluate this over the minibatch- mini-batch B.
And then I also have the notion of a hidden layer, uh,
and hidden layer is an intermediate representation for,
uh, input x, right?
So here, I'm using this, uh,
h of x to be W1 times x to denote the hidden layer, right?
It's some transformation of x that is not yet,
uh, the final output, uh, of the network.
And then of course, then I can rewrite this to say f of x is,
you know, uh, h of x,
which is the first product and then evaluate it, uh, um,
on the- on the s- on the g of h, uh,
which is the multiplication with, uh, W_2.
What this means now, if I wanna do what is called a forward pass,
I start with x, I multiply with W_1,
and then I multiply with W_2 to get the output.
So the way I can think of this as I start from x,
I apply W_1 to- to basically compute h. Now I,
uh, take h to, uh, um,
and apply function G to it, which is again,
I multiply with W_2 and I- I get the- I get now
the output f. And now I want to evaluate f,
uh, with respect to the loss.
So we have this kind of, uh,
nesting or chaining of functions.
And if I wanna do back-propagation now,
back-propagation means I have to now compute the derivative,
the gradient, and I wanna work backward.
So what does this means is that if these are my model parameters,
I start from the loss and compute gradients backwards.
So I would start with a loss, for example,
and I'm interested to compute the gradient of the loss,
uh, with respect to W2.
Then I have to go from the loss,
compute- take the derivative with respect to f,
and then I have to take, uh,
f and take a derivative with respect to W2, right?
So I went from lost to f to W, uh,2.
While, for example, to compute the derivative of the loss,
uh, with the- with respect to W_1,
I have to take the- the loss compute f of
the derivative with respect to f. Take f compute the derivative with respect to W_2,
and then kind of take the result of that W_2,
take a derivative, uh,
with respect to, uh, W_1.
And you can see kind of how I'm working backwards and how, uh,
as I go deeper into the network,
I can kind of re-use,
uh, uh, previous computations.
And this is why this is called a back-propagation because I kind of- kind of, uh,
working backwards, um, uh,
from the output all the way towards the, uh- the input.
And this then tells me how to update my parameter values so that,
uh, the discrepancy, the value of the loss will be smaller.
Um, note that in- in my case that I showed you so far,
we used a very simple two layer neural network
which is- which if- if you look at it carefully, uh,
is still a linear, uh,
function because W_1 times W_2 is,
uh- is another matrix or- or- or a vector.
But basically, it means that by chaining things,
we did not get any more expressive power.
This was still a linear model, right?
So, um, in this case,
in this simple example,
f is still a linear model with respect to x.
Now ma- no matter how many weight matrices do we compose,
how many Ws do we have.
But if we introduce non-linearities, for example, um,
a rectified linear unit defined like this and here's how it looked
like or a sigmoid function defined like this and here is,
you know, the pictorial version of it.
Then, um, these things become much more interesting
because now, by introducing non-linearity,
so actually increase the expressivity, uh,
of our model and the more than Ws we chain,
um, the more, uh, expressive the model will be.
And this now leads us to the model that is called multi-layer perceptron.
And in each layer of a multi-layer perceptron,
we combine linear transformation with the non-linearity.
So meaning so far,
we talked about W times x.
What to do now is, uh,
also apply a non-linearity tool,
for example, a sigmoid, uh,
or a- or a, uh, RELU, uh, function.
Um, here b is just a bias to a- a constant to exclusively take it out.
One way is also to assume that the feature vector is
one- one element or one entry longer,
and that entry is always value one,
and then these buyers becomes kind of part of b,
be- becomes a, uh, uh,
uh- a row- a row in b.
So, um, this is- this is now how,
uh, multi-layered perceptron works.
It's the same as we had before,
but just I sent things through a non-linear, uh, activation function.
And now, you know, if I want- now I can take this access at a given layer and I can,
uh, keep, uh, uh, chaining them by- by multiplying with another W,
sending through another, uh,
non-linear layer, multiplying with another W,
another non-linear layer, and I can make deeper and
deeper and more and more complex, uh, neural networks.
But in terms of optimizing them because of the,
uh, chain rule we explained here.
Um, the gradient computations can basically be done, uh,
mechanistically by the deep learning framework and we don't need to worry
about actually writing them down or worrying about how to do optimization,
uh, you know, which is great and really speeds up the development of,
uh, machine learning algorithms.
So to summarize, we talked about how to define machine learning using
objective function of minimizing the loss with respect to model parameters.
Uh, f, as we said,
this probability function can be a simple linear layer,
just W times x or a multi-layered perceptron where it's W times x,
uh, passed through a non-linearity, or, you know,
some other more- more complex neural network.
Um, and the idea is that we sample a batch,
uh, of input, uh, x.
We call this a mini-batch.
We then do the forward propagation,
um, to compute, uh,
the value of loss.
And then we do the backward propagation,
where we obtain gradients of the loss with
respect to the model parameters using the chain rule.
And then that- now that we have computed
the gradients with respect to the model parameters,
we use stochastic gradient descent, um, uh,
over this mini-batches to optimize our parameters Theta over multiple iterations.
And this, uh, you know,
now concludes our, um,
deep learning tutorial, and what we are going to talk about next is actually,
uh, graph neural networks.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 06.3 - Deep Learning for Graphs.txt
Next, we are going to talk about deep learning for
graphs and in particular, graph neural networks.
So now that we have kind of refreshed our notion of,
uh, how general, uh,
neural networks- uh, deep neural networks work,
let's now go and generalize neural networks so that they can be applicable to graphs.
That's what we are going to do next.
So, uh, the content is that we are going to talk about local network neighborhoods.
And we are then going to describe
aggregation strategies and define what is called, uh, computation graphs.
And then we are going to talk about how do we
stack multiple layers of these neural networks, uh,
to talk about how do we describe the model parameters training- how do we fit the model,
and how do we se- how do we- and give
a simple example for unsupervised and supervised training.
So this is what we are going to talk about and what we are going to learn,
uh, in this part, uh, of the lecture.
So the setup is as follows.
Um, we are going to assume we have a graph G that has a set of vertices,
a set of nodes.
It has an adjacency matrix.
Right now let's assume it's binary,
so means unweighted graph.
Let's also, for simplicity, assume it's undirected,
but, uh, everything will generalize to directed graphs as well.
And let's assume that every node, uh,
also has a para- uh,
a node feature vector X associated with it, right?
So, um, you know,
what are node features?
For example, in social network,
this could be user profile,
user image, a user age.
In biological network, it could be a gene expression profile,
uh, gene functional information.
And, for example, if there is no, um,
node feature information the- in the dataset, um,
what people like to do is either use indicator vectors,
so one-hot encodings of nodes,
or just a vector of all constants, uh, value 1.
Uh, those are two, uh, popular choices.
Sometimes people also use a degree, uh,
as fe- a node degree as a feature, uh, of the node.
And then another piece of notation,
we are going to- to denote N of v to be a set of neighbors of a given node,
uh, v. And that's basically the notation,
the setup, uh, we are going to use,
uh, for this part of the lecture.
Now, if you say,
how could I go and apply deep neural networks to graphs?
Here is a naive, uh, simple approach.
The idea you could have is to say,
why don't I represent a network with adjacency matrix?
And then why don't I append the node features to the adjacency matrix?
Now think of this as a training example and feed it through a deep neural network, right?
It seems very natural.
I take my network,
represent it as an adjacency matrix,
I append node features to it.
Now, this is, you know,
the input to the neural network,
um, and I'm able to make predictions.
Uh, the issue with this idea is several.
First is that the number of parameters of
this neural network will be multiple times the number of nodes in the network.
Because number of inputs here is the number of nodes plus the number of features.
And if you say how- what is the- how many training examples I have,
I have one training example per node.
So it means you'll have more parameters than you have training examples
and training will be very unstable and it will easily overfit.
Another issue with this is that this model
won't be applicable to graphs of different sizes.
Because if now I have a graph that has a height of seven nodes,
it's unclear how to fit a graph of seven nodes into five different inputs, right?
Because here we have a graph of five nodes.
So that's one, uh,
big- uh, big issue.
And another big issue is subtle but very
important is that this approach will be sensitive to know node ordering.
Meaning right now I number nodes as A, B,
C, uh, D, and E in the following order so my adjacency matrix was like this.
But now if I were to call for example node E- to call it A, B,
C, D, and E,
then the shape of my adjacency matrix would change, right?
The nodes, uh, would be permuted.
The rows and columns of the matrix will be permuted even
though the information is still the same.
And in that case,
the mind would get totally confused and wouldn't know what to- what to output.
So the point is that in graphs there is no fixed node ordering so
it's unclear how to sort the nodes of the graph that you could,
um, you know, put them as inputs in the matrix.
That's much easier in images because you say I'll start with the top-left pixel,
and then I- I'll kind of go line-by-line.
In- in the graph,
the same graph can represented by- in- by many different adjacency matrices,
because it all depends on the ordering or- uh,
in- in which you labeled or numbered the nodes.
So we have to be invariant to node ordering.
So the idea of what we are going to do is we're going to kind of,
take- borrow some intuition from convolutional neural networks,
from computer vision, but generalize them to graphs.
So here's a quick idea about what convolutional neural networks do.
Like if you have an image here,
you know, represented as this grid,
then you define this convolutional operator, basically,
this sliding window that you are sliding over the image, right?
You start at the top-left.
This is now a three-by-three operator.
You compute something over this,
uh, area of the image,
and then you slide the operator by, you know,
some number of steps to the right and apply the same operator again.
And- and you can keep doing this and you can imagine that now this will give
you a new image of different size, of different- uh, um, uh,
different number of rows and columns to which you can
apply another, uh, convolutional operator,
another kind of sliding window type operator that kind of goes over the rows,
uh, uh, of that- of that image.
And if you just chain this, uh,
together, uh, you can then, uh,
come up, uh, with convolutional neural networks and, uh, good predictions.
Our goal here is to generalize this notion of convolution between simple lattices,
between simple- uh, beyond simple matri- uh,
matrices, uh, and also leverage node features and attributes.
Like, for example, text or images that might be attached to the nodes of the network.
The issue is that our networks are much more complex.
So defining the node- the notion of a sliding window,
let's- let's say, uh,
you know, a three-by-three window,
it's- is- is very strange because maybe in some case, you know,
the sliding window may only cover three nodes in other-
in other case the sliding window may cover many more nodes.
And it's unclear how to define this notion of a window,
and then it's also unclear how to define the notion of sliding the window over the graph.
And this is a big, uh,
complexity and a big challenge that, uh,
graph neural networks have to do, uh,
to be able to be applied to complex network data.
So the idea that makes this,
uh, work is the following step in intuition.
So the idea is that single convolutional- single layer of a convolutional,
uh, neural network, basically what it does it,
for example, takes- uh, takes the, uh,
area of three-by-three pixels,
apply some transformation to them and creates a new pixel.
And that's the way you can think of it, right?
And now we can take this operator and slide it across the image.
What we'd like to do in the graph is something similar, but, you know,
if you wanna apply this operator in terms of a- uh,
in terms of a, uh,
let's say like a sliding window,
then we will have a center of the sliding window, which is a node,
and this center is going to kind of borrow,
uh, aggregate information from its neighbors, right?
The same way here you can imagine that this is the center of the operator and
it's kind of collecting information from- from its neighbors,
denoted by arrows, takes its own value as well and creates a new value,
a new pixel, uh, for itself.
So the idea is that- that what convolutional operators are certainly doing,
they are transforming information from the neighbors,
combining it, and creating a new kind of a message.
So this is when today's lecture also relates to the,
uh, last lecture when we talked about message passing, right?
So today here we can think about a node collecting information from its neighbors,
collecting messages from its neighbors, aggregating them,
combining them, and creating a new message.
So, uh, that is the- that is the idea.
So how graph convolutional neural networks are going to work.
Um, the idea is that node's neighborhood defines the neural network architecture.
So basically the structure of the graph around a node of
interest defines the structure of the neural network.
Um, so the idea is,
if i wanna make a prediction for this red node,
I, here, uh, in the network,
then the way we can think of this is that i is going to take information,
uh, from its neighbors and neighbors are
going to take information from neighbors of neighbors.
And we are going to learn how to propagate this information,
how to transform it along the edges of the network,
how to aggregate the heat,
and how to create a new message that then the next node up the chain can again aggregate,
transform, um, and- and compute.
So in some sense,
the way we can think of graph neural networks is a two-step process.
In the first step process we determine the node computation graph.
And in the second process,
we then propagate, um,
the- the- the information,
we propagate and transform it over this computation graph.
And this computation graph defines the architecture or the structure of the underlying,
uh, neural network, right?
So, um, in this way,
we learn how to propagate information across
the graph structure to compute node features or,
uh, node, uh, embeddings.
So that's the intuition.
Let me give you an example of what I mean by this.
Consider here a very small,
um, input graph on six nodes.
Um, and what we will want to do is,
the key idea would be that you want to generate base- node embeddings based on
the local structure of the neighborhood around that target node.
So for this input graph and this is the target node, uh,
here is the structure of the neural network that is going to
make computation to be able to make a prediction for node A.
And let me explain you why is this neural network has this structure.
The reason is that node A is going to
take information from its neighbors in the network; B, C,
and D. So here are B, C,
and D. And then of course,
this is kind of one layer of computation,
but we can unfold this for multiple layers.
So if we unfold this for one more layer,
then node D takes information from its neighbor A.
And that's why we have this edge here.
Node C, for example,
takes information from its neighbors A, B,
uh, E, and F. They are- they are here; A, B,
and F. And then D takes information from A and C because it's connected to nodes,
uh, A and C. So now,
what does this mean is that if this is the structure of the, uh,
graph neural network, now what is- what we have to define,
what we have to learn is we have to learn
the message transformation operators along the edges as well as the aggregation operator.
Say, because node B says,
"I will collect the message from A,
I will collect the message from C. These messages will be transformed,
aggregated together in a single message,
and then I'm going to pass it on."
So that now node A can again say,
"I'll take message from B,
I will transform it.
I will take a message from C,
transform it, message from D, transform it.
Now I am going to aggregate these three messages into the new message and
pass it on to whoever is kind of in the layer, uh, above me."
So that is essentially the idea.
And of course, these transformations here,
uh, uh, aggregations and transformations will be learned.
They will be parameterized and distributed parameters of our model.
What is interesting and fundamentally different from classical neural networks,
is that every node gets to define its own neural network architecture.
Or every node gets to define
its own computation graph based on the structure of the network around it.
So what this means is, for example,
that, uh, blue node,
D here, its computation graph will be like this,
will very skinny because D takes information from A,
and A takes it from B and C. So it's a- you know,
this is now a two-layer neural network,
but it's very skinny, very narrow.
While for example, node C has a much bigger,
much wider neural network,
because C collects information from- from its four neighbors and
then each of the neighbors collect it from its own set of neighbors.
So that architecture, structure of this green neural network corresponding to nodes,
target node C, is very different than the one from the node, uh,
D. So what is interesting,
um, conceptually is that now,
every node has its own, uh,
computational graph or it has its own, um, architecture.
Um, some nodes, if the neighborhood of the network around them is similar,
for example, these two nodes will have the same computation graphs.
Like this is E and F,
you see the kind of the structure of these computation graphs,
these neural network architecture trees, uh, is the same.
But in principle, every node can have its own computation graph, that's first thing.
And then the second thing that's interesting,
now we are going to train or learn over multiple architectures simultaneously, right?
So it's not that we have one neural network on which we train
now every node comes with its own neural network,
uh, architecture, neural network structure.
And of course, the structure of the neural network for
a given node depends on the structure of the network around this node,
because this is how we determine the computation graph.
And that's kind of a very important, er,
kind of deep insight into how these things are
different than kind of classical, uh, deep learning.
So now, how do- how does this work as we have multiple layers, right?
So the point is that the model can be of arbitrary depth.
We can create an arbitrary number, uh, of, uh,
layers and nodes have embeddings at each layer.
Um, and the embedding at layer 0 of a given node
is simply initialized as its input features X.
And then the layer K embedding gets information
from nodes that are kind of K hops away, right?
So the way this would work is,
layer 0 embedding for nodes is simply their feature vectors,
so here, denoted by X.
Then for example, embedding of, um,
node B at layer 1 would be sum aggregation of feature- feature vectors of, ah,
of neighbors, uh, A and C, uh,
plus its own feature vector,
and this is now embedding of this node at layer 1.
And then this will be passed on, uh,
so that now node 2 can- node A can compute its embedding at layer 2.
So what it means is that the embedding of A at layer 0
is different than its embedding at layer 1- sorry, at layer 2.
So at every layer,
a node will have a different, uh, embedding.
And also, we are only going to run this for a limited number of steps.
We are not going to run this kind of infinitely long or
until it converges as- as we did in the last lecture.
We don't have this notion of convergence.
We'd only do this for a limited number of steps.
Each step corresponds to one layer of the neural network,
corresponds to one hop,
uh, in the underlying network.
So if we want to collect information from K hops away from the starting node,
from the target node,
we are going to need a K layer neural network.
And because networks have final diameter,
it makes no sense to talk about, I know,
100 layer, uh, deep neural networks.
Again, unless your network has diameter or that you know,
the longest, shortest path is of 100 hops.
So now that we have, uh,
defined the notion of how do we create
this computation graph based on the structure of the neighborhood around a given node,
now we need to talk about these transformations that happen in the neural network.
And the key concept is neighborhood aggregation.
Um, and the key distinction between different approaches,
different graph neural network architectures is how different,
uh, how this aggregation, uh, is done.
How this information from, uh,
children nodes is aggregated and combined,
uh, with the information or message of the pattern.
One important thing to notice is because
the ordering of the nodes in a graph is arbitrary;
this means that we have to have our aggregation operator to be,
um, uh, permutation invariant.
Right? So it- it- it means that, um,
we- we can order the nodes in any order and if we aggregate them,
the aggregation will always be the same, right?
That's the- that's the idea.
It doesn't matter in what order we aggregate,
we want the- the result to be always the same because it doesn't matter whether,
you know, node B has neighbors A- A and C,
or C and A,
they are just- it's just a set of elements.
So it doesn't matter whether we're aggregating, you know,
in that sense from A- A and C or C and A.
You should always get the same result.
So neighborhood aggregation function has to be,
uh, order invariant or permutation invariant.
So now, of course,
the question here is- that we haven't yet answered is,
what is happening in these boxes?
What do we put into these boxes?
How do we define these transformations?
How are they parameterized? How do we learn?
So the basic approach is, for example,
is to simply average information from the neighbors,
uh, and apply a neural network.
So average- so a summation,
use permutation or that invariant because in any number,
you sum up the- the- the numbers,
in any way you sum up the numbers,
you will always get the same result.
So average or a summation is a,
uh, permutation invariant aggregation function.
So for example, the idea here is that every- every of
these operators here will simply take the messages from the children,
uh, and average node,
and then decide to take this average and make a new message out of it.
So the way we can, um,
do this is do the, er,
we aggregate the messages and then we apply our neural network,
which means we apply some linear transformation followed by
a non-linearity to create a next, uh, level message.
So let me, uh, give you an example.
The basic approach is that we want to average messages coming from the children,
coming from the neighbors and apply a neural network to them.
So here is how this- uh,
how this looks like in equation. So let me explain.
So first, write H is a- our embedding and, uh, subscript means,
uh, the node and superscript denotes the neu- uh,
the level of the neural network.
So at the beginning are the zeroth layer,
the embedding of the node V is simply its feature representation.
And now we are going to create higher-order embeddings of nodes.
So right so this is a level zero so at level one,
what are we going to do is- is getting an equation and let me explain.
So first we say,
let's take the embeddings of the nodes from the previous uh,
from the previous layer.
So this is the embedding of this node v from the previous layer.
And let's multiply, transform it with some matrix B.
Then, what we are saying is let's also go over the neighbor's uh,
u of our node of interest v. And let's take the previous level embedding of every node u,
let's sum up these embeddings uh,
and average them so this is the total number of neighbor's.
And then let's transform this uh, uh,
aggregated average of embeddings of ma- of uh, children,
multiply it with another matrix and uh,
send these through a non-linearity, right?
So basically we say, I have my own message- my own embedding if I'm node v,
my own embedding from the previous layer, transform it.
I aggregate embeddings from my children,
from my neighbor's from previous level,
I multiply that to be the different transformation matrix.
I add these two together and send it through a non-linearity.
And this is a one layer of my neural network.
And now of course I can run this or do this for several times, right?
So this is now how to go from level l,
to level l plus 1, um,
you know to- to- to compute the first layer embeddings,
I use the embeddings from level zero,
which is just the node features.
But now I can run this for several kinds of iterations.
Lets say several- several layers,
maybe five, six, 10.
And then whatever is the final um, eh, um,
hidden representation of the final layer uh,
that is what I call uh,
the embedding of the node, right?
So we have our total number of capital L layers and
the final embedding of the node is simply h of v at the final uh,
at the final layer- level- at the final level of uh, neighborhood uh, aggregation.
And this is what is called a deep encoder,
because it is encoding information from
the previous layer- lay- lay- layer from a given node,
plus its neighbor's transforming it using matrices B and W,
and then sending it through a non-linearity to obs- to obtain next level uh,
representation of the node.
And we can basically now do this uh,
for several iterations, for several uh, layers.
So how do we train the model if every node gets its own computational architecture,
and every node has its own transformation uh,
parameters, basically this W and B?
The way we train the model is that we want to define what are the parameters of it?
And parameters are this matrices W and B,
and they are indexed by l. So for every layer we have a different W,
and for every layer we have a different uh, B.
And the idea here is that we can now uh,
feed this embeddings into the loss function and we can
run stochastic gradient descent to train the rate parameters,
meaning uh, B_l and uh, W_l.
Um, and these are the two parameters,
right one is the weight matrix for uh, neighborhood aggregation,
and the other one is the weight matrix for transforming hidden um, hidden vector uh,
embedding of uh, of- of the node itself uh,
to create then the next level uh, embedding.
So this is uh,
how we- how we do this,
what is important is that this weight matrices are shared across different nodes.
So what- what this means is that this l and v are not indexed by the node,
but all nodes in the network use the same transformation matrices.
And this is an important detail.
Um, as I have written things out so far,
I have written them out in terms of nodes aggregating from neighbor's.
But as we saw uh, earlier in graphs,
many times you can also write things in the matrix form.
So let me explain you how to write things uh,
in the matrix form, right?
Many aggregations can be performed efficiently if you uh,
write them out in terms of matrix operations.
So what you can do is you can take your um,
matrix H and simply stag the embeddings of the nodes together,
like here so every node is an embedding for a- for a- uh,
given uh, node of a given layer.
So we can define this notion of a matrix H^l,
and then write the way I can simply compute them basically by saying,
what is the sum o- of the embeddings- aggregation of embeddings um,
of nodes u that are neighbor's of v. This is simply
taking the- taking the uh, the correct um,
entry in the- in the adjacency matrix and multiplying it with the matrix uh,
H. And this way,
I'm basically averaging or aggregating the embeddings coming from the neighbor' s. Um,
then I can also define this notion of a diagonal matrix,
where basically this matrix is zero only on the diagonal of it.
We have uh, non-zero entries that are uh,
the degrees of individual nodes,
and then if you say what is the inverse of this diagonal matrix
D. It is another diagonal matrix where on the uh,
edges where around the entries on the diagonal,
I have now one over the degree um- so that
D- D times inverse of D is an identity matrix, right?
So now if you take this and combine it with this uh,
D minus 1 then you can write the neighborhood aggregation,
basically averaging of the neighborhood embeddings simply has uh D to the minus 1.
So the inverse of D, times adjacency matrix,
times the embeddings at level H,
at level l. So basically,
what this means is that I can write things in terms of this summation and averaging,
or I can write it as a product of three matrices,
a product of this diagonal matrix that has one over the diagonal uh,
one over the degree on the diagonal.
So this corresponds to this theorem uh,
A corresponds to summing over the neighbor's and H uh,
uh, superscript l are the embeddings of the nodes from the previous layer.
So this means I can think of this in terms of this kind of matrix equation,
or I can write it basically as neighborhood uh, aggregation.
So rewriting the update functioning matrix form then write- is- is like this, right?
It's basically take the- uh,
your embedding and multiply it with B, uh,
take the embeddings of the neighbor's from previous layer um,
and multiply them with W. Um,
so red part of corresponds to neighborhood aggregation,
blue part corresponds to uh, to self-transformation um.
And in practice what this implies is that uh,
efficient sparse matrix multiplication can be used
to train these models very efficiently.
So you can basically represent everything goes matrices,
and then you have matrix gradients uh,
and everything would work uh, very nicely.
Now, in the last few minutes,
I wanna talk about how to train this thing.
So the node embeddings z are- are a function of the input graph.
And we can train this in a supervised setting in a sense that we wanna minimize the loss,
the same way as we discussed so far, right?
I wanna make a prediction based on the embedding,
and I wanna minimize the discrepancy between the prediction and the truth.
Ur, where, you know,
y could be, for example,
an old label or a- or a scalar value.
Um, or I could even apply this in an unsupervised setting,
where I would say I want, you know,
the- the similar- where node labels are unavailable,
I can use the graph structure for supervision.
So I could define the notion of similarity and say, you know,
the dot product between the embeddings of
two nodes has to correspond to their similarity in the network.
And I could use now deep encoder to come up with the embeddings of the nodes,
rather than using the shallow, uh, uh,
shallow encoder, uh, but I could use the same decoder as in node to vet,
meaning the- the random walk and,
um, similarity matching using the dot problem.
So that's you- I can apply this in both settings.
Um, to explain how I could do,
uh, unsupervised training a bit more.
So the idea would- would be that similar,
uh, nodes have similar embeddings.
So the idea would be that, you know,
let- let y- let y_u,
v denote- be kept value one if node u and v are indeed similar,
for example, they- they code- they, uh,
code according to the same random work as defined in nodes to work.
Decoder of the two embeddings,
let's say, is a simple dot-product.
It says embedding of one times the embedding of the other,
and you want the- the discrepancy between the similarity and the, uh,
similarity in the graph versus similarity in the embedding space to be,
uh, small, so we can define this through the cross entropy.
Um, and then we could again just basically run this, uh, optimization problem,
to come up with the graph neural network that makes,
uh, the predictions, um,
as we- as we discussed.
So, um, the way we are going to train this,
is that we are going to train this either, as I said,
in this kind of unsupervised way,
and of course we can also directly train this in a supervised way.
Which means that perhaps, you know,
for a given node we have some, uh,
we have- we have some label about a node,
maybe this is a drug-drug interaction network,
and you know whether this drug is toxic,
uh, or not, whether it's safe or toxic.
So we could then say, you know,
given this neural network,
predict at the end the label of the node.
Whether it's safe or toxic.
And now, we can backpropagate based on label.
So both- both are o- both are possible,
either we directly train to predict the labels,
or we can train based on the network similarity,
where network similarity can be defined using random works,
the same way as in,
uh, node to work.
So for, uh, supervised training,
uh, basically what we wanna do is,
we wanna define, uh,
the loss, uh, in terms of let's say classification,
this is the cross entropy loss for a binary classification.
Basically here is the prediction of the label for a- for a- uh,
for a given color whether it's toxic or not,
this is whether it is truly toxic or not, um,
and then the way you can think of this is y takes value one if it's toxic,
and zero if it's not.
If the true value is zero,
then this term is going to survive and it's
basically one minus the lock predicted- prob- uh,
one minus the predicted probability.
So here we want this to be the predicted probability- to be as small as
possible so that one minus it becomes close to one because log of 1 is 0,
so that this discrepancy is small.
And if the- if the,
uh, class value is one,
then this term is going to survive because
1 plus 1- 1 minus 1 is 0 and this- this goes away.
So here we want this term to be as close to one as possible.
Which again would say, if it's- uh,
if it's toxic, we want the probability to be high.
If it's not toxic,
we want the probability [NOISE] to be low- uh,
the predicted probability, uh, of it being toxic.
And this is the cross, uh, entropy loss.
So this is the encoded input coming from node embeddings.
Uh, these are the classification rates,
uh, for the final classification.
Uh and these are the, uh, node labels,
is it basically toxic, uh, or not.
Um, and I can optimize this loss function,
uh, to basically come up with, uh,
the- the parameters B and W that give me the embedding that then makes,
uh, good or accurate predictions.
So let me just give an overview and,
uh, finish, uh, and complete the lecture.
So the way we think of modern design is that given the graph,
we wanna compute the,
uh, the embedding of a target node.
First, we need to define the neighborhood aggregation function.
Um, uh, this is the first thing we have to do.
The second thing we have to do then is to define the loss function on the embedding,
like the cross entropy loss I was just discussing,
um, and then we need to train.
And the way we can train the model is that we train it on a set
of nodes in a- on a batch of nodes.
So we select a batch of nodes,
create the computation graphs,
and this is a batch of nodes on which we train.
Um, and then what is interesting is that,
after the model is trained,
we can generate embeddings for any nodes, uh, as needed,
and simply apply [NOISE] our model to them by basically just doing the forward pass.
So this means that we can apply our model,
even to nodes that we have never seen during training.
So this means that, uh,
we cannot train on one graph and transfer the model to
the other graph because this particular set of, um,
nodes could be used for training the parameters [NOISE] to be optimized here,
and then we could apply this to a new set of nodes,
to a new set of, uh, computation graphs.
So this means that our model has inductive, uh, capability,
which means that same aggregation parameters,
same W and B,
are shared across all nodes.
Uh, and the number of model parameters in
this case is sublinear with the size of the network,
because W and B only depend on the embedding dimensionality and the size,
number of features, and not on the size of the graph.
And this means that graph neural networks are able to generalize to unseen nodes.
And that's a super cool, uh,
feature of them, because for example,
this means that you can train your graph neural network on one graph,
and you can apply it to a new graph.
Because you determine matrices [NOISE] W and B here,
and then you can transfer this,
uh, to the new network.
You can, uh, for example,
train on one organism,
and transfer this to the new organism here.
This is, let's say, a biological network.
This is also very useful for the networks that
constantly evolve because you can take the snapshot of the network,
create the computation graphs here,
and determined the parameters so that when then in production,
and you know, derives,
you quickly create a computation graph for it.
Just do the forward pass,
and here you have the embedding for it.
So no- no retraining the model, um, is necessary.
And that is super cool because it means you can train- train on one graph,
transfer to another one,
train on a small graph,
transfer the model to a big,
uh, graph, or to an evolving graph.
So let me summarize,
uh, the lecture for today and finish here.
What we did is, we generated node embeddings by
aggregating no- node neighborhood information.
We saw our first basic variant of this idea that
simply averages up the messages coming from the neighbors.
Um, and the key distinction between different architectures as we are going to see next,
is that- is how this aggregation process is being done.
And, uh, what I'm going to discuss in
the next lecture will be the architecture called, um, um,
GraphSAGE that generalizes this
into basically a framework where different types of aggregations,
different kas- kinds of transformations, uh, can be used.
So, um, thank you very much, uh,
for the lecture, um,
and very happy, uh,
to take questions now.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 07.1 - A general Perspective on GNNs.txt
We are going to generalize what we talked about,
uh, last time, uh,
which will be all about generalizing and mathematically formalizing,
uh, graph neural networks.
The idea for today's lecture,
is to talk about deep graph encoders and mathematically formalize them,
and also show you the design space, the idea,
the diversity of what kind of design choices, uh,
do we have when we are making, uh,
these types of, uh, decisions,
uh, building these types of architectures, right?
So what we want is we wanna build a deep graph encoder that takes the graph, uh,
as the input, and then through a series of non-linear, uh, transformations,
through kind of this deep neural network,
comes up with a set of, uh,
predictions that can be at node level,
can be at the level of sub-graphs,
pairs of nodes, um, and so on.
And what we talked about last time,
is that the way we can define convolutional neural networks on top of graphs is
to- to think about the underlying network as the computation graph, right?
So the idea was when we discussed if I wanna make a prediction for a given,
uh, node in the network,
let's say this red node i,
then first I need to decide how to compose a computation graph,
um, and based on the network neighborhood around this node.
And then I can think of the- um,
of the computation graph as the structure of the graph neural- of the neural network,
where now messages' information gets passed, uh,
and aggregated from a neighbor to neighbor towards
to the center node so that the center node can make a prediction.
And we talked about how graph neural networks allow us to
learn how to propagate and transform, um,
information across the edges of
the underlying network to make a prediction and embedding,
uh, at a given node.
So the intuition was that nodes
aggregate information from their neighbors using neural networks,
so we said that every node in the network gets to define
its own multi-layer neural network structure.
This neural network structure depends on the neighbors and the,
uh, graph structure around the node of interest.
So, for example, node B here takes information from two- two other nodes,
uh, A and C because they are the neighbors of it,
uh, in the network.
And then, of course, the goal will be to learn, uh,
the transformations in- um, in- in this,
uh, in this neural network that- that would be parameterized and this way,
uh, our- our approach is going to work.
So the intuition is that network neighborhood defines a computation graph,
and that every node defines a computation graph based on its, uh, network neighborhood.
So every node in the graph essentially can get its own neural network architecture,
because these are now different kind of neural networks,
they have, uh, different shapes.
So now with this quick recap,
let's talk about how do we generally define
graph neural networks and what are the components of them,
and how do we mathematically formalize, uh, these components?
So first, in this general framework,
is that we have two, uh, aspects.
We have this notion of a message and we have a notion of aggregation.
And different architectures like GCN, GraphSAGE,
graph attention networks and so on and so forth,
what they differ is how they define this notion of aggregation,
and how they define this notion,
uh, of a message.
So that's the first important part,
is how do we define basically a single layer of
a graph neural network, which composed basically by taking the messages,
uh, from the children,
transforming them and aggregating them.
So that's the transformation and aggregation,
are the first two core, um, operations.
The second set of, uh,
operations is about how are we stacking
together multiple layers in a graph neural network, right?
So do we stack these layers sequentially?
Do we add skip connections and so on?
So that's the- that's the, uh,
uh, second part, uh, of the equation,
is how do we add this layer, uh,
connectivity when we combine Layer 1,
uh, with Layer 2.
Um, and then the- the last part that, eh,
- that is an important design- design decision
is how do we create the computation graph, right?
Do we say that the- the input graph equals the computation graph,
or do we do any kind of augmentation?
Maybe we wanna do some feature augmentation,
or we wanna do some graph structure manipulation.
Again, uh, in this lecture,
I'm just kind of giving you an overview of the areas where I will
be going to provide more detail and where we are going to go, uh, deeper.
So that's the- that's the third- the fourth area where this becomes,
um, very important, design decisions, uh, have to be made.
And then the last part is in terms of the learning.
You know, what kind of learn- what kind of objective function,
what kind of task are we going to use to
learn the parameters of our graph neural network,
right? So how do we train it?
Do we train it in a supervised, unsupervised objective?
Do we do it at the level of node prediction, edge prediction,
or entire graph, um, level prediction tasks?
So these are, essentially now gave you a kind of an overview of the parts, uh,
of the design- design space, uh,
for neural, graph neural network, uh, architectures.
So as I said, first is,
defining the layer, then it's defining connectivity between layers.
It's about, uh, uh, eh, layer connectivity.
It's about graph manipulation,
augmentation, feature augmentation, as well as,
uh, finally the learning objectives.
So these are the five, uh,
pieces we are going to talk about.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 07.2 - A Single Layer of a GNN.txt
So first, let's discuss about how do we define a single layer of a graph neural network, right?
So what- what goes into a single layer?
A single layer has two components.
It has a component of a message transformation.
And it has a component of message aggregation,
and as I mentioned,
different graph neural network architectures basically
differ in how these operations are being done,
among other kinds of things that differ between them.
So what is the idea of a single GNN layer?
The idea is that we want to compress a set of messages,
a set of vectors coming from the children,
from the- from the bottom layer of the neural network.
In some sense compress them by aggregating them.
And we are going to- to do this as a two-step process,
as a message transformation and message aggregation, right?
So if we think about this,
we are getting a set of children at the- at the bottom, a set of inputs.
We have- we have an output.
What do we have to do is take the message from each of the child and transform it.
Then we have to aggregate these messages into a single message and pass it on.
So the way you can think of this is that we get
messages, denoted as circles here, from the three neighbors,
from the previous layer.
We also have our own message, right?
Message of the node v from the previous layer.
Somehow we want to combine this information to create
the next level embedding or to the next level message for this node of interest.
What is important here to note is that this is a set.
So the ordering in which we are aggregating
these messages from the children is not important.
What is arbitrary?
And for this reason,
these aggregation functions that aggregate, that summarize,
that compress in some sense,
the messages coming from the children have to
be order invariant because they shouldn't depend,
in which ordering, am I considering the neighbors?
Because there is no special ordering to the neighbors,
to the lower level,
to the children in the network.
That's an important detail. Of course,
another important detail is that we want to combine
information coming from the neighbor- from the neighbors
together with a node's own information from the previous layer as denoted here.
So I'm connecting information from level l minus 1 to create
a message at level l. And I'm collecting information from the neighbors,
from the previous layer,
as well as from the representation of that node itself at  the previous layer.
So let me now make things a bit more precise.
So we will talk about message computation as the first operation that we need to decide.
And basically, the message computation takes
the representation of the node at the previous layer and
somehow transforms it into this message information.
So each node creates a message which will be sent to other nodes in the next layer.
An example of a simple message transformation is that you take
the previous layer embedding of a node and multiply
it with the matrix W. So this is a simple linear layer,
linear transformation, and this is what we talked about in the previous lecture.
All right? So that's the first part.
Then we need to decide this message function.
In this case, it's simply this matrix multiply.
The second question is about aggregation.
The intuition here is that each node will aggregate the messages from its neighbors.
So the idea is that I take now
these transformed messages m that we just defined on the previous slide, right?
So I take these transformed messages coming from nodes u,
from the previous level that I transformed,
let's say in this case with W, and I want to aggregate them into a single message.
All right, I want to take this thing and kind of compress it, aggregate it.
What are some examples of aggregation functions?
A summation is a simple aggregation function,
an average is an order invariant aggregation function as well as for example, Max,
we take the maximum message or to the maximum coordinate-wise value,
and that's how we aggregate.
And again, all these- all these aggregation functions are order invariant.
So for example, one concrete way how you could do this is to say, uh-huh
the level l embedding for node v is simply a summation of
the transformed messages coming from the neighbors u of that node of interest,
v. And this is where the messages from previous layer got
transformed and now we simply sum them up to have
the embedding for the node at level l. And of course,
now this node at level l,
to send to- to create a message for level l plus 1.
It will take now W l plus 1,
multiply it with h,
and send it to whoever is above them in the graph neural network structure.
So this was now a message operation,
message transformation, and message aggregation.
One important issue is,
if you do it the way I defined it so far is that information
from the node itself could get lost, right?
Basically, that computation of message for node v for level l does not directly
depend on what we have already computed
for that node- same node v from the previous level, right?
So for example, if I do it simply as I show here,
we are simply aggregating information about neighbors,
but we don't really say,
okay, but who is this node v?
What do we know about node v before?
So an opportunity here is to actually,
to include the previous level embedding of node v.
Then we are computing the embedding of v for the next level.
So usually basically a different message computation will be performed, right?
What do I mean by this is, for example,
the message transformation matrix W will be applied to the neighbors u.
While there will be a different message aggregation function B that
will be applied to the embedding of node v itself.
So that's the first difference, right?
So that the message from the node itself from previous layer will be multiplied by B,
while messages from neighbors from previous layer are going to be multiplied by
W. And then the second difference is that after aggregating from neighbors,
we can aggregate messages from v itself as well.
And usually, this is done via a concatenation or a summation.
So to show you an example,
the way we can, we can do this is to say,
ah-ha, I'm taking my messages from neighbors and I'm aggregating them.
Let's say with a- with a summation operator.
I'm taking the message from v itself, right,
like I defined it up here.
And then I'm going to concatenate these two messages
simply like concatenate them one next to each other.
And that's my next layer embedding for node v. So simply I'm saying,
I'm aggregating information from neighbors,
plus retaining the information about the node that I already had.
And this is now a way how to keep track of the information that the node has
already computed about itself so that it doesn't
get lost through the layers of propagation,
let's say by concatenation here, or by summation.
That's another popular choice.
So putting all this together, what did we learn?
We learned that we have this message where
each node from the previous layer takes its own embedding,
its own information, transforms it,
and sends it up to the parent.
This is denoted here through this message transformation function.
Usually, this is simply a linear function like a matrix multiply.
And then we have this message aggregation step
where we aggregate transformed messages from the neighbors, right?
So we take these messages m that we have computed here and we aggregated them.
We aggregate them with an average,
with a summation, or with a maximum pooling type approach.
And then what we can also do is,
another extension here is to also add a self message and concatenate it.
And then after we have done all this,
we pass this through a non-linearity,
through a non-linear activation function.
And this last step is important because it adds expressiveness.
Often, you know, this non-linearity is written as sigma.
In reality, this can be a rectified linear unit or a sigmoid,
or any other specific type of
non-linear activation function, popularly other types of neural networks as well.
But that's essentially how a single layer of graph neural network looks like.
So now that we have seen this in abstract,
I want to mention some of
the seminal graph neural network architectures that have been developed
and kind of interpret them in
this unified message transformation, message aggregation framework.
So, last lecture, we talked about graph convolutional neural network or a GCN.
And I've wrote this equation,
I said, ah-ha, the embedding of node v at layer l is simply an average of the embeddings of
nodes u that are neighbors of we normalized the by the- by the N-degree of
node v and transformed with matrix W and sent through a non-linearity.
So now the question is,
how can I take this equation that I've written here and write it in
this message transformation plus aggregation function.
And the way you can- you can do this is simply take this W and distribute it inside.
So basically now W times
h divided by number of neighbors is the message transformation function.
And then the message aggregation function is simply a summation.
And then we have a non-linearity here.
So this is what a graph convolutional neural network is,
in terms of message aggregation and message transformation.
Um, so to write it even more explicitly,
each neighbor transforms the message by saying,
I take my, uh,
previous layer embedding, multiply it with w,
and divide it by the,
uh node degree of v, so, uh,
this is normal- normalization by node degree and then the aggregation is a summation
over the neighbors, uh,
of node v and then applying, uh,
a nonlinearity activation function here,
uh, denoted as sigma.
So this is now a G- GCN written as
a message transformation and a message aggregation, uh, type operation.
So that's, um, number,
uh, the first classical architecture.
Uh, the second architecture I want to mention is called GraphSAGE,
and GraphSAGE builds- builds upon the GCN,
but extends it in, uh,
several important, uh, aspects.
Uh, the first aspect is that it realizes that
this aggregation function is an arbitrary, uh,
aggregation function, so it allows for multiple different choices
of aggregation functions, not only averaging.
And the second thing is,
it- it talks about, uh,
taking the message from the node itself, uh,
transforming it, and then concatenating it with the aggregating- aggregated messages,
which adds, uh, a lot of expressive, uh, power.
So now let's write the GraphSAGE equation.
In this message plus, uh,
aggregation type operations, right?
Messages computed through the,
uh, uh, aggregation operator AGG here.
Um, and the way we can think of this is that this is kind of a two-stage approach.
First is that, um, uh, we, um,
we take the individual messages,
um, and transform them,
let's say through, uh, linear operations,
then we apply the aggregation operator that basically
gives me now a summary of the messages coming from the neighbors.
And then, uh, the second important step now is that I take the messages coming
from the neighbors already aggregated, I concatenate it with v's own,
um, um, message or embedding, uh,
from the previous layer,
concatenate these two together,
multiply them with a- with
a transformation matrix and pass through a non-linearity, right?
So the differences between GCN is here and another more important difference
is here that we are concatenating and taking our own,
uh, embedding, uh, as well.
So, um, to, now to say what kind of aggregation functions can be used?
We can simply take, for example,
weighted average of neighbors,
which is what our GCN is doing.
We can, for example, take any kind of pooling,
which is, uh, you know, take the, uh, uh,
take the- transform the neighbor vectors and apply
a symmetric vector function like a min or a max.
So you could even have, for example,
here as a transformation,
you don't have to have a linear transformation.
You could have a multilayer perceptron as
a message transformation function and then an aggregation.
Um, you can also not take the average of the messages,
but your sum up the messages.
And, uh, these different, um, uh,
aggregation functions have different theoretical properties.
And we are going to actually talk about the theoretical properties and consequences, uh,
of the choice of the aggregation function on the expressive power,
uh, of the model,
um, in one of the future lectures.
And what you could even do, um,
if you like, is you could apply an LSTM.
So basically you could apply a sequence model, uh,
to the- to the messages coming from the neighbors.
Um, and here the important thing is that a sequence model is not order invariant.
So when you train it,
you wanna permute the orderings so that the- you teach the- the sequence model,
not to keep, uh,
kind of to ignore, uh,
the ordering of the messages that it receives.
But you could use something like this, um, as a,
uh, as an aggregation, uh, uh, operator.
So a lot of freedom, uh, to choose here.
And then the last thing to mention about
GraphSAGE is that adds this notion of l2 normalization.
So the idea is that we want to apply l2 normalization to the embeddings at every layer.
And when I say l2 normalization,
all I mean by that is we wanna measure the distance,
some of the squared values, uh,
of the entries of the embedding of a given- of a given node,
take the square root of that,
and then divide by the distance.
So basically this means that the Euclidean length of
this embedding vector will always be equal to 1.
And sometimes this is quite important and,
uh, leads to big, uh,
performance improvement because without l2 normalization, the embedding, uh,
vectors of nodes can have different scales, different lengths, um,
and in some cases,
normalization of the embedding results in performance improvement.
So after l2 normalization step,
as I introduced it here,
all vectors will have the same, uh, l2 norm.
They'll have the same length,
which is the length of, uh, 1.
Uh, this is how, uh,
this is defined, um,
if- if you wanna, uh, really see it.
So l2 normalization is also an important component, um,
when deciding on the design decisions on the specific architecture of the,
uh, graph, uh, neural network.
And then the last, uh,
classical architecture that I wanna talk about is called graph attention network.
And here we are going to learn,
uh, this concept of an attention.
So let me first tell you what a graph attention network is,
and then I will define the notion of attention and,
uh, how do we learn it and what- what it intuitively means.
So, uh, the motivation is the following.
Writing graph attention network when we are aggregating messages from the neighbors,
we have a weight, um,
associated with every neighbor, right?
So for every neighbor u,
of node v, we have a weight, alpha.
And this weight we call attention weight.
[NOISE] And the idea is that this weight can now tell me how important of a neighbor, um,
is a given- is a given node,
or in some sense,
how much attention to pay to a given, to a message from a given node u?
Because if these weights are different,
then messages from different nodes will have different weight,
uh, in this summation. That's the idea.
So now let's make a step back.
Explain why- why- how- how this is motivated,
why it's a good idea,
and how to, uh, learn these weights.
So if you think about the two architectures we talked about so far,
so the GCN and GraphSAGE.
There is already an implicit notion of this alpha.
So alpha_uv is simply 1 over the degree of node v. So basically this is
a weight factor or an importance of a message coming from u for the node v. And,
uh, so far, you know,
this alpha was defined implicitly.
Um, and we can actually define it,
um, more explicitly or we can actually learn it.
Uh, in our case,
alpha was actually for all the,
uh, incoming no- uh,
nodes u, uh, alpha was the same.
It only depended on the degree of node v but didn't really depend on the u itself.
So it does a very limiting kind of notion of attention, right?
So in- in GraphSAGE or GCN,
all neighbors are equally important to node v,
when aggregating messages and the question is,
can we kind of, er, uh, generalize this?
Can we generalize this so that we learn how important is a message from a given node,
uh, to the node that is aggregating messages, right?
So we wanna learn message importances.
And this notion of an importance is called attention,
and attention- the word attention is kind of inspired
by the- by- in a sense with cognitive attention, right?
So attention alpha focuses on the important parts of
the input data and kind of fades out or ignores, uh, the rest.
So the idea is that the neural network should devote more computing power,
more attention to that small important part of the input,
um, and perhaps, you know,
ignore- choose to ignore the rest.
Um, and which part of the data is more important depends on the context.
And the idea is that we are going to learn, uh,
what part of the data is important through the model training process.
So we allow the model to learn importance of different,
uh, of different pieces of input that it is, uh, receiving.
So in our case, we would want to learn this attention weight that
will tell us how important is a message coming from node u, uh,
to this, uh, node, uh,
v. And we want this attention,
of course, depend on the u as well as on,
uh, v as well.
So the question is, right, how do we learn, uh,
these, uh, attention weights, these weighting factors, uh, alpha.
So the goal is to specify an arbitrary importance, uh,
between, um, between neighbors,
um, when we're doing message aggregation.
And the idea is that we can compute the embedding, uh,
of each node in the graph following these attention strategies
where nodes attend over the messages coming from the neighborhood,
by attend, I mean give different importances to them.
And then, uh, we are going to, um,
implicitly specify different weights to different nodes, uh, in the neighborhood.
And we are going to learn these weights, these importances.
The way we are going to do this is,
uh, to compute this as a, uh,
byproduct of the attention mechanism,
where we are going to define this notion of attention mechanism
that is going to give us these attention scores or attention weights.
So let- let us think about this attention, uh, mechanism, a,
by first computing attention coefficients,
e_vu across pairs of nodes,
uh, u and v based on their messages.
So the idea would be that I wanna define some function a,
that will take the embedding of node u at previous layer,
embedding of node v at previous layer,
perhaps transform these two embeddings,
and then take these as input and prod- give me a weight, right?
And this weight will tell me the importance of, uh,
u's message on the, uh,
on the node, uh, v. So for example,
just to be concrete, right?
If I wanna say, what is the attention coefficients e_AB?
It's simply some function a,
of the embedding of node A at the previous step, uh,
and the embedding of node B at the previous step,
at the previous layer of the graph neural network,
and this will give me now the weight, uh, of this,
uh, or importance of this particular, uh, edge.
So now that I have these, uh, coefficients,
I wanna normalize them to- to get the final attention weight.
So what do I mean by, for example, uh,
normalize is that we can apply a softmax function,
uh, to them so that these attention weights are going to sum to 1.
So I take the coefficients e that we have just defined,
I, uh, exponentiate them, and then, you know,
divide by the s- exponentiated sum of them so that, uh,
these, uh, attention weights, uh,
alpha now are going to sum to 1.
And then, right when I'm doing message aggregation,
I can now do a weighted sum based on the attention weights, uh, alpha.
So here are the alphas.
These are these alphas that depend on e,
and e is the,
uh, is the, um,
is- depends on the previous layer embeddings of nodes, uh,
u and v. So, for example,
if I now say, how would aggregation for node A look like?
The way I would do this is I would compute these attention weights, uh- uh,
Alpha_AB, Alpha_AC, and Alpha_AD because B,
C, and D are its neighbors.
Uh, these alphas will be computed as I- as I show up here,
and they will be computed by previous layer embeddings,
uh, of these, uh,
nodes on the endpoints of the edge.
And then my aggregation function is simply
a weighted average of the messages coming from the neighbors,
where message is, uh- uh- uh,
multiplied by the weight Alpha that we have,
uh, computed and defined up here.
So that's, um, [NOISE] basically the idea of the attention mechanism.
Um, now, what is the form of this attention mechanism a?
We still haven't decided how embedding of one node
and embedding of the other node get- get combined,
computed into this, uh- uh,
weight, uh, e. Uh,
the way it is usually done is,
uh- um, you- you have many different choices.
Like you could use a simple, uh, linear layer, uh,
one layer neural network to do this, um, or, uh,
have alpha, uh, this, um,
function a have trainable parameters.
Uh, so for example a p- uh,
a popular choice is to simply to say: let me
take the embeddings of nodes A and B at the previous layer,
perhaps let me transform them,
let me concatenate them,
and then apply a linear layer to them,
and that will give me this weight, uh, e_AB,
to which then I can apply softmax,
um, and then based on that, ah,
softmax transformed weight, I use that weight as,
uh- uh, in the aggregation function.
And the important point is that these parameters of fu- of, uh, function a,
this attention mechanism a, uh,
the- basically parameters of these functions are trained jointly.
So we learn the parameters of
the attention mechanism together with the weight matrices,
so message transformation matrices,
um, in the message aggregation step.
So we do all this training in an end-to-end, uh, fashion.
What this means in
practice is that working with this type of attention mechanisms,
uh, can be tricky because,
uh, this can be quite finicky.
Uh, in a sense,
may- it's- sometimes it's hard to learn,
hard to make it converge,
so what we can also do is, uh, to, uh,
expand this notion of attention to what is called a multi-head attention.
And multi-head attention is a way to stabilize
learning process of the attention mechanism,
and the idea is quite simple.
The idea is that we'll have multiple attention scores.
So we are going to have multiple attention mechanisms a, um,
and each one- and we are going to train- learn all of them, uh, simultaneously.
So the idea is that we would have different functions a,
for example, in this case, we would have three different functions a,
which means I would- we would get three different, uh,
attention coefficients, attention weights for a given edge vu.
And then we will do the aggregation,
uh, three times, uh,
get the aggregated messages from the neighbors,
and now we can further aggregate, uh,
these messages into a single, uh, aggregated message.
And the point here is now that we- when we learn these functions a^1, a^2, a^3,
we are going to randomly initialize parameters of each one of them,
and through the learning process,
each one of them is kind of going to converge to some local minima.
But because we are using multiple of them,
and we are averaging their transformations together,
this will basically allow our model to- to be- to be more robust,
it will allow our learning process not to get
stuck in some weird part of the optimization space,
um, and, kind of,
it will work, uh, better,
uh, on the average.
So the idea of this multi-head attention is- is simple.
To summarize, is that we are going to have
multiple attention weights on the- on the same edge,
and we are going to use them, uh,
separately in message aggregation,
and then the final message that we get
for a- for a node will be simply the aggregation,
like the average, of these individual attention-based, uh, aggregations.
One important detail here is that each of these different, uh,
Alphas has to be predicted with a different function a,
and each of these functions a,
has to be initialized with a random, uh,
different set of starting parameters so that each one gets a chance to,
kind of, converge to some,
uh, local, uh, minima.
Uh, that's the idea and that adds to the robustness and stabilizes,
uh, the learning process.
So this is what I wanted to say about the attention mechanism,
and how do we define it?
So next, let me defi- let me discuss a bit the benefits of the attention mechanism.
And the key benefit is that this allows implicitly for
specifying different importance values to different neighbors.
Um, it is computationally efficient in a sense that computation of attention,
uh, coefficient can be parallelized across all the incoming messages, right?
For every incoming message,
I compute the attention weight, uh,
by applying function a that only depends on
the embedding of one node and the embedding of the other node,
uh, in the previous layer,
um, so this is good.
It is, uh, in some sense storage-efficient because, um,
sparse matrix operators do not require,
um- um, too many non-zero elements.
Basically, I need one entry per node and one entry per edge,
so, uh, you know,
that's cheap, that's linear in the amount of data we have, um,
and it has a fixed number of parameters,
meaning the, uh, mesh,
the- the attention mechanism function a has a fixed number of parameters that is,
uh, independent of the graph size.
Another important aspect is that,
uh, attention weights are localized.
They attend to local network neighborhoods,
so basically tell you what part of the neighborhood to focus on,
um, and they generalize,
meaning that they- they give me this, uh,
inductive capability, which means that, um,
this is a shared edge- edgewise mechanism and
does not depend on the graph structure- so- on the global graph structure.
So it means can I can transfer it across the graphs,
so this function A is transferable between graphs or from one part of the graph,
uh, to the next part of the graph.
So, um, these are the benefits and kind of the discussion,
uh, of the attention mechanism.
To give you an example,
um, here is, um, uh,
an example of a, um,
uh, of a network, uh, called Quora.
This is a citation network of different papers coming from different disciplines.
And different disciplines,
different publication classes here are colored in different, uh, colors.
And, uh, what we tried to show here is with
different edge thickness is the attention score um,
between a pair of nodes uh, i and j.
So it's simply a normalized attention score by basically saying,
what is the attention of i to j,
and what's attention of uh,
j to i across different uh, layers uh,
k. Notice that attentions,
um, can be asymmetric, right?
One mes- the, uh,
message from you to me might be very important,
while message from me to you [LAUGHTER], for example,
might be less important,
so do- it doesn't have to be symmetric.
And, um, if you look at,
you know, in terms of improvements,
for example, the graph attention networks can
give you quite a bit of improvement over, let's say,
the graph convolutional neural network, uh, because, uh,
because of this attention mechanism and allowing you to learn what to
focus- what to focus on and which sub-parts of the network, uh, to learn from.
So this is an example of graph attention network,
how it learns attentions,
and of course there has been many, uh, upgrades, iterations,
of this idea of attention on graph neural networks,
but the graph attention network,
you know, two years ago was, uh,
was the one that first developed and proposed depth.
So, um, this is, uh, quite exciting.
So, uh, to summarize what have we learned so far.
We have learned that, uh,
about classical graph neural network layers and how
they are defined and what kind of components do they include.
Um, and they can often, you know,
we can often get better performance by- by,
uh, combining different aspects, uh,
in terms of design,
um, and we can also include,
as I will talk about later,
other modern deep learning modules into graph neural network design, right?
Like for example, we'll- I'm going to talk more about batch normalization, dropout,
you can choose different activation function,
attention, as well as aggregation.
So these are kind of transformations and components you can choose to pick,
um, in order to design an effective architecture for your- for your problem.
So as I mentioned,
many kind of modern deep learning modules or techniques can
be incorporated or generalized to graph neural networks.
Um, there is- like for example,
I'm going to talk about- next about batch normalization,
which stabilizes neural network training.
I'm going to talk about dropout that allows us to prevent over-fitting.
Um, we talked about attention to- attention mechanism that
controls the importance of messages coming from different parts of the neighborhood,
um, and we can also talk about skip connections, uh, and so on.
So, um, let me talk about some of these concepts,
uh, in a bit more, uh, detail.
So first, I wanna talk about the notion of batch normalization and
the goal of batch normalization is to stabilize training of graph neural networks.
And the idea is that given a batch of inputs,
given a batch of data points, in our case,
a batch of node embeddings,
we wanna re-center node embeddings to zero mean and scale them to have unit variance.
So to- to be very precise what we mean by this,
I'm given a set of inputs,
in our case this would be vectors of, uh, node embeddings.
I then can compute what for- for every coordinate,
what is the mean value of that coordinate and what is
the variance along that coordinate, uh,
of the vector, uh,
acro- across this n, uh,
input points, X, that are part of a- of a mini-batch.
And then, um, I can also have, um, in this case,
I can have, um, uh, um,
uh, then, uh, you know, there is the input,
there is the output that are two trainable parameters, gamma,
uh, and beta, and then I can,
uh, come up with the output that is simply,
um, I take these inputs x,
I standardize them in a sense that I subtract the mean and divide by
the variance along that dimension so now these X's have, uh,
0 mean and unit variance and then I can further learn how to
transform them by basically linearly transforming them by multiplying with gamma and,
uh, adding a bias factor,
uh- uh, bias term, beta.
And I do this independently for every coordinate,
for every dimension of,
uh, every data point of every embedding,
i, that is in the mini batch.
So to summarize, batch normalization stabilizes training,
it first standardizes the data.
So stand- to standardize means subtract the mean,
divide by the, uh,
by the standard deviation.
So this means now X has 0 mean and variance of 1,
so unit variance, and then I can also learn, uh, these parameters,
beta and gamma, that now linearly transform X
along each dimension and this is now the output of the, uh, batch normalization.
Um, the second technique I wanna discuss is called, uh, dropout.
And the idea- and what this allows us to do in neural networks is prevent over-fitting.
Um, and the idea is that during training,
with some small probability P,
a random set of neurons will be set to 0.
Um, and, uh, during testing,
we are going to use all the neurons,
uh, of the network.
So the idea is if you have a,
let's say, feed-forward neural network,
the idea is that some of the neurons you set to 0 so
that information now flows only between the neutrons that are not,
uh, set to 0.
And the idea here is that this forces the neural network to be
more robust to corrupted data or to corrupted inputs.
That's, uh, the idea and because the neural network is now more robust,
you- it prevents neural network, uh, from over-fitting.
In a graph neural network,
dropout is applied to the linear layer in the message function.
So the idea is that when we take the message from, uh,
node u from the previous layer and we were multiplying it with this matrix W here,
uh, to this linear layer,
to this W, we can now apply dropout, right?
Rather than saying here are the inputs multiplied with W to get the outputs m,
you can now basically set some of the parts,
uh, of the input, uh,
um, as well as the output, uh,
to 0, and this way, mimic the dropout.
That's the- that's the idea,
and, uh, as I said,
in terms of dropout,
what it does is it helps with, um,
preventing over-fitting of, uh, neural networks.
The next component of our graph
neural network layer is in terms of non-linear activation function.
Right, and the idea is that we apply this, uh,
activation to each dimension of the,
uh, of the embedding X.
What we can do is apply a rectified linear unit,
which is defined simply as the maximum of x and 0,
so the way you can think of it based on the inputs- input x,
the red line gives you the output.
So if the x is negative,
the output is 0,
and if x is positive, the output is,
uh, x itself, uh,
and this is most commonly used, um, activation function.
And we can also apply a sigmoid activation function.
A sigmoid is defined here,
here is its shape.
So as a function of the input,
the output will be- will be on value 0 to
1 and this basically means that you can take a
x that- that has a domain from minus infinity to plus infinity and kind
of transform it into something- to- to the bounded output from 0 to 1.
And this is used when you wanna restrict the range of
your embeddings when you wanna restrict the range of your output.
And then what empirically works best is called a parametric ReLU, uh,
and parametric ReLU is defined as the maximum of
x and 0 plus some trainable parameter alpha,
minimum of x and 0.
So this basically means that empirically,
uh, if x is greater than 0, uh,
you output x itself and if it's less than 0,
you- you output some x multiplied by some coefficient,
uh, a, in this case.
This is not an attention weight,
this is a different coefficient, uh, we trained,
and now the shape of the parametric ReLU looks like I show
here and empirically this works better than ReLU because you can train this,
uh, parameter, uh, A.
So, uh, to summarize what we have discussed and what we have learned so far,
we talked about, uh,
modern deep learning modules that can be included into
graph neural network layers to achieve even better performance,
we discussed about linear transformations,
batch normalization, dropout, different activation functions,
and we also discussed the, uh,
attention mechanism as well as different ways,
uh, then to aggregate, uh, the messages.
Um, if you wanna play with these different,
um, architectural choices in an easy way,
we have actually developed a package called GraphGym that basically allows you to,
very quickly and easily try out, uh,
and test out different design choices to find the one that works best,
uh, on your, uh, individual, uh, problem.
So, uh, if you click this, this is a link,
it will lead you to GitHub, um,
and you can play with this code to see
how different design choices make a practical difference,
uh, to your own, uh,
application or use case.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 07.3 - Stacking layers of a GNN.txt
So, uh, good.
So far we talked about a single graph neural network layer.
What I'm going to talk about next is to talk about, uh,
how do we stack, uh,
layers into a multi-layer,
uh, graph neural network.
So, we first talked about, uh,
designing and defining a single layer of a graph neural network,
where we said that it comp- it composes of
a message transformation operation and a message aggregation operation.
We also talked about additional,
uh, things you can add to training,
which is like batch normalization,
choice of different activation functions,
choice of L2 normalization,
um, things like that.
What we want to talk next is to talk about how can you stack these, uh,
layers one on top of each other and, uh, for example,
how can you add or skip connections to graph neural networks?
So that's the, uh,
topic I wanna, uh, discuss next.
So the question is,
how do I construct a graph neural network based on
the single layer that I have, already defined.
And, ah, a standard way,
a usual way would be to stack graph neural networks sequentially.
So basically the idea is I- under- as the embedding of node at layer 0,
I simply use the raw node features of the node and then I'm, you know,
transporting them individually layer by layer up to some,
uh, number of layers.
For example, in this case,
we have now a three-layer, uh,
graph neural network where on the input I get the raw node features x.
They get transformed into the embedding at a level 3 of, uh,
node v. That is actually an important issue in,
uh, graph neural networks that prevents us from stacking too many layers, uh, together.
And the important point here that I want to make
is introduce you the notion of over-smoothing,
um, and discuss how to prevent it.
And then one thing I wanna also make, uh,
a point about is that the depth of the graph neural network
is different than the depth in terms of number of layers in,
let's say convolutional neural networks.
So the depth of graph neural networks really tells me how many
hops away in the network do I go to collect the information.
And it doesn't necessarily say how complex or how expressive, uh,
the entire network is,
because that depends on the design of each individual layer of the graph neural network.
So kind of the point I'm trying to make,
the notion of a layer in
a graph neural network is different than the notion of a layer in,
let's say, a convolutional neural network.
So the issue of stacking many GNN layers together is
that GNNs tend to suffer from what is called an over-smoothing problem.
And the over-smoothing problem is that kind of node embeddings,
you know, converge to the same or very similar value.
And the reason why this is happening is because,
uh, if the receptive fields,
as I'm going to define them later of the- of the networks are too big,
then basically all the network- all the neural networks collect the same information,
so at the end, the final output is also the same for all different nodes.
And we don't want to have this problem of over-smoothing
because we want embeddings for different nodes to be different.
So let me tell you more about what is an over-smoothing problem
and why does it happen and how do we, uh, remedy it.
So first, we need to define this notion of a receptive field.
A receptive field, uh,
is a set of nodes that determine the embedding of the node of interest.
So in a K layer GNN, uh,
each node has a receptive field of k-hops- k-hop neighborhood around that node.
And this becomes, uh,
important because for example,
if you say I wanted to link prediction between the two yellow nodes,
uh, in this graph.
Then the question is,
how ma- as I increase the,
uh, depth of the network,
how many, uh, and look at
the corresponding computation graphs of the two, uh, yellow nodes?
uh, the question is, how, um,
how big, uh, how big is the receptive field, right?
So for example, what I'm trying to show here is for a single node, uh,
the receptive field at one layer,
so one hop away is, you know,
this four, uh, five different,
uh, red nodes denoted here.
If I say now, let's do a two-layer neural network.
This is now the receptive field.
It's all neighbors and all the neighbors of neighbors.
It's like one-hop neighborhood and two-hop neighborhood.
And if I go to a three-layer neural network,
in this case of a small graph,
now, notice that basically,
majority or almost every node in the net- in the- in
the underlying network is part of my graph neural network architecture.
So this means that this yellow node is going to collect information from
every other node in the network to combi- to determine its own, uh, embedding.
Now, if you, for example wanna do link prediction, um,
then you, uh, you wanna say whether a pair of nodes is,
uh, connected or not.
And what is interesting in this case is that, um,
the- the number of neighbors that are shared grows very
quickly as we increase the number of hops in the graph neural network.
So now, uh, I have a different visualization here.
I have two nodes denoted by yellow and I
compute one-hop neighborhoods from each and I say what nodes are in the intersection?
What nodes are shared?
And here, one node is shared.
Then if I say, let's compute 2-hop neighborhood,
now, all these neighbors are shared.
And if I say how many neighbors are shared, uh,
between 3-hops, how many nodes are share- shared within three hops?
Again, you see that basically almost all the nodes are shared.
And the problem then becomes that as the network is aggregating all this information
and all the- all the nodes- all the graph neural networks basically get the same inputs,
it will be increasingly hard to differentiate between different nodes, uh, you know,
let's say the nodes that are- that are going to be connected in
the network and the nodes that are not connected in the network.
So, uh, you know,
how do we explain the notion of over-smoothing with this definition of a receptive field?
Uh, you know, we know that the embedding of a node is-
this determined by its receptive field, right?
And if two nodes have highly overlapping receptive fields,
there- there- then their embeddings are also going to be most likely, uh, similar.
So this means that if I stack many GNN layers together,
then it means nodes will have highly overlapping receptive fields, uh,
which means they will collect information from
the same part of the network and they will aggregate it in the same way.
So node embeddings will be highly similar.
So it means it can be very hard for us to distinguish between
different nodes and this is what we call an over-smoothing problem, right?
It's like you collect too much information from the neighborhood and then,
um, if you collect kind of too much,
everyone collects the same information,
so every node kind of has the same information,
computes the same embedding and it is very hard to differentiate between them.
So the question is,
how do we overcome over-smoothing?
Uh, first is that we are cautious about how many,
um, layers, how many GNN layers, uh, do we use.
So what this means that, unlike in, uh,
neural networks in other domains like
convolutional neural networks for image classification,
adding more layers to our graph neural network does not always skip.
So first, what we need to do, uh,
to determine how many layers is good is to analyze then the-
the amount of information we need to make a good prediction.
So basically, analyze different depths,
different receptive fields, and try to get a good, uh,
balance between the diameter of the network and the amount of
information that a single GNN is aggregating goal.
Because if the depth is too big, then basically,
the receptive field of a single node may basically be the entire, uh, network.
The second thing is that we wanna s- setup the number of GNN layers L to be, uh,
a bit, uh, more than the receptive field we, uh,
we like, but we don't wanna make L to be unnecessarily large.
Um, so that's one way,
uh, to deal with this.
Another way to deal with this is to say,
how do we enhance expressive power of a GNN if the number of layers is smaller?
The way we do this is the following.
Um, right, how do we make GNNs more- more
expressive if we cannot make them more expressive but making them deeper.
One option is to- to add more expressive power within a GNN layer.
So what this means is that in our previous examples, uh,
each transformation or aggregation function was only one linear transformation.
But we can make aggregation and transformation become deep neural networks by themselves.
So for example, we could make the aggregation operator and the transformation operator,
let's say a three-layer- um,
uh, multilayer perceptron network, uh,
and not just a simple, uh,
linear, uh, layer in the network.
In this way add, um,
express- ex- expressiveness, uh, to the neural network.
Right, so now our single layer graph neural network is really a three-layer,
uh, deep neural network, right?
So the notion of a layer in a GNN and a notion of
a layer in terms of transformations, uh, is different.
So another way how we can make shallow GNNs more expressive is to add,
uh, layers that do not pass messages.
So what this means is that a GNN does not
necessarily have to contain only GNN layers, right?
We can, for example, have, uh,
multilayer perceptron layers before and after the GNN layers.
And you can think of these as pre-processing and post-processing layers.
To give you the idea, right,
we could take the input- uh,
massive inputs- um, input features,
transform them through the preprocessing step of
multilayer perceptron, apply the graph neural network layers,
and then again have a couple of, um, um, multilayer,
uh, perceptron layers that do the post-processing of embeddings.
So we can think of these as pre-processing layers that are-
that are important when encoding node features.
For example, if node features represent images or text,
we would want to have an entire CNN here, for example.
Um, and then we have our post-processing layers, uh,
which are important when we are reasoning,
or- or transforming over whether the node, uh, embeddings.
Uh, this becomes important if you are doing,
for example, graph classification or knowledge graphs,
where the transformations here add a lot to
the expressive power of the graph neural networks, right?
So in practice, adding these pre-processing and post-processing layers,
uh, works great in practice.
So it means we are combining classical neural network layers with graph,
uh, neural network layers.
So, uh, the- the last way how we can,
um, uh, think about, uh,
shallower graph neural networks,
but being the more expressive is to add this notion of a skip connection, right?
And the observation from Over-Smoothing problem that I discussed was
that node embeddings in earlier GNN layers can sometime,
um, better differentiate between different nodes earlier,
meaning lower layer, uh, embeddings.
And the solution is that we can increase the impact of
earlier layers on the final known embedding to add the shortcuts,
uh, in the neural network,
or what do we mean by shortcuts is skip connections, right?
So if I go now back to my picture from the previous slide,
what I can add is,
when I have the, um- the GNN layers,
I can add this red connections that basically skip a layer and go from,
um connectly- directly connect,
let's say the GNN layer 1 to the GNN layer 3,
and they skip this layer 2, uh, in-between.
So the idea is that this is now a skip connection.
Uh, so the message now gets duplicated.
One message goes into the transformation layer and,
uh, weight update,
while the same message also gets dup- duplicated and just kind of
sent forward and then the two branches are summed up.
So before adding skip connections,
we simply took the message and transformed it.
Now with a skip connection,
we take the message, transform it,
but then sum it up or aggregate it with the untransformed, uh, message itself.
So that is, um,
an interesting, uh, approach, um, as well.
So why do we care about skip connections and why do skip connections work?
Um, intuition is that skip connections create what is called, uh, mixture models.
Mixture model in a sense that now your model is
a weighted combination of a previous layer and the current layer message.
In this way basically means that you- you have now mixing
together two different layers or two different, uh, models.
Um, there is a lot of skip connections to add, right?
If you, um- if you have, let's say,
n skip connections, then there is 2 to the n possible message-passing paths,
which really allows you to increase the expressive power and gives neural network,
uh, more flexibility in terms of how
messages are passed and how messages are, uh, aggregated.
Uh, and, uh, first right, um,
we can also automatically get a mixture of automa- uh, shallow GNNs,
as well as the deep GNNs through the,
uh, message-passing layers, right?
So basically what I mean by this is you could have a three-layer,
uh, neural network and then by adding skip connections,
you can basically now have the final output to
be some combination of a one-layer neural network,
a two-layer neural network,
A one-layer neural network fed into the, uh,
third layer of the neural network and all this aggregated,
um, into the final output.
So now you can think that the final output is a- is a- is a combination of, uh,
in this case, four different, uh,
neural network, uh, architectures, right?
And, you know, the way you can- you can think of it is to say, oh,
I have three layers,
I add these three skip connections.
What this really does is,
you can think of it now that you have
four different neural network architectures that you are mixing or adding together,
uh, for, uh- during learning.
So it's a very efficient representation that really you can think of it in this,
um- in this second way.
So how would you apply skip connections,
uh, to graph neural networks?
For example, if we take a classical, uh,
graph convolutional neural network, uh,
architecture and add skip connections to it,
uh, this is how it would look like.
Before in the standard GNN layer- GCN layer,
we take the messages from the neighbors, uh,
transform them, and, uh,
average them together and we can think of this as our f of x from the previous slide.
So a GCN layer with a skip connection would be the same f of x plus,
uh, the lay- the message, uh,
from the previous, uh, layer, right?
So here we are just adding in the message from the previous layer and then
passing it all through the, um, non-linearity.
So this is- what this means is we added
the skip connection here in a single layer through this,
uh, blue part, uh, here.
And of course, um,
we have many other options for skip connections.
We can make them skip one layer,
we can make them skip, uh, multiple layers.
Um, there is a- there is an interesting paper, uh,
from ICML called Jumping Knowledge Networks, where for example,
the proposal is to add these skips from a given layer
all the way to the- to the last layer where you basically now can think,
Oh, I have a o- one-layer neural network,
I have a two-layer neural network,
I have a three-layer neural network.
I take their inputs and,
uh- and then aggregate them to get the final, uh, output.
So by basically directly skipping to the- to the last- to the finals- final layer, um,
the final layer can then aggregate all these embeddings from
neural networks of different depths and this way basically,
uh, determine what information is more important,
uh, for, let's say for the prediction task.
Is it the information from very close by
nearby nodes or is it really about aggregating bigger parts of the network?
And by adding these skip connections,
basically this allows us to weigh or to determine what information is more important,
something that has been aggregated across multiple hops, or something that has been,
let say, aggregated over zero hops or over only a single hop?
Um, and that is very interesting and again,
adds to expressivity, uh, and,
uh, improves the performance,
uh, of graph neural, uh, networks.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 08.1 - Graph Augmentation for GNNs.txt
Welcome, everyone, to the class.
Um, we are going to continue with the discussion of, uh,
what kind of design choices that we have when, uh,
training or, uh, designing graph neural networks.
Um, and then in the last part of the lecture today,
we're going to talk about graph neural network training, um,
and various other aspects of how do we make these,
um, models, uh, work.
So to start, um, to remind everyone,
we are discussing deep graph, uh,
encoders, in particular, graph neural networks,
where the idea is that given the input graph on the left,
we wanna transform it through several layers of non-linear, uh, transformations.
Uh, several layers of a neural network to, um,
come up with good predictions at the level of node's, uh,
address, as well as entire graphs.
Um, and the, the formalism we have
defined is called graph neural network, where basically,
for every node in the network,
we define the computation graph that is
based on the network neighborhood around that given target node.
So essentially this means that
the input graph network structure
around the target node defines the neural network structure.
And then we discussed that now,
in order to make this neural network, uh,
architecture work because every node gets to define its own neural network architecture,
its own computational graph that will depend on the position of the node in the network.
Then what we can do in the, um,
to make this work is we have to define several different operators,
um, in the architecture.
So first we said we need to define what we call a message passing function,
a message transformation function that will take the message from the child,
transform it, and pass it towards the parent.
We have to define the notion of message aggregation that
will take the transformed messages from the children
and aggregate them in
an order invariant way to produce the- the one combined message from the children.
Then we said that when this message arrives to the parent,
we need to decide how to combine it with the parent's own message
from the previous level to then create the embedding of the node,
which can then be passed on.
So this is how we defined a single layer of a graph neural network.
Then we discussed how to combine or how do you link, or stack multiple layers together.
Um, and what we are going to discuss today,
is the point Number 4 around what kind of graph and
feature augmentation can we create to shape the structure of this neural network?
As well as we are going to talk about
learning objectives and how to make the training work.
So that's, uh, the plan for today.
So first, let's talk about graph augmentation for,
uh, graph neural networks.
So the idea is that raw input graph does not
necessarily need to present the underlying computation graph, all right?
So what I discussed so far was that if I wanna create
a graph neural network for a given target node in the network,
then I take the information from the neighbors of it.
Here are the neighbors,
and then each of the neighbors takes information from its own neighbors.
And this defines the graph neural network.
However, um, this translation from
the input graph structure to
the graph neural network structure does not need to be kind of one-to-one.
I don't need to take the raw input graph and,
uh, interpret it as the computational graph.
I can use, um,
various kinds of techniques to create the computation graph of the graph neural network.
And the two techniques we are going to talk about is
graph feature augmentation and graph structure augmentation.
So what we assume so far as I said,
is that the raw input graph directly defines
the computational graph of the graph neural network.
And there are many good reasons why we would- why we would want to break this assumption.
So, um, we would wanna break it at the level of node features.
Many times, for example,
input graphs may lack, um,
features, attributes, perhaps you wanna- sometimes,
um, the features are also, uh,
hard to encode so we may wanna help the neural network to learn a given concept easier.
And then in terms of graph structure, sometimes,
graphs- input graphs tend to be too sparse and it's
inefficient to do message passing over
a very sparse graph, it would take a lot of iterations,
a lot of GNN depth.
Sometimes they are too dense and the message passing becomes too costly.
If you think, for example,
doing message passing on an - on top of
an Instagram or Twitter network and you hit the Kim Kardashian node,
then you need to aggregate from all her gazillions of followers, right?
So that is very expensive.
So the question is when you hit a,
a high degree node,
what do you- what do you do?
Do you really need to aggregate from all the neighbors of that high degree node?
Or perhaps can you just select a subset of the neighbors?
And then another important consideration is that sometimes this graph is just too large,
so we cannot fit the computation graph into the GPU memory.
And again, certain augmentation techniques are needed.
So basically, the point is that sometimes
it is unlikely that the input graph happens to be
the optimal computation graph for computing
GNN-based embeddings and the techniques we are
going to discuss next will give you some ideas.
What can we do to improve the structure of the graph so that it
lends better to the graph neural network embeddings?
So we are going to talk about, uh,
augmentation approaches and we are going to talk
about in particular the graph feature augmentation, where,
um, it- it can be the case that the input graph lacks attributes,
lacks features and we are going to create features so that,
uh, GNN has easier time to learn.
And then we'll also talk about the graph structure augmentation.
As I said, if graph is too sparse,
we can- we will be able to add virtual nodes and edges.
If it's too dense,
we can decide to do some kind of sampling of neighbors when doing message-passing.
And if graph is too large,
then we can subsample subgraphs to compute the embeddings.
And this last point we are going to,
to talk in more detail when we discuss scaling up GNNs.
But these are some of the techniques we are going to learn about today.
So why do we need feature augmentation, right?
So first, I wanna talk about graph feature augmentation.
So let's discuss why do we need this.
Sometimes, input graphs do not have any node features.
This is common, right?
If the input is just the graph adjacency matrix.
And what I'm going to discuss next is several standard approaches.
How do you deal with this situation and what can you do?
So first idea is that you simply assign a constant value,
a constant feature to every node.
So basically all the nodes have the same future value, value of 1.
And then if you think of what aggregation does,
it basically counts how many neighbors,
how do- does a node have at level- at Level 1?
How many do they have at Level 2?
How many do they have at Level 3?
So this would, in some sense,
allow you still to capture some notion of how does
the network neighborhood structure about a given node look like,
even though all the nodes have the same feature,
which is uh, which has a value 1.
Another idea that you can do is to assign unique IDs to nodes.
So basically these IDs are then converted to one-hot vectors, right?
So basically it means if you have a network on, um, six nodes,
then the idea is that you can assign a one-hot encoding to every node in the network.
So what I mean by this is now a feature vector for every node in
the network is simply a six-dimensional binary vector where you know,
node ID Number 5 has a- has a value of 1 up here.
If this is, you know, the node ID Number 5.
Um, also notice that this ordering of the nodes is totally arbitrary.
So there are some issues with one-hot encodings because it might
be hard or impossible to generalize them across different graphs.
But if you work with a single graph,
then this type of approach might be fine because
every node basically has now a unique one-hot encoding.
There is a flag value 1 at the ID of that single node.
And this now allows you to,to learn very expressive models because the models know
actually what are the IDs of the neighbors of the node in the network.
Of course, it might be costly because now your feature representation, your,
your number of attributes that the node has number of features,
the node has equals the number of nodes in the network.
So- so, um, that,
that is quite an expensive feature representation
for a node if the network is large. [NOISE]
So how do these,
uh, two approaches compare, right?
How does this adding a constant feature versus one-hot encoding,
uh, how do they compare, right?
In terms of their expressive power constant feature,
so every node having a value of 1,
um, has kind of medium expressive power, right?
Not- all nodes are identical,
but as- as we will talk about this later, uh,
GNN can still learn about
the graph structure and the neighborhood structure around the node, right?
In some sense, let's say an aggregation function like summation allows you to say,
"Hi, I have three- if this is the node of interest,
I have three neighbors at level 1.
I have, let's say, uh,
two neigh- two neighbors at level 2," and so on and so forth, right?
So this is what this we'll be able to learn.
So it's still able to capture some simple part of the graph, uh, structure.
Uh, one-hot encoding, uh,
has high expressive power, right?
Each node has a unique ID,
so a node's specific information can be stored or can be retrieved or learned.
So you can learn, "Oh,
I have a neighbor with ID number 2," and
perhaps that is important to determine your own label.
So, um, the expressive power is very high.
Um, you know, do we allow for- does this approach allow for,
uh, in, uh, an inductive learning capability?
What- what does this mean is, uh,
generalization to unseen nodes or generalization to, um,
nodes that are not yet part of the network or
generalization to a new graph that I have never seen during training.
Of course, the constant feature has high inductive, uh,
learning, uh, capability, because it's simple to generalize the new nodes, to new graphs.
We assign constant feature to them and just apply a GNN.
While for example, in, um, uh,
in the one-hot encoding case,
uh, we cannot really do that, right?
Because, uh, we cannot generalize to new nodes, right?
New nodes introduce new IDs.
Those IDs were not part of the training.
The GNN does not know how to,
uh, embed unseen nodes.
Um, so this is the- this is the issue,
um, in terms of one-hot encodings.
You need to know the entire node set at the time of training,
and you need to know the edges of that node set at the time of training.
In terms of computational cost, um,
as I said, uh,
constant node feature is very cheap.
It's just one- one value per node,
while the one-hot encoding has high because each node has a,
um, uh, feature vector that is length of the size of the network, right?
Its, uh, number of vertices is the dimensionality of the feature vector,
so we cannot apply these to larger graphs.
And then, you know, when would you apply one or the other?
You would, um, you can apply the constant feature essentially to any graph,
uh, whenever you care about inductive setting and generalizing to new nodes.
Uh, but, uh, the expressive power of the model will be limited.
On the other hand, one-hot encoding, um,
is very powerful, allows you to learn much more intricate structure around the network.
But it can only be applied to small graphs and to
transductive settings basically where all the nodes
and all the edges are known at the time of the training of the model.
So this was in terms of, uh,
feature- one idea of feature augmentation when we have no features, uh, on the nodes.
Another motivation for why we would want to do
some feature augmentation is that sometimes,
uh, certain structures are hard to learn for a GNN.
So sometimes, we actually want to encode a bit of
a graph structure into the node attribute vector as well.
So, uh, the idea, for example,
is that, um, to give you an example,
like kind of an edge example is that it's very hard for a GNN to count,
uh, you know, what's the length of a cycle a node is on?
So, um, and the question is, you know,
could a- could a GNN learn the length of a cycle a given node, uh, resides in?
And unless you have discriminative node features,
uh, this is not possible, right?
So what I mean by it is, for example,
here in this example,
node v_1 resides on a cycle of length 3,
while here this, the node v_1 resides on a cycle of length 4.
And in both cases, right,
v_1 has degree 2,
and its neighbors have degree 2.
So it's kind of the question is, you know,
how do I- how does this node know that now it's in a cycle of,
uh, length, uh, 4 versus 3.
Something like this is very important, for example,
in chemistry because these- these could be different kinds of, uh,
chemical structures or different kinds of, uh, ring structures.
And, uh, the reason why a, uh, uh,
plain GNN cannot differentiate between,
you know, node 1 in, uh,
a cycle of length 3 versus a cycle of leng- length 4
is that if you look at the GNN computation graph,
re- both- for both of these nodes V_1 and V_2,
the computation graph is exactly the same.
Meaning, V_1- V_1 and V_2 have two neighbors,
um, each, and then, you know,
these neighbors have, uh,
one neighbor each and the computation graph will always look like this.
Uh, unless, right, you have some way to discriminate nodes based on the features.
But if the nodes have the same,
um, set of, uh,
features that not- you cannot discriminate them based on their attributes,
then you cannot learn,
uh, to discriminate node V_1 from V_2.
They will- from the GNN point of view,
they will all, uh, look the same.
I'm going to go into more, uh,
depth, uh, uh, around this,
uh, this example and, er,
what are some very important implications of it and
consequences of it when we are going to discuss the theory of,
uh, graph neural networks.
But for now, uh,
the important thing is to see to understand that a GNN has a hard time capturing, um,
or is not able to capture whether a node is on
a length 3 cycle or a length 4 cycle unless these nodes,
um, on the cycle would have some discriminative features.
And the reason why we are not able to distinguish between
these two nodes or why GNN is not able to dis- distinguish,
uh, between these nodes is because computation graph looks,
uh, the same in both cases, right?
A node has two neighbors,
and then each of these neighbors has two other neighbors.
And if- if all the neighbors look the same,
they don't have any discriminative colors to them,
then computation graphs, in all cases,
will look the same so,
uh, the embeddings will be the same.
So it doesn't matter whether you are part of
a cycle or you are a part of a infinite length,
uh, chain, the computation graph will always be the same.
So the GNN won't be able to distinguish the nodes again,
unless there is some discrimination between the nodes.
So if nodes have different colors,
then a GNN could,
uh, capture the pattern.
So, um, what is the solution?
The solution is to create a feature vector for every node that would,
uh, that would, for example,
give me the cycle count, right?
So basically, I would augment the node features with the cycle count information.
So one idea, for example would be is to create this,
uh, um, vector where, you know,
this is the number of cycles of length 0 the node participates in,
number of cycles of length 1,
length 2, length 3, right?
So length 1 is a self-loop,
length 2 would be, um,
if in a directed graph would be a, uh,
reciprocated connection, length 3 is a,
um, a triangle, you know,
length 4 is a square, right?
And, uh, you could now, uh,
append this type of feature vector to
whatever feature vector you already have, uh, for the nodes,
and this way increase the expressive power of the graph neural network,
especially if your intuition is that, let's say,
cycle information, uh, is important.
And of course, there are many other, uh,
commonly used, uh, uh,
techniques to augment features.
People very much like to include node degree.
It's a very simple to compute feature, but again,
allows you to disti- distinguish between different nodes.
You can include clustering coefficient that essentially
counts how many cycles of length 3 a node participates in.
So this is, um, triangle counting.
Uh, you could also have other types of, uh,
features like PageRank or,
uh, node centrality metrics, right?
So essentially, any features we have introduced in Lecture 2, um,
you could include to augment, uh,
the GNN to help it learn the network structure,
uh, better and, uh, faster, right?
So in some sense, in many cases,
the goal of the- of the machine learning, uh,
scientist is to essentially understand the intuition betw- be- behind
the learning algorithm and tries to help the- the algorithm to learn the patterns,
perhaps, we as domain scientists would know are important.
And by encoding some of the graph features,
many times, you can very much, uh,
speed up and improve the performance of the model because you kind of help,
uh, you know, you point the model to the- to the places where there might be good signal.
So this was about augmenting the features, uh,
of the node, and we talked about adding a constant feature,
we talked about adding, um,
one-hot encoding, and we also talked about,
um, adding various kinds of graph, uh,
structure information like the cycle count or node degree,
uh, to- to augment the node feature information.
Now, I'm going to switch, uh,
gears and I'm going to talk about adding and changing the graph structure information.
So we are going to augment
the underlying graph structure again to help, uh, with to the learning.
The way we are going to do this is to add virtual nodes and virtual edges.
So the first motivation we wanna discuss is we wanna augment sparse graphs,
so we wanna add a virtual address.
A common approach, for example,
would be to connect two hop neighbors via virtual edges.
So the intuition is,
or one way to say this is that instead of using
the adjacency matrix A of a- for a GNN computation,
we are going to use A plus A squared, right?
If you, again, uh,
remember early on in the course,
maybe Lecture 2, 3,
we discussed that powering the adjacency matrix, um, counts,
um, the number of, uh, nodes that,
uh, are neighbors at level 2,
level 3, and so on.
So by basically adding- by powering the matrix and adding, uh,
A adding it to the adjacency matrix,
now basically we connect all the nodes that are two-hop neighbors.
Um, and this is very interesting, for example,
especially in bipartite graphs.
Because if in a bipartite graph where for example,
authors and the papers they author,
you create a square,
then basically you create a projection and you
created a paper co-authorship network,
or an author colle- collaboration network, right?
So this would mean that you either connect
two authors that have written at least one paper together,
or you connect two papers if they were written, uh, by the same author.
It just depends do you do, um,
you know,  A times A transpose or do you do, uh,
A transpose times A,
in a case of a bipartite graph because the adjacency matrix, uh, won't be square.
So, uh, that's- that's one idea how you can kind of include
additional information and what this will help in
the graph neural network is that rather than, you know,
if you think about message passing,
an author sending a message to the paper and then paper
sending it back to the author by connecting two authors,
they will be able to directly exchange messages,
which means that the depth- the number of layers of the graph neural network will be,
uh, able to be smaller, um,
and you'll be able to train it, uh, faster.
Of course, the um,
- the issue will then become that you have too few- that you will have
too many neighbors to aggregate from and that may add er, more complexity.
But we'll talk about how to fix that um, er later.
So if graphs are too sparse,
er, as I said,
one idea is to- to connect nodes that are-that are
two edges-virtual edges between length 2 or length 3 connected nodes.
Another idea is to add a virtual node um,
and the virtual node will then connect to, let's say,
all or some carefully chosen subset of the nodes in the graph, right?
So for example, imagine you have
a super sparse graph where have two nodes are very far apart in the graph, right?
They are, let's say, ten hops apart, right?
And now if one node needs to send a message to the other node,
you need a length, you need
a ten layer graph neural network to be able to allow for that communication.
But in some cases,
you may know that these two nodes actually need to communicate.
Even though they are farther apart in the original graph,
they need to send messages to each other, right?
One depends on the other.
So what you can do in that case is you can create a virtual node and then connect,
for example, several nodes to it.
And this way, you can connect nodes that are very far in the original graph structure er,
to be able to communicate with each other much more efficiently, right?
So basically, after adding a virtual node,
er, all the nodes will have a smaller distance to each other.
So you'll- the message passing will be more er,
- more efficient, it will happen faster.
The depth of the graph neural network won't have to be er, that large.
Er, so that's another technique that sometimes um, is er,
-is-is a good idea to have in your toolbox um,
if the basic approaches don't work.
And then er, the last thing I want to talk about- about graph structure information is
not when you have too few edges in the graph and you want
to kind of make the message passing er, more efficient.
Er, the question becomes,
what if you have too many edges?
What if the graph is too large, right?
Again, think of my er,
Kim Kardashian example, right?
Or Lady Gaga used to be er,
the highest degree node in the Twitter network a few years ago,
but I think she is not number one anymore.
But the point is, right, you have these high degree nodes in networks and
aggregating messages from millions- over tens of millions or hundreds of millions of er,
people that are connected to it can become er, quite er, expensive.
So um- so far, right?
We said, let's use all the nodes, all the neighbors er,
for message passing when defining
the graph neural network and the idea that we are going to explore here and
introduce is, what if I sample node's neighborhood er, for message passing?
And of course I could do random sampling,
but it turns out there are far better heuristics er,
that allow you to really carefully select
what neighbors to collect information from and what neighbors to ignore.
So um, here is the idea.
So the idea is that, for example,
what if we randomly choose two neighbors to pass messages from in a given layer?
So for example, for our neighbors of node A,
we-we would decide that out of the three neighbors,
we are only going to select two of them and ignore the third one.
So this would, for example,
in this case mean that A only collects information from B and D but ignores
the C. So now our message-passing computation graph would look like this.
Why is this good? This is good because now computation graph is much smaller.
Um, of course, why is it bad?
It's because perhaps node C has
very important information that would allow
us to make a better prediction at node A,
but because we ignored it,
this will be- it will be harder for the neural network er,
to learn that better, right?
So this is kind of the trade-off here is,
yes, you gain computational efficiency,
but in the worst case,
you kind of lose some of the expressive power er,
because you-you-you dropped out er,
some edges, you dropped out some information that might be important.
And of course, in practice,
um, if you have a super high degree node,
you can really sub-sample how many neighbors you- you er,
aggregate information from because you really want to kind of aggregate from
important neighbors and from all the kind of noisy unimportant ones,
er, you can er, - you can ignore them.
That's kind of the intuition.
So- and of course we can, um, er,
do this sampling differently er, every time.
We could even make it, er,
such that sampling changes between er,
layers and this way,
the network- the aggregation functions actually become er,
robust to how many neighbors do you sam- do you collect the information from, right?
So the idea would be, for example,
in the-in the next layer,
or in the next er, minute- er,
in the next er, epoch of training,
we can sample different nodes er,
for the same node A , right?
So what this would mean is that, for example,
now we re-sample and we decide to collect from C and D but ignore er,
B and this is now how the computation graph would look
like and this is also good because it adds a
robustness to our neural network training approach, right?
It er,-it now means that er,
A will be able to collect information-will learn how to robustly collect
information from some subsets of its- of its neighbors and won't er,
suffer to much if, for example,
an edge is er, missing in the network.
So that's also, um,
one reason why this er,
neighborhood sampling, er, is a good approach.
Um, and why is this interesting?
It's because in expectation,
right after a lot of different er,
random samplings, we will get embeddings
similar to the case when all nodes are being used.
Er, but the benefit is that this greatly reduces computational cost and,
you know, in the small graphs,
this might not be er,
so obvious but if you-if, you know,
node A has 30 million neighbors,
then we cannot aggregate from 30 million.
The question is, can we decide what are the top 100,
maybe top 1,000 most important nodes?
Most important, let's say friends,
if this is a social network or the true friends er, of this person,
and we want to aggregate information er,
from them rather than from all the kind of, er,
random followers, er, all over the world, right?
Um, and by doing this sub-sampling,
we can make the computation graphs much, much,
much smaller, which allows us to scale GNNs to massive graphs.
And this is very important in industrial applications
like recommender systems and social networks,
where you have, you know,
networks of billions and tens of billions of er,
nodes and edges and you need these  kind of,
um, techniques to be able to scale.
Er, and in practice, this is a very good approach to scale
up- to scale up graph neural networks.
So er, this is what I wanted to say about neighborhood sampling er,
and give you this er, example.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 08.2 - Training Graph Neural Networks.txt
Next, I wanna talk about how do we train,
uh, graph neural networks, right?
So far we talked about how do we augment
the feature vector of the node and how can we augment the- the graph structure.
And we talked about how to augment the graph structure by adding edges to improve
message-passing or how do we drop edges to increase the efficiency,
especially in natural graph social networks where you have high degree nodes,
you don't wanna aggregate from the entire neighborhood of the node,
but you wanna kind of carefully sub-select,
uh, the part of the network to aggregate from.
So, uh, this is the reason,
uh, why you wanna do these augmentations.
Now I wanna talk more about how do you do the training?
How do you deal with the outputs?
How do you define, uh,
the loss functions, measure performance, and so on?
So the next talk is,
um, how do we train a GNN, right?
Like, what kind of learning objective do we wanna define and,
um, how are we going to, uh,
do, uh, all these, uh, together?
So GNN training pipeline has the following, uh, steps, right?
So far we talked about the input graph,
we talked about how to define the graph neural network,
and we talked about how the graph neural network produces node embeddings.
What we haven't talked about yet is
how do you get from node embeddings to the actual prediction?
And then once you have the predictions,
how do you evaluate them based against some ground truth labels?
And how do you compute the loss,
or how do you define the losses,
the discrepancy between the predictions and the true labels?
Right? Um, so far we only said, aha,
GNN produces a set of node embeddings, right?
Which means that this is a- a representation of node L at the final layer,
layer L, uh, of the graph neural network.
And I can just think of this as, you know,
some representations, some vectors attached to the nodes of the network.
Now, the question is, uh, you know,
how is this second part defined?
What do we do here in terms of prediction heads, evaluation matrix,
where do the labels come from,
and what is the loss function we are going to optimize?
So let's first talk about the prediction head, right?
Uh, prediction head, this means the output of the g- of the- of the final model,
um, can have di- we can have different prediction heads.
We can have node-level prediction heads.
We can have, er, link-level,
edge-level, as well as entire graph-level prediction heads.
So let me talk about,
uh, this, uh, first.
So, er, for prediction head,
the idea is that different tasks require different types of,
uh, prediction outputs, right?
As I said, we can have our entire graph-level,
individual node-level, or, um,
edge-level, which is a pairwise between a pair of nodes.
So for the node-level of prediction,
we can directly make prediction using node embeddings.
So basically after a graph neural network computation,
we have a d-dimensional node embedding,
uh, for every node in the network.
Um, and if, for example,
we wanna make a k-way prediction,
which would be basically a classification of nodes
among k different classes or k different categories,
um, this would be one way.
Or perhaps we wanna regress,
uh, against 10, uh, sorry,
k different targets, k different,
uh, characteristics of that node.
Uh, the idea would be quite, uh, simple.
We just say, um, you know,
the output, er, head of, uh,
for a given node is simply some, er,
matrix time the- times the em- final embedding of that node, right?
So this basically means that W will- will map node embeddings, uh,
from this embedding space to the- to the prediction,
uh, to the prediction space.
To the, in this scalar- case, let's say, uh,
k-dimensional output because we are interested in,
uh, k-dimensional, um, uh,
prediction, so a k-way, uh, prediction.
Um, in or- one more thing I will add for the rest of the lecture,
I'm going to use this hat symbol to denote
the predicted value versus the ground truth value, right?
So whenever I use a hat,
this means this is a value predicted by the model,
and then I can go and compare y hat with y,
where y is the true- true label and y hat is the predicted, uh, label, right?
And now that I have y-hat,
I can compare it to, uh,
y and I can compute, uh, the loss,
the discrepancy between the prediction, uh, and the truth.
This is for node-level tasks.
For edge-level tasks, we have to make a prediction using pairs of node embeddings, right?
So, again, suppose we wanna make a k-way prediction,
then what we need is a- is a prediction head that takes
the embedding of one node and the other node and returns, uh, y hat.
Now, y hat is, uh,
defined on, uh, pairs of nodes.
This will be, for example, for, uh, link prediction.
So let me tell you what are some options for creating this,
uh, um, edge-level, uh,
head, uh, for prediction.
So one option is that we simply concatenate, uh,
embeddings of nodes u and v and then apply a linear,
uh, layer, a linear transformation, right?
And, ah, we have seen this, er,
this idea already in graph attention, right?
We said when we computed the attention between nodes u and v,
we simply concatenated the embeddings,
passed them through a linear layer,
um, and that gave us, uh,
the prediction of the attention score between,
uh, a pair of nodes.
Here, we can use the same,
the same idea, where basically we can take the embeddings of u and v,
concatenate them, basically just join them together,
and then apply a linear predictor on top of this.
So basically multiply this with the matrix and
perhaps send through a non-linearity or anything like that,
like a sigmoid or a softmax if we like, right?
So, uh, idea would be that,
um, the prediction is simply,
um, you know, it's a linear function that then takes the, um, h 1, er,
h of u and h of v, concatenates them,
um, and up- and maps this, uh, er,
to the- 2D dimensional embedding into a k-way,
uh, prediction or a k-dimensional output.
Another idea, uh, rather than concatenating is also we can do,
uh, a dot product, right?
So we basically say,
our prediction between u and v is simply a dot product between their embeddings.
If I simply do the dot product between the embeddings,
then I get a single scalar output.
So this would be a one-way prediction,
like link classification or link prediction.
Is thi- is that our link or not, right?
So basically just that a, um,
one variable kind of binary, uh, classification.
Now, if I wanna have a k-way prediction, if I wanna, for example,
predict the type of the link and I have multiple types,
er, then I would basically have this kind of, uh,
uh, al- almost similar to this kind of multi-hat prediction,
where basically I can have a different, uh, um,
uh, matrix, uh, W that is trainable,
um, and I have one for every output class, right?
So for every, uh, output, er, class,
I would have a different, uh,
matrix W that essentially,
the way you can think of it is it takes, er,
let's say the vector u and then it transforms it by shrinking or extending,
rotating, and translating it,
and then, uh, multiplying that- that with, ah, h of, uh,
v. So it's still a dot product,
but the input vector gets transformed, right?
And, um, every- every class gets to learn its own transformation,
how to basically, uh,
rotate, uh, translate, um,
and- and shrink or expand the vector so that the dot product,
um, is, uh, is, uh,
such that the predict- that the value,
the output values, uh,
are well, um, are well predicted.
And then, right, once I have a prediction for every of the classes,
I can simply concatenate them,
and that's my final prediction, right?
So for k-way prediction in binary, just to summarize,
I can define this matrix W, one per output class,
and then learn this type of, uh, uh,
linear, uh, predictor based on a dot product.
And then, er, the last thing to discuss is,
how do we do, er, graph-level, er, prediction, right?
Here, we wanna predict using all the node embeddings in our graph, right?
And, again, let's suppose we wanna make a k-way prediction.
So what we want is we wanna have these, uh, uh,
prediction head that, uh, makes one prediction on the entire graph.
So what this means is we have to take the individual node embeddings, right,
for every node and somehow aggregate them to- to
find the embedding of the graph so that we can then make a prediction, right?
So in this sense, this, uh,
head for graph, uh,
prediction- graph-level prediction is
similar to the aggregation function in a GNN layer, right?
We need to aggregate all these embeddings of nodes to create
a graph-level embedding and then make a graph-level, uh, prediction.
So let me tell you how you can define this,
uh, graph, uh, prediction head.
There are many options,
uh, for us to do this.
[BACKGROUND] So one needs to do global mean pooling, right?
So basically you take the embeddings of all the nodes and you average them.
That would be one possibility.
Another possibility is max pooling,
where you would take- take
coordinate-wise maximum across the embeddings of all the nodes.
Um, and then another option is that you do summation-based pooling,
where you basically just sum up the embeddings of all the nodes, uh, in the graph.
Um, and this will,
depending on the application and depending on the graph- graphs you are working with,
uh, different, um, options are going to work the, uh, uh, better.
You know, mean pooling is interesting because
the number of nodes does not really play the role.
So if you- if you wanna compare graphs that have very different,
uh, sizes, then perhaps mean pooling is the best option,
but if you really wanna also understand how many nodes are
there in the graph and what is the structure of the graph,
then sum-based pooling, uh, is a better option.
Um, of course, there are also more advanced,
uh, graph, uh, pooling, uh, strategies.
And I'm just going to give you next an idea,
uh, how, uh, how you can improve this.
Um, the reason why we may wanna improve this is that the issue is that
global pooling over a large graph will use a lot of information.
Um, and I wanna illustrate what I mean by this is by- with this simple toy example,
where you can think that we have nodes that have only one dimensional embeddings, right?
So the embeddings are just a single number.
And imagine I have two graphs.
In one case, you know,
I have the values like minus,
uh, node 1 has embedding minus 1,
node 2 has embedding minus 2, node 3, 0,
you know, 4 has embedding 1,
and 5 has embedding, uh, 2.
And perhaps I have a different graph,
um, where embeddings are very different, right?
Like minus 10, minus 20,
0, 10 and 20, right?
Then I can say look clearly G_1 and G_2 have very different node embeddings.
Their structures could be very, very different.
But if I do any kind of global sum based pooling, for example,
if I sum or if I take the average,
then for both of these, um,
uh, I will get the same value.
So it means that from the graph embedding point of view,
these two no- these two graphs will have the same embedding value.
So they'll have the same representation.
So we cannot differentiate,
we cannot separate them out.
We cannot classify them into
two different classes because they have the same representation.
They both have the, uh, the representation, uh, of 0.
So this is one issue,
kind of, uh, uh,
a very simple, uh,
kind of edge case example, um,
why- why global pooling, uh,
many times can lead to unsatisfactory results,
especially if their graphs are, uh, larger.
A solution to this is to do a hierarchical pooling.
And hierarchical pooling would mean that I don't
aggregate everything together at the same time,
but I'm aggregating smaller groups and then,
you know, I take a few nodes, aggregate them.
I take another subset of nodes, aggregate them.
Now I have two aggregations.
I further aggregate these,
and this way I can hierarchically,
uh, aggregate things, uh,
subsets of nodes together.
So let me give you a- a toy example and then I'll tell you about how one can do this.
Um, so imagine I will been going to aggregate using, uh, a
rectified linear unit as a non, uh,
as a nonlinearity and a summation as the aggregation function, right?
And imagine that I decide to aggregate
hierarchically in a sense that I first aggregate first two nodes,
then I aggregate that the last three nodes,
and then I aggregate the aggregates, right?
So , uh, for, uh, graph 1,
how will this look like is,
I first aggregate minus 1 and minus 2,
um, and then pass it through ReLU,
I get a 0, then I aggregate the last three nodes.
Uh, here's the aggregation.
I get the value of 3.
Now I aggregate 0 and the 3 together,
and I obtain a 3.
So this means the embedding of this graph G_1 is 3,
because we worked with single-dimensional embeddings right?
Now for G_2, here is my,
uh, here's my graph, right?
So again, if I do the, uh,
I- if I do the first two nodes,
the ReLU will be 0.
If I do the second,
uh, the last three nodes, the,
uh, the ReLU output of this aggregation will be 30.
If I now further aggregate this using the same aggregation, uh,
function, so I aggregate zero 0 and a 30,
um, I will get, uh, a 30.
So now the two graphs have very different embeddings.
One has an embedding of 3, the other one of 30.
So we are able now to differentiate, right?
We are to distinguish them because they have different embeddings.
They- they do not overlap in the embedding space.
So that's an idea or an illustration how hierarchical pooling, uh, may help.
So now, of course the question is, uh,
how do I decide who tells me what to aggregate first and how to hierarchically aggregate?
And the insight that allows you to do this really well in graphs is that
graphs tend to have what is called community structure, right?
If you think of social networks,
there are tightly knit communities inside social networks.
So the idea would be that if I can detect these communities ahead of time,
then I can aggregate nodes inside communities into,
let's say community embeddings,
and then I can further aggregate community embeddings into
super community embeddings and so on and so forth hierarchically.
This will be one strategy,
would basically be to apply what is called a community detection or
a graph partitioning algorithm to split the graph into different, uh,
clusters, denoted here by these different, uh, colors,
and then aggregate inside each of the cluster,
each of the communities,
and then keep to- to create basically for each community a supernode.
This is now an aggregate that embedding of all the members of the community.
And then I could again look how communities link to each other,
uh, aggregate based on that,
get another supernode and keep aggregating until I get,
uh, to the prediction head, right?
And, uh, one option,
as I said, to do this,
would be to simply apply a graph partitioning, graph clustering community detection,
uh, algorithm to identify what are the clusters in the graph,
what are these densely connected groups?
And then you would, you know,
do this in the level of the original network.
Then you will do this again at level 1,
you do this at level 2,
until you can have a single supernode,
which you then can input into a prediction head.
Uh, what is interesting is that you can do this
actually in a way so that you learn how to partition the network, right?
You don't need to download some external software and make this assumption that,
you know, communities are important.
What you can do, and there is our paper linked up here called DiffPool,
because this is kind of differential pooling operator that
allows you to learn how to aggregate nodes in the network.
And the simple idea how to do this is to have two independent graph neural networks,
uh, at each, uh, level here.
And one graph neural network is going to compute node embeddings.
This is standard, what we have talked so far.
But what is clever is that we will also have
a second graph neural network that will compute the clusters that nodes belong to.
So what I mean by this is it will determine
which nodes should belong to the same, uh, cluster.
Which nodes should be aggregated, er, together,
which embeddings should be aggregated together to create this, uh, supernode.
And the cool thing is that you can train GNNs A and B at each level together in parallel.
So this means that you can supervise how to
cluster the network and how to aggregate the, uh,
the network infor- the node embedding information to-
to come up with the optimal way to embed,
uh, the underlying network.
So this is kind of the most advanced way how you can-
how you can learn to hierarchically pool, uh,
the network in order- in order to make a
good faithful embedding, uh,
of the entire network.
So, uh, this is what I wanted to,
uh, uh, show in this case.
Now that we have talked about prediction heads,
let's talk about actually the predictions,
uh, and the labels.
So the second, uh, part I wanna talk about, uh,
is this part here,
which wi- which is about,
uh, predictions and labels.
So, um, we can broadly distinguish between supervised and unsupervised, uh, learning.
Supervised learning on graphs would be where,
uh, labels come from some external sources.
Perhaps, for example, nodes have belonged to different classes.
Users in social network, uh,
you know, uh, are interested in different topics.
Uh, if you have graphs, molecules,
perhaps every molecule, you know,
we know whether it's toxic or not- not.
Or we know how is i- its drug likeness,
which is something that chemists,
uh, worry about, right?
This is supervised, basically l- supervision labels come from the outside.
And then there is also the notion of unsupervised learning on graphs where the signal,
the supervision comes from the graph itself.
An example of this would be,
for- for example, uh,
link prediction task, right?
Where we want to predict whether a pair of nodes is connected.
Here, we don't need any external information.
All we need is just, um,
pairs of nodes that are connected and pairs of nodes that are not connected.
Um, and sometimes the difference between supervised
and unsupervised is blurry because both you
can formulate as optimization tasks and in both you kind of still have supervision.
Uh, just in some cases supervision is external and
sometimes supervision is, uh, internal, right?
So, um, you know, uh,
for example, if you train a GNN, uh,
to predict node clustering coefficient,
you would kind of call this unsupervised learning on
graphs because the supervision is not external.
And sometimes unsupervised learning is also called self-supervised, right?
Basically, it's the data that say- that-
the- the input data gives you the su- the supervision to the model.
So a link prediction task is an example of a self-supervised,
uh, learning task, right?
Where basically we take the unlabeled data but still
define supervised prediction tasks based on the structure,
uh, of that data.
So, um, let me first talk about supervised,
uh, labels on graphs.
So supervising labels come from specific use cases.
Um, and let me give you a few examples, right?
For node labels, you know,
you could say, oh,
in a citation network perhaps, uh, uh,
subject area that a node,
that a paper belongs to,
that's my external label,
is defined for every node.
Uh, for example in, um, [NOISE] uh,
in, uh, link prediction for pairwise, uh, prediction tasks,
for example, in a transaction network, um,
I could have the label y to- for
every transaction to tell me whether that transaction is fraudulent or not, right?
I have some external entity that tells me, it verifies
every transaction and says which ones are fraudulent and which ones are not.
So that could be the label.
Is it fraudulent or not?
And, you know, for entire graphs, for example,
if I work with molecules, as I said,
drug likeness or, uh, uh,
toxicity would be an example of an externally defined , uh,
label that we can now predict for the entire graph,
for the entire, uh, molecule.
Um, and you know, uh, one- one advice is,
is that to reduce your task to a node,
edge, or a graph,
uh, labeling task sees these tasks are standard and easy to work with, right?
So, um, what this means is that sometimes
your machine learning modeling task will come in as a,
not as a node classification task but as a link prediction task.
But if you can formulate it as one of these three tasks,
this means- means that you can reuse a lot of existing research and you can reuse,
um, a lot of existing methodology and, uh, architecture, right?
So a heavy fo- casting
the prediction tasks in terms of these three fundamental graph level tasks,
definitely helps because it's, uh, easy,
because you can lean on a lot of, uh,
prior work and prior, uh, research.
So now that we've talked about supervised tasks,
let's talk about unsupervised signals on graphs.
Um, the- the idea here is that sometimes
you only have our graph and we don't have any external labels.
Um, and the solution here is to define self supervised learning tasks, right?
So the models will still be- be the same,
they will still be the loss
just the supervision signal will come from the input graph itself.
So what are some examples of this?
For example, for node level tasks, you could say,
let me predict statistics such as node clustering coefficient or a page name.
Perhaps what one option would also
be that if you work with the molecule- molecular graphs,
maybe you would say, let me predict what type of atom is a given node, right?
Is it a hydrogen, is it a carbon,
um, is it oxygen.
That would be one way of a self-supervised task in a sense that you are
trying to predict some attributes- some property of that node.
For link prediction for edge - edge level tasks.
Very natural way to self-supervise is to hide a couple of answers
and then say can I predict whether a pair of nodes are connected or not?
Right? Can I predict whether there should be a link or not?
And that's a level of self supervision.
And then for graph level tasks.
Uh, again, we can think of different graph statistics.
For example, we can say,
are two graphs isomorphic?
Uh, you know, what kind of motifs, graphlets, do two graphs have?
Um, and you could use these as a way, uh,
to supervise, uh, at the graph level, right?
Um, and notice that in all these tasks that I defined now,
we don't require any external ground truth labels.
we just use the graph structured information in whatever,
um, is the input, uh, data.
So now that we've talked about,
uh, uh, predictions, uh,
and labels, now let's discuss the loss function and, um,
and talk about what kind of loss functions would I
use to measure discrepancy between the prediction and the labels, uh,
so that I can then optimize this loss function and basically back propagate all
the way down to the parameters of the graph neural network, uh, uh, model.
So the setting is the following,
uh, we have n data points,
and each data point can either be an individual node
and individual graph or an individual edge.
So for node level prediction will say,
each node has a - has node i has a label.
And here's the predicted label.
For edge level again,
we'll say each edge has a-
each potential edge has a label and we're going to predict that label.
Label could be does the edge exist or not?
Maybe it's the type of the edge,
whether it's fraudulent or not, and so on.
And similarly, last, right, for, uh,
graph level, I'm denoting this as g, you know,
for each graph, you have the true label - the true value versus the predicted value.
And I'm going to use this notation y-hat and
y to refer to the prediction- to the prediction,
uh, predicted value and the true value.
And I'm- I'm going to omit the superscript- sorry
the subscript so that would basically to denote what,
uh, prediction- specific prediction task we're talking about.
Because at the end, y is just an output and I can
now work with these outputs and compare the y-hat versus y.
So, uh, an important distinction is are we
doing classification or are we doing regression?
Uh, classification means that label's y,
uh, have discrete categorical values, right?
It would be, you know, what topic does the user like?
While in regression, we are predicting continuous values.
You want to predict drug likeness of a molecule or toxicity level of a molecule, right?
Binary prediction would be or a classification would be is it toxic or not?
Regression would be predict the toxicity level.
Um, and GNNs can be applied to both of
these settings to classification as well as to regression.
Um, and the difference will be between
classification regression is essentially in the loss function and the,
uh, in the evaluation method.
So let me first tell you about classification loss.
The most popular classification loss is called cross entropy.
We have already talked about this in Lecture 6,
where basically the idea is if I'm doing a K-way prediction for i-th data point then, uh,
cross entropy between the true label and the predicted label y hat is simply a sum over,
um, uh all these K different classes, uh,
the y value of that class,
for i-th data point times the log, uh,
predicted, uh, class value and y hat,
you can interpret as a probability.
Um, and the way this basically works is to - to say the following, right?
You can imagine that my,
uh, y is, uh, like this.
So this is not a binary vector that tells that
my particular let's say node belongs to class Number 3.
So it is a one-hot label encoding.
And then, you know, the y hat would now be,
um, a vector, uh, a distribution.
Perhaps we can apply, we apply Softmax to it.
So it means that all these entries sum to 1.
And this,  now you can interpret as the probability that, uh,
y is off, uh,
class number, uh, number, uh, 3.
And the idea is if you look at this equation, right?
Because the- the- the predicted probabilities- the probabilities will sum to 1.
What we want is that,
uh, wherever there is a value of,
uh, the true class is- is not here.
We want the probability there to be very high because 0 times anything is 0.
So we want it- we want the, uh, probabilities, uh,
to be- to be low here because log something close to 0
gives me a high negative value but if I multiply it with 0, it doesn't matter.
So I would want to, uh, predict low numbers here,
but wherever the- the value is 1,
I want to predict a high probability because, um,
here I'd be multiplying with 1,
but log of something that is close to 1 is, uh, is 0.
So again, the cross entropy loss- the discrepancy in this case will be small.
So basically this- this loss will force
the predicted values to the class- to other classes that
what data point i does not belong to- to have
low values, and where whatever class it belongs to, it will force it to have high value.
And that's the idea because if this is 1,
I want the second term to be as small as possible.
The way I make it small is to make it as close to 1 as possible, right?
Remember that these entries have to sum to 1.
And this is now loss
defined at a single data point i.
So the total loss is simply a sum over all the data points and
then the cross entropy loss for each individual data point.
So these is in terms of classification loss,
uh, the most popular one.
For regression loss, what is a standard loss is called mean squared
error or- or equivalently also known as the L2 loss.
And essentially what we are doing, we are saying, uh,
if I have a K-way prediction task and trying to
predict K area and values for a given node, uh, i,
I'm simply summing over all the K and I'm taking the discrepancy between
the true value minus the predicted value
and i square that so that this will always be positive.
And basically the idea is like the- the loss will take
the smallest value when y and y hat are as aligned as possible.
So when these differences are as small as possible.
And the reason why we like to take the quadratic loss
here is because it's smooth, it's continuous.
It's easy to take derivative of.
It's always positive.
A lot of kind of nice properties.
So again, this is a loss- mean squared error loss on- on a- on a pair on one data point.
And now over if I have N training examples,
then I simply sum up the losses of individual data points.
And this is now the - the loss on the entire dataset.
So this is the in terms of classification and regression loss.
There are also other losses that,
uh, they like to use, for example,
there are these losses called maximum margin losses that are very useful
if you don't care about predicting a binary value,
or a regression, but you care about the node ordering.
Perhaps you want to sort the nodes according to- according to some value.
And the idea is that what you care is for the nodes to be sorted properly,
and you don't care so much what exact values they have.
You want to know who are the top K nodes.
In this case, you would use some kind of triplet based loss it's
called, because you want to enforce one node to be ranked higher than,
um, than the other node or you would be using,
um, some kind of max margin, uh, type loss.
So now that we talked about loss functions,
uh, let's also talk about, uh, evaluation metrics.
So for evaluation metrics, um,
we - I - for regre- how we evaluate
regression is that we generally compute what is called,
uh, the mean square - squared, uh, error.
Um, and the way this is defined is, uh, again,
it's the squared difference between the predicted value and true value.
You defi- you - you take the average, so you, uh,
divide it by the total number of data points and then you take the square root.
So this is kind of analogous to the, um, uh,
to the L2, uh, loss that you optimize.
This is now, uh,
how you report performance,
uh, of your model.
You can also do mean absolute error,
where now you just take diff- absolute differences and divide,
uh, by the number of data points.
Um, if you look into Sklearn,
so the scikit-learn Python package that all of us will be using during the class, uh,
there is a lot of different, uh,
metrics, uh, already, uh,
implemented there, and here I just give you,
uh, two most common ones.
Uh, for classification, uh, you can, uh,
what is - a very standard way to report is what is
called classification accuracy where basically you'll just say,
"What number of times did my predicted, uh,
variable, uh, predicted class match the true class?"
And you say, "What fraction of times did I cor- correctly predict the class?"
Uh, this is nice, ah,
metric if your classes are, um,
about balance, balance meaning is that I know
half of the data points is positive and half is negative.
If you have very imbalanced classes,
imagine that you only have 1%,
uh, uh, positive, uh, class, uh,
data-points and 99% are negative,
then the problem with accuracy is that it's very easy
to get high accuracy by just predicting,
a negative class all the time, right?
So if I - if I'm for example, um,
if I have the case where I say is a transaction fraudulent or not,
and let's say 99% of transactions are non-fraudulent,
then my - my - I can get very high accuracy of 99%
if my classifier always says, uh, non-fraudulent, right?
So this is the problem with the accuracy is that if classes are imbalanced,
then trivial classifiers may have very high, um, uh, accuracy.
So, uh, to get more, uh,
to go deeper and - and to deal with some of these issues, um,
there are other metrics that are more sensitive to the- this decision,
what do we, the- what do we call as positive and what do we call as negative?
Because the - the models will usually output,
let's say some probability,
a value between 0 and 1,
and we have to decide on the threshold to
say everything above this threshold is positive,
everything below the threshold, uh, is negative.
Um, and the, uh,
precision-recall type metrics, uh,
uh, are one example to do this, um,
and there is also the, uh,
ROC AUC so the area under the receiver operating characteristic that I'm also going to,
uh, discuss and talk about.
So first is if you wanna know more than just accuracy,
then what you can define is this notion of, uh,
what is called confusion matrix,
where you basically say, "A-ha,.
What is the predicted value?
Is it predict positive or negative versus what is the actual,
the true value, is it positive or negative?"
And then you can count how many examples,
how many data points are each - in,
uh, in each of these four cells, right?
When you correctly predict negative you - you do plus 1 here,
when you correctly predict uh, positive,
you do a plus 1 here because predicted and actual match,
and then you can make two, uh,
types of mistakes called false positives.
These are, uh, examples where you predicted positive,
but they are actually negative.
And you then also have false negatives where,
uh, you predicted negative,
but the class is actually positive.
So, uh, accuracy is simply true positives plus
true negatives divided by the sum, uh, of all of them.
So this is here, right, it's the number of data points.
Precision, it's called out of all - um,
it says, "How many true positives are there?"
And I normalize this by true positives and false positives.
So pre- precision says,
"Out of all the positive predictions I made,
what fraction of them are true?"
And then recall says, um,
it says again true positives,
but I divide it by true positives and false negatives, right?
So I'm basically saying,
out of all positive, uh,
examples in the data set,
how many did I actually predict positive, right?
So recall says, "Out of all positives in the data set,
how many I have predicted positive?"
And precision is, out of predicted positives,
how many are actually positive?
Um, and then you can combine precision and recall into
some - into a single metric that is called, uh,
F1 score, uh, defined as
2 times precision times recall divided by precision plus recall.
This formula is a harmonic mean of precision and a recall.
That's why, uh, uh, it is defined this way.
This is harmonic mean or harmonic average of precision and recall.
Uh, in information retrieval, in, uh,
text-mining, people like to use,
uh, F1 score, uh, a lot.
And then the last, uh,
evaluation metric that I wanna talk about is, uh, ROC AUC.
So basically a receiver operating characteristic
or a receiver operating characteristic curve.
And this measures the trade-off between
the true positive rate and what is called the false positive rate.
Um, and true positive rate is really recall,
and false positive rate is defi- divided by
false positives divided by false positives plus true, uh, negatives.
And the way you usually, uh,
write it is that you've - you - you - you
draw true posi- a false positive rate versus true positive rate.
And - and this, um,
e- evaluation matrix comes from the field of, um,
medicine where in basically use - rather than thinking of it as - what - who is,
I know who is sick or not,
or you can kind of think of it as you sort people by a risk of having a disease.
And now you can imagine, uh, asking,
aha, I will take top,
two top, three top, four,
and you ask how good is the top,
how good - how clean are the to- top k candidates?
Because this classification threshold that determines
who - what - what points are positive and what points are negative,
in some sense is, uh, arbitrary.
And uh, what is interesting,
if you, um, take the positive and negative examples and, uh,
randomly sort them, then the - then you would get this,
uh, true positive versus false positive rate,
it will be a straight line.
So a random classification will give you a straight line.
And then, um, if you do better classification than the random,
then, uh, this, uh,
this particular line is going to approach, uh,
kind of more and more,
uh, this top corner here.
So if - if it would be a perfect classification,
basically it would go up immediately and then, er, remain flat.
So them, uh, the main trick that people like to use to - to, uh,
characterize the performance of a classifier is the area under the ROC curve, right?
So a random classifi- classifier will have the area
of 0.5 because it cu- covers half of the square,
and a perfect classifier would have an area, uh, of one.
So ROC AUC is the area under this,
uh, ROC curve, this,
uh, this is called the ROC curve.
And, uh, the higher the area,
the better the classifier,
uh, and a random classifier has an area of 0.5.
So 0.5 is not good,
0.5 is bad, it's basically random.
Um, one intuition - what did the, uh,
area under the AUC, uh,
the, uh, uh, ROC curve tells me,
is it tells me the probability that if you pick two, uh,
a positive and a negative point at random,
what's the probability that a positive point will be ranked higher than a negative point?
So if it's 1,
this means you gave a perfect classification,
all the positive points are ranked above all the negatives.
And if you give a random classification,
then the probability that the positive is above a negative is - is half because,
uh, it's random, right?
So, um, this is one intuition about the AUC, uh, curve.
So, um, we have talked about the training pipeline today, um, about, uh,
how do we define the prediction head,
we talked about the evaluation metrics,
we talked about where the labels come from,
and we talked about the loss function.
What we are going to talk about, uh,
next week is we are actually going to talk about how do you set up training,
how do you do training, how do you set up the datasets,
how do you split your data between test evaluation,
um, and, um, yeah,
training datasets to be able to do, uh,
efficient, uh, training and to get, uh, good results

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 08.3 - Setting up GNN Prediction Tasks.txt
Welcome everyone, uh, back to the class.
Uh, super excited to,
uh, talk to you all today.
Um, we are going to discuss,
um, several very interesting topics.
Uh, first, we are going to finish,
uh, some of the parts, uh,
that are left unfinished from the last lecture,
and then we are going to dig into the theory of,
uh, graph neural networks.
So let's first talk about,
uh, where we ended last time, right?
So last time we talked about how do you design graph neural networks?
What kind of, um, uh,
design choices do you make and, eh,
how do you set up the entire,
let's call it training inference pipeline.
And we talked about how do you setup the input graph?
How do you define the graph neural network architecture?
How do you then use it to create node embeddings?
And then we also talked about how do you get from
node embeddings to the prediction- prediction head.
And we talked about the different prediction heads, uh,
based on node classification, uh,
link, uh, prediction as well as graph classification.
We talked about how do you then make predictions?
How do you compare those predictions with ground truth labels, uh,
to optimize the loss function and be able to do
back-propagation all the way down to the graph neural network structure?
And then we also talked about various kinds of, uh,
evaluation metrics you can use, uh,
to assess, estimate, uh,
the performance of your model.
One thing that still remains, uh,
unanswered is, uh, how do we set up the tasks?
How do we properly split into the training,
validation, uh, and test set?
And what I wanna talk, uh,
in this part of the lecture,
is address this question,
how do we split our datasets- our graph dataset into train, validation, and test sets?
So, um, we have two options,
um, when we are splitting a dataset.
We can, uh, split our dataset at once and call this a fixed split, where basically,
we want to take a dataset and split it into three disjoint independent pieces.
We wanna have a training set that we are going to use to optimize GNN model parameters.
We are going to use a validation set which we- we can use to
tune hyperparameters and various kinds of con- constants and decision choices,
uh, in terms of,
uh, the modeling architecture.
And once basically using the training set and the validation set,
we finalize our final model,
final- final values of hyper-parameters,
final values of different design dimensions.
Uh, we are then going to apply our model to
this new independent test set that was held out all the time,
and we only use it to report final, uh, performance.
And this is a fair way to evaluate because we
used training and validation to build the model,
then we fix the model and we apply it to this, uh, independent,
unseen never touched before test set and we pre- uh,
we report, uh, our performance.
Um, what is interesting in graphs, is that, uh,
sometimes we cannot guarantee that the test set will really be held out,
meaning that there will be no information leakage
from training and validation sets into the test set.
And this is why, uh, this becomes interesting.
So that's that's in terms of a fixed split.
Once we have created the fixed split,
we could actually make it such that it is a random.
And what this would mean, is that we could randomly split our data into training,
validation, and test set,
and then we could kind of report- rather than on a single split,
we could report average performance over different,
uh, uh, let's call them, uh, random splits.
So one is just to create the split and work with it forever.
The other one is to kind of create the split,
but one component of that split is that there is some randomness and we can
then try out many different instantiation of the split and report the average,
uh, to provide even more,
uh, robust, uh, results.
So, um, you know,
why is splitting graphs special?
Uh, imagine if you have an image dataset or a document dataset.
Then in such datasets,
you assume that, uh,
uh, data points are independent from, uh, each other.
So this means that, uh,
each data point here is an image, and, uh,
because they are independent from each other,
it is easy to split them into training, test,
uh, and validation, uh, set.
Um, and there is no leakage because each image is a data- data point, uh, by its own.
Uh, splitting a graph is different.
Um, the problem with the graph is that nodes are connected with each other.
So for example, in node classification,
each data point is a node now, um,
but these nodes are not independent from each other.
The nodes are actually connected, uh,
with each other, meaning that,
you know, for example, in this case,
if I look at node 5, in order to predict node 5, it will,
in terms of a graph neural network also collect information from nodes,
uh, 1 and 2.
So this means that, uh,
nodes, um, 1 and, um, uh,
2, um, will affect,
uh, the prediction on- on- of node 5.
So if 1 and 2 are the training dataset and 5 is the test dataset,
then clearly we have some,
um, information, uh, leakage.
So, uh, this is why this is interesting.
So then the question is,
um, what are our options?
What can we- uh, what can we do?
Um, we can do the following.
Um, the first solution is to do what we call,
um, a transductive setting,
where the input graph can be observed over, for all the dataset splits.
So basically we- we will work with the same graph structure for training,
validation, and, uh, test set.
And we will only split the node- the node labels,
meaning we'll keep the graph structure as is,
but we're going to put some nodes into the training,
test and, uh, validation, uh, set.
So we are only splitting, uh, node labels.
This means that at the training time,
we compute embeddings of nodes,
let's say 1 and 2 because they are our training set, uh,
using the entire graph,
but only labels of 1 and 2.
And at validation time we compute embeddings of the, uh,
entire graph and only evaluate on, uh, uh, uh,
labels of nodes 3 and 4 because we have the data, uh, for them.
So this would be one possible,
uh, approach, uh, to do this.
Another approach is what we call,
uh, an inductive setting, where we, uh,
break the edges between the splits into, uh,
multiple graphs or multiple independent graphs, multiple independent components.
So now we have, uh,
three graphs that are independent.
Um, and in our case that would mean is that we would just drop these , uh, dotted edges.
So now we have three different graphs and we can call one a training graph,
a validation graph, um, and a test graph.
Um, so this means that when now we are making,
let's say a prediction, uh,
in the test set about node five,
we are not affected, uh,
by the, uh, prediction of- uh,
by the label or the structure from- information from node 1 anymore.
So this means that at the training time we compute embeddings
only over the graph that includes nodes 1 and 2,
um, and only, uh,
in using the labels of 1 and 2.
And at validation time,
we compute the embedding using the graph, uh,
over nodes 3 and 4, uh,
and evaluate based on the nodes, uh, 3 and 4.
Of course, the problem with this- with this approach,
is that now I have- I have thrown away quite a few edges,
quite a bit of graph information,
and if my graphs are small,
this is not, uh, preferred.
So kind of the trade-off is either I have some leakage of structured information between,
uh, training, validation and test set,
but at least the labels are independent in
the transductive setting or in an inductive setting,
I actually have to throw away the edges so that I chop
the graph in two different independent pieces and then,
uh, run or evaluate over those pieces.
So in- the solution in that transactive setting is that the in- input graph, uh,
can be, uh, observed, uh,
for all, uh, dataset splits,
training, validation, and test.
Um, and this is- this is interesting because, um,
it allows us to basically operate in this what you could also think of
as semi-supervised setting where the graph is given, uh,
the- the edges of all the nodes are given,
uh, the features of all the nodes are given,
but the lab- but the labels of nodes are only,
uh, only a subset of node labels, uh, is observed.
So, uh, to summarize,
in the transactive setting,
we have training, test,
and validation split, uh,
all on the same graph.
Where data consists of one connected graph.
The entire graph can be observed,
uh, uh, in all the data splits.
So it means all the nodes,
all the edges and all the node features,
uh, but we only split the,
uh, the labels, meaning, uh,
some labels are observed,
other labels- labels are unobserved.
And this, uh, setting is applicable both to node prediction,
uh, as well as edge class- edge prediction tasks.
The inductive setting, which is the training,
validation, and test sets,
are on different graphs.
Uh, the dataset here consists of multiple graphs that are independent from each other.
We only observe the graph structure- note features as well as,
ah, let say node labels,
uh, within the split.
Um, and this allows us to really test how can we generalize to  unseen graphs.
The- the drawback of this approach is that we
have to take the original graph and chop it into many,
uh, different, uh, small, uh, pieces.
Uh, and this way we throw away some, uh, graph formation.
This was now in terms of node classification.
Here I give you an example in terms of transductive classification,
you simply split, uh, nodes into training,
uh, validation, and test sets.
Um, in the inductive setting, uh,
we basically- we can assume we have multiple different, uh, graphs, um,
uh, given and we can, um, uh,
take some graphs into the training set,
others in the validation and test set.
If we don't have multiple graphs,
we have to create multiple graphs by basically dropping the edges
or cutting the edges between them so that we get to these different,
uh, connected components and then put them in the training, test,
uh, and validation, uh, set.
So now that we have talked about node classification,
let's switch and let's go to the next, uh,
classification task, which is graph classification.
In graph classification, uh,
induc- inductive setting is well defined because we have independent graphs,
um, and we can simply split them into the training, validation, and test sets.
So basically, we put some graphs into the training set,
some in the validation set,
and some in the test set.
Se here, we basically have independent, uh, graphs.
It's easy to split them,
there is no crosstalk,
there is no information leakage,
um, and this can be done.
Perhaps, um, the trickiest of all the settings,
uh, for machine learning with graphs is link prediction.
Um, and the goal of link prediction is to predict missing edges.
And setting up link prediction requires a bit of thought and it
can be tricky because link prediction is an unsupervised,
uh, or self-supervised task.
We need to create labels and the data splits,
uh, on our own.
So this means that we need to hide some of the edges from
the GNN and let the GNN predict the existing edges.
So the idea is, for example,
if I have the original graph,
I have to hide a couple of edges,
for example, these two red edges,
so that I say this is the input and I want my- my GNN to be able to predict these two,
uh, output, uh, edges, right?
So in some sense, we'll have to take some edges out,
hide them from ourselves and try to, uh, predict them.
So, uh, this is interesting and it creates a bit more complexity because,
um, in a GNN,
we have two types of edges.
We will have the message-passing edges,
so edges that the GNN can use to create the embedding,
and then we'll have these what we'll call supervision edges,
which are the edges that are part of,
let's say, our training, uh, dataset.
So for, uh, link prediction,
we need to split edges twice.
In first step, we assign two types of edges to the original graph.
As I said, some edges we'll call message passing edges.
They are edges used for the GNN to operate over.
And then we'll also call- what we'll call supervision edges.
These are the edges for us to compute the objective function,
loss function, performance of the model, and so on.
Um, and after the step 1, um,
only message edges will remain in the graph and supervision edges are the,
uh, edges that are used for supervision of, uh, edge prediction,
uh, made by the model and will not be used, uh, by the GNN.
Um, so this- this is the first step.
And then in the second step, we split, uh,
edges into train, test, and validation set.
Um, and we have two options.
One is to do what is called inductive link prediction split, where, you know,
we- let's say we have a dataset with three graphs and each split will be,
uh, indepe- will be an independent graph.
So we'll have a training set,
we'll have our validation set and a test set, three different graphs,
and each graph will have a different, um, uh,
split in terms of training,
validation, and test edges, right?
So here this would be the message-passing edges and the supervision edges.
In the second validation graph,
y- you know, we'll have different, um, uh,
supervision edges and different message-passing edges,
and then, in the test set, uh, the same.
So this is, uh,
one way, um, uh, to do this.
Another way to do this is to do this in,
uh, the transductive setting.
And if we do transductive link prediction split, um,
this is generally the default when people talk about link prediction.
And suppose we only have one- one input graph.
What we have to do in this, uh, one,
um, input graph, by definition,
the transductive- of the- uh,
what we mean by transductive learning,
the entire graph can be observed,
uh, for all the datasets' splits.
But since edges are both, uh,
part of the graph structure and the supervision,
we need to hold out, uh,
validation and test edges.
Um, and then to train the training set,
we need to further hold out, uh,
the supervision edges, uh, on the training set.
So let me now, uh,
show you how exactly to do this.
So, uh, for transductive link predict- um, link prediction,
we will take the original graph and we are going to create
a training dataset where we'll have a couple of training edges,
um, er, for message-passing as well as the supervision edges.
At validation time, we are going to use the training edges and the, uh,
training message-passing edges and training supervision edges,
uh, to be able to predict the validation edges.
And then at the test time,
we are going to use the training message edges,
training supervision edges, as well as validation edges to predict the test edges.
So now you can see how, uh,
at training time the graph is sparser,
at the validation time,
we need- we get to see the supervision edges,
but we have to predict the validation edges,
and at the final test time,
we get to see all the edges but the test edges,
and we need to predict,
uh, the test edges.
So here you see how, uh, basically, these,
uh, sets are nested in one another.
So, um, why do we use a growing number of edges?
Because one way to think of this if- is if the graph is evolving over time,
then you could say at some early time,
this was the graph, then it grew by adding this edge.
So that was the graph and it added another edge,
and so on and so forth.
So you can think of this almost like as splitting it along, uh, uh,
uh, three- into thee- three different,
um, time, uh, intervals.
And that's perhaps the best way to define,
uh, link prediction, uh, on a graph.
So to summarize, transductive link prediction split is a bit, um, um,
uh, tricky because we need to take
the original graph and s- and basically have four types of edges.
Training message-passing edges, training supervision edges,
validation edges, as well as,
uh, test edges.
Um, and we have nice,
uh, even though this, you know,
might be a bit cumbersome, uh, to split, uh,
around, uh, this allows us, uh,
to do- uh, to do the- to do things well because we have, uh, good tools,
meaning DeepSNAP and GraphGym that allow us,
uh, to do this, uh, for free, uh, for us.
So, uh, to summarize, uh,
we- we talked about the entire GNN training pipeline from evaluation metrics,
loss functions, labels, as well as, uh, the predictions.
And these are some of the tools, uh,
you can use that allow you to kind of manage these in an end-to-end, uh, fashion.
So to summarize, uh,
we talked about, um, uh,
the GNN layer and how do we define it,
we talked about how to stack different GNN layers together,
and then we also talked about graph augmentation in
terms of feature augmentation and graph structure augmentation,
and what I talked, uh,
now was about learning, uh,
objectives and, uh, how do we,
uh, set up different, uh, tasks.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 09.1 - How Expressive are Graph Neural Networks.txt
So now we are shifting gears and we are talking about theory of graph neural networks.
And we are in particular going to ask ourselves how expressive are graph neural networks,
what are graph neural networks able to learn,
and what are they not uh, able to learn?
This is really the question for uh,
this part of the lecture.
So we talk about deep uh,
neural networks applied to graphs,
where through several um,
layers of non-linear propagation,
we are trying to come up with the embeddings of nodes, embeddings of networks.
So we can do various kinds of machine learning prediction tasks.
The key idea that graph neural networks have is this idea of aggregating
local neighborhoods around a given node
of interest to generate the embedding for that node.
Right, So this is kind of the classical picture we've been showing
several times in our discussions so far.
Right, so the intuition is that nodes
aggregate information from their neighbors using neural networks.
And so far we discussed what kind of design choices you have when deciding how to,
how to operationalize or how to design
this message transformation and aggregation operations.
So today, right now we are going to talk about the theory of graph neural networks.
And in particular, we are going to ask how powerful are graph neural networks.
How expressive are they?
What can they learn and what can they not learn?
This is especially important because there are many different and GNN models.
Right, we talked about the GCN, graph convolution neural network.
We talked about that, the graph attention network.
We talked about the GraphSAGE network and we talked about
the entire design space of these types of models.
So the question is, what is their expressive power?
Which basically means what is their ability to distinguish different nodes,
different graph structures, and how
expressive are they in learning different types of patterns?
And then what would be super cool today is that we will be actually able
to design the maximally expressive GNN models.
So in some sense, we'll be able to design
the most powerful graph neural network there is which is super cool.
So that's the- that's the plan.
So background is, we have many graph neural network models,
they all have different,
they all differ in terms of how they propagate,
aggregate, and transform messages.
And the question is,
can we understand what is their expressive power and how these different design choices
actually lead to different type of models.
So for example, in a graph,
convolutional neural networks, GCN.
It's using what is called mean pooling.
Right, so basically when,
when we aggregate information from neighbors,
we use  element-wise mean pooling.
And then we use a linear transformation plus a ReLU, nonlinearity.
That's for example, what a GCN is.
For example, GraphSAGE uses a multi-layer perceptron plus element-wise,
let say maximum pooling which is- which is different.
And the question is, what is better?
Is max better than, than average?
Or what's the difference between the two in terms of,
let's say, theoretical properties and expressive power?
Um, there is an important note I wanna- I wanna make so that we don't get confused later.
In graph neural networks,
we have two aspects.
We have the aspect of node features,
node properties, and we have the aspect of a graph structure.
And for the purpose of this lecture,
I'm going to use colors of nodes to represent their feature vectors.
What I mean by this, if two nodes are of the same color,
then they have the same feature vector,
they have the same feature representation.
Right? So for example,
in this graph here,
numbers represent node IDs.
So I can say I'm talking about node 1,
I'm talking about node 2,
but the features these nodes have are all the same.
So there is no featured information that would
allow me to distinguish nodes from one another, right?
So we don't, the color means what is the feature vector of the node.
So now for example, the question would be,
how well can a GNN distinguish different graphical structures, right?
Because if every node has
its own unique feature vector then it's easy to distinguish the nodes.
You just look at their feature vectors.
But if all the feature vectors are the same, like in this case,
all nodes are yellow,
then the question is, can you still distinguish the nodes?
Can you learn that node 5 is different than node 4,
for example, in this case.
So and in graph neural networks,
we are particularly interested in this notion of local neighborhood structures.
Where basically we are interested in quantifying
the local network neighborhood around each node in the graph.
So for example here,
let's say I'm interested in nodes 1 and 5 and I
say could I learn to distinguish nodes 1 and 5?
Distinguishing them would be quite easy
because they have different neighborhood structures,
even if you look at the number of edges that is adjacent to each of them,
you know, node 1 has degree 2 and node 5 has degree 3.
So it'll be very easy to distinguish them.
If you can capture the degree of the node in a graph neural network,
then you can differentiate between nodes 1 and 5.
Let's look now at the second example.
How about nodes 1 and 4?
Are they distinguishable?
Right? If you look at it from the single layer,
single hop neighborhood, then node 1 has degree 2.
And node 4 has degree 2.
So if I only am able to capture the degree of the node itself,
I cannot differentiate between 1 and 2, right?
They have the same feature vector and they have the same degree.
However, 1 and 2 are still different because if I look at,
let's say the second-degree neighborhood.
Right? You could say, ah-ha,  node 1 has two neighbors.
One has degree 2 and 1 has degree 3.
While node 4 also has two neighbors,
but one has degree 1 and the other one has degree 3.
So if I'm able to capture the degree of the node
4 of the node itself plus the degrees of the neighbors,
then 1 and 4 are distinguishable because their neighbors have different degrees.
Right? So now you see how maybe immediately two- two nodes look the same.
But if you go deeper into the network here,
I go to the neighbors,
then the two nodes become distinguishable.
And that is very interesting.
So now let's continue this investigation and look at another pair of nodes.
Let's look at nodes 1 and 2.
What is interesting is that 1 and 2- actually, um,
in this graph neural,
in this network are indistinguishable from one another.
Because they are kind of symmetric in the graph, right?
They both have degree 2, um, uh,
their neighbor, um, they both have two neighbors.
Uh, one of degree- of degree 2 and one of degree 3.
Um, if you go to the second hop neighborhood,
it's, uh, the node number 4,
uh, that has degree 2.
So basically their- their, uh,
network neighborhood is identical
regardless how deep or how far do we explore the network.
Because in- in both cases,
you know, they have,
each- each of them has one node of degree 2,
one node of degree 3.
At two-hop neighborhood, um,
they both have one neighbor of degree 2.
Three hops away they both have one neighbor of degree 1.
So you cannot distinguish one and two unless somebody gives you
some feature information that would allow you to s- tell 1 from 2.
But based on the graph structure,
you cannot distinguish them because they're kind of symmetric.
Their- their positions are isomorphic,
uh, in the graph, right?
So that's an exa- an- an example,
kind of trying to build up intuition how this,
uh, will all, uh, work out.
So the key question we wanna,
uh, look at is,
can a GNN node embedding distinguish different local neighborhood structures, right?
Local meaning neighborhoods structures around a given node.
And if it can, the question is when and if not,
what are the failure cases of graph neural networks?
So- so what we'll do next is we need to understand
how a GNN captures local neighborhood structures.
And we are going to understand this through this key concept of a computational graph.
So let me now talk about, uh,
what is a computational graph, right?
The way you think of this is that each, uh,
layer, uh, a GNN aggregates,
uh, neighborho- neighboring embeddings.
So in a GNN, uh,
we generate an embedding through
a computational graph defined on the node neighborhood structure.
So for example, if I say here is node 1,
the computational graph- let's say if I do
a two-layer GNN for node 1 is created here, right?
Node 1 aggregates information from nodes 2 and 5. Here they are.
Node 5- node 5 aggregates information from its neighbors,
5 has neighbors 1, 2, and 4.
And node 2 here,
aggregates information from its neighbors,
node 1 and node 5.
So this is what we call a computation graph.
It simply shows us how the messages gets- get aggregated from level,
uh, level 0 to level 1 to level 2.
And this is now the computation graph that describes
the two-layer graph neural network for,
uh, node, uh, node ID, uh, 1.
That's the idea here.
And what is interesting is that now if I take, for example, uh,
node I- node, uh,
number 2 and I create a computation graph for itself,
uh, here it is, right?
Two aggregates from nodes 1 and 5, uh,
5 again aggregates from,
uh, uh, 1, 2 and 4.
And, uh, node number 1 aggregates from 2 and 5, right?
What you notice is that computational graphs for nodes 1 and 2 are actually identical.
They both have, uh,
two children at- uh,
at level 1 and they have,
you know, one- one has 2,
and one has fi- uh,
one has 3, uh,
at level, uh, 0.
So what this means is,
because a GNN is only doing message-passing information,
uh, without any node IDs,
it only uses node feature vectors.
This means that, you know,
if you look at these propagation,
uh, trees, these computation graphs,
right now they are different because you say, oh,
obviously here is node number 1 and here is node number 2.
So obviously these trees are different.
But if you only look at the colors- if you only look at the node feature information,
then this is how these trees look like.
They look identical and there is no way to tell nodes apart from each other.
So in all cases,
all the graph neural network can do,
can aggregate, you know,
the information from these nodes.
They all have yellow color and here it can aggregate yellow color.
So all it can do it is to say,
oh, I have three yellow children.
This guy can say I have two yellow children.
And then this- here we can say, uh-huh you know,
I have two children an- and one of them has two and
the other one has three, uh, further children.
And that's how we can describe this computation graph.
But the point is that for two different nodes,
1 and 2, the computation graphs are the same.
So without any feature information,
without any node attribute information, uh,
these two- these two nodes,
these two computation graphs are the same.
So these two nodes will be embedded into the same point in the embedding space.
And what this means is that they will overlap so the graph neural network won't be able,
uh, to distinguish them,
uh, and won't be able to classify node 1 into a different class than node 2,
because their embeddings will be exactly the same.
They will overlap because the- the computation graphs are the same,
and there is no distinguishable, uh,
node feature information because that's kind of our assumption, uh, going in.
So if there is an important slide of this lecture,
this is the most important slide,
is that basically we- GNNs capture
the local neighborhood structure through the computation graph.
And if computation graph of two nodes are the same,
then the two nodes will be embedded exactly into the same point in the embedding space,
which means that we are not able to classify one into one class,
and the other one into the other because they are- they are identical,
they are overlapping, so we cannot distinguish, uh, between them.
So this means just kind of to summarize,
is that in this simple example,
a GNN will generate the same embedding for nodes 1 and 2 because of two facts.
First is that because the computational graphs are the same, they are identical.
And the second important part is that
node feature information in this case is identical, right?
All- all nodes the assumption of this lecture
is that node features are not useful in this case,
so all nodes have the same feature.
They are all yellow, right?
And because GNN does not care about node IDs,
it cares about the attributes,
features of the nodes and aggregates them.
This means that this GNN is not able to distinguish nodes 1 and 2.
So 1 and 2 will always have
exactly the same embedding so they will always be put into the same class,
or they will be assigned,
uh, the same label.
Which, uh, which is interesting and, uh,
which now seems, uh,
quite- uh, quite daunting,
a bit disappointing, right?
That we so quickly found a corner case,
or a- or a failure case for graph neural networks
where they basically cannot, uh, distinguish nodes.
So the important point that I wanted to make here is that in general,
different local neighborhoods define different computation graphs, right?
So, uh, here are computation graphs,
uh, for different nodes.
These are computation graphs for nodes 1 and 2, uh,
computation graphs for nodes 3 and 4,
as well as computation graph,
uh, for node 5.
So now we already know that we won't be able to
distinguish 1 and 2 because they have the same computation graphs.
That's- that's some- that's fact of life.
There's not much we can do.
But the question still remains,
how about 3 and 4?
Or 3 and 5?
Will our graph neural network be able to distinguish these nodes,
because obviously they have different computational graphs.
So perhaps the graph neural network is able to remember,
or capture the structure of the computation graph,
which means that nodes 3 and 4 will get a different embedding,
because their computation graphs are different, right?
That's the- in some sense,
the big question, right?
So basically what I'm- what is the point is?
The point is that computational graphs are identical to
the rooted subtree structures around each node, right?
So we can think of this rooted subtree as it defines
the topological structure of the neighborhood around, uh,
each node and two nodes will be able to distinguish
them in the best case if they have different rooted subtree structures,
if they have different computation graphs.
Of course, maybe our graph neural network is so imperfect that is
not even able to distinguish nodes that have different computation graphs,
meaning that the structure of these rooted trees is different.
And what we are going to look at next is under what cases,
you know, can 2 and 3 be distinguished,
and in what cases 2 and 3 will simply be lumped
together into the same, uh, embedding. So-
Kind of to continue on this, right?
GNN's node embeddings capture rooted subtree structures.
They basically cap- they wanna capture the structure of the graphing- of the,
uh, computational graph of the network neighborhood around a given node.
And the most possible expressive graph neural network will map different, uh,
rooted subtrees into different node embeddings,
uh, here, for example,
represented by different, uh, colors, right?
So one and two,
because they have exactly identical computation graphs and exactly identical features,
will be mapped to the same point.
There is nothing we can do about
that with the current definition of graph neural networks.
Um, but for example,
nodes 3, 4 and 5,
they don't have identical computation graph structures,
so they should be mapped into different,
uh, points in the embedding space, right?
So the most expressive graph neural network will basically be
able to learn or capture what is the structure of the computation graph,
and based on the structure of the computation graph assign
a different embedding for each computation graph.
Um, that's the main, uh,
uh, premise, uh, that,
uh, we are making here.
So we wanna ensure that if two nodes have different computation graphs,
then they are mapped to different points in the embedding space.
And the question is,
can graph neural networks, uh, do that?
There is an important concept for mathematics that will allow us, uh,
to make further progress in understanding whether
a graph neural network can take two different computation graphs,
two different, um, rooted subtrees
and map them into different points in the embedding space.
And that is this notion, uh,
or definition of what an injective function is.
And a function that maps from the- the- from the domain X,
uh, to domain Y is called injective.
If it maps different elements into the different outputs.
So what this basically means that f retains the information of the input, right?
It means that whatever- whatever inputs you get,
you'll always map them into distinct,
um, distinct points, sort of distinct outputs.
Meaning, for example, it's not that 2 and 3 would
collide and you would give the same output A.
So every input maps to a different output.
That's a definition of an injective function.
And we will- this is a very important concept because we will use it,
uh, for the lo- rest- rest of the lecture, uh, quite heavily.
So we wanna know how expressive is a graph neural network.
And most expressive graph neural network should map these subtrees,
these, uh, computation graphs to node embeddings injectively,
meaning that for every different subtree,
we should map it into a different point in the embedding space,
um, and if this mapping is not injective,
meaning that two different inputs,
two different subtrees get mapped to the same point,
then, um, this is not an
injective mapping and that is, uh, the issue.
So we wanna have and show that graphed- what we wanna develop
a graph neural network that has this injective mapping where
different subtrees get mapped into different points,
uh, in the embedding space.
So the key observation, uh,
that will allow us to make progress is that trees of
the same depth can be recursively characterized,
uh, from the leaf nodes,
uh, to the root nodes.
So what I mean by this is if we are able to distinguish one level of the tree,
then we can recursively,
uh, take these, uh,
levels and aggregate them together into a unique description of the tree.
So what I mean by this is, for example, um,
the way you can characterize the tree is simply by the number of
children each node has, all right?
So for example, here you could say, aha,
at the lower level one node has three neighbors,
three children, and the other node has two children.
Um, and then you can say a-ha and then the node, uh, uh,
the- the- the- the root has,
uh, has, uh, um, has,
uh, two children as well.
So I can characterize this by saying, aha, um,
you know, uh, we have,
uh, two neighbors at, uh,
level 0, we have three neighbors at level 0, and, uh,
we have, uh, two neighbors,
uh, at level, uh, at level 1.
While for example, for this particular computation graph,
I have one child here, uh,
three- three children here,
and then, again, two children here.
So this description is different than their description.
So it means I'm able to separate out, um,
or to distinguish between these two- these two different, uh, uh trees.
Uh, the important thing is that trees can be decomposed level by level,
so if I'm able to capture the structure of a single level of the tree,
perhaps even just this level,
then I can recursively do this,
uh, level by level.
So what I mean is, um,
we only need to focus on how do we characterize one level of this, uh,
computation graph or this, uh,
rooted subtree around a given node, uh, of interest.
So, um, let's continue thinking and setting up the problem.
So if each step of GNN, uh,
aggregation process can fully retain the neighborhood information,
meaning how many children,
uh, neighbors does a given node have?
Then, uh, the generated node embeddings can distinguish different,
uh, subtree structures, right?
If I can say, um,
at level, uh, 1, uh,
in- in one tree I have two- two children in the other one I have three.
Um, and if I can kind of capture
this information and propagate it all the way up to the node 1.
And in this other tree I can kind of capture the information that one,
uh, one node has one child and the other node has, uh, three children.
And again, I'm able to retain this information, uh,
all the way to the top layer,
then obviously the- the number of children, um, is different.
So these two, uh, these two trees, uh,
we are able to, uh, distinguish them, right?
So the point is, in some sense,
are we able to aggregate information from the children and somehow
store it so that when we pass it on to our parent in the tree,
this information, uh, gets retained?
As in this case, the information that
two and three got retained all the way up to the root of the tree.
That's, uh, the question, uh, we wanna answer.
So in other words,
what we wanna do is we wanna say that
the most expressive graph neural network would use
an injective neighborhood aggregation for each step,
for each layer of the or for each level of the computation graph.
So this means that it will map
different neighborhoods into different, um, embeddings, right?
So we want to be able to capture the number of children at level 1,
oh sorry, at level 0, at level 1,
and then aggregate this- kind of retain this information as we are pushing it
up the tree so that the tree knows how many children each of its,
um, each of its, uh,
uh, inner nodes, uh, have.
So that's essentially the idea.
So the summary so far is the following.
To generate a node embedding,
GNN uses a computational graph that
corresponds to a rooted subtree structure around each node.
So if I have a node,
I have this notion of a computational graph that is simply a-
a rooted subtree structure that describes
the local neighborhood structure around this node.
And then different rooted subtrees,
different computation graphs will be distinguishable
if we are using injective neighborhood aggregation,
meaning we are able to distinguish different subtrees.
And GNNs can- as we are going to see, um,
GNNs can fully distinguish different subtree structures
if at every level its neighborhood aggregation,
meaning it's aggregation for the children, is injective,
which means that no information, uh, gets lost.
So then we can fully characterize, uh,
the computation graph and distinguish one computation graph, uh, from the other.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 09.2 - Designing the Most Powerful GNNs.txt
So given the insights so far,
let's now go and design the most powerful graph neural network.
So let's go and design the most expressive,
uh, graph neural network.
And let's develop the theory that will allow us, uh, to do that.
So the key observation so far is that the expressive power of
a graph neural network can be characterized by
the expressive power of the neighborhood aggregation function they use.
Because the more expressive
the neighborhood aggregation leads to a more expressive graph neural network.
And we saw that if the neighborhood aggregation is injective,
this leads to the most expressive, uh, GNN.
A neighborhood aggregation being injective,
it means that whatever is the number and the features of the- of the children,
you- you map every different combination
into a different output, so no information get- gets lost.
So let's do the following next,
let's theoretically analyze the expressive power of different, uh, aggregation functions.
So the way you think of neighborhood aggregation, uh,
basically taking the information from the children and aggregating is that
neighborhood aggregation can be abstracted as a function over a multi-set.
A multi-set is simply a set with repeated elements, right?
So if you say, "I'm a node here and I aggregated
from two neighbors for two children," this is the same as saying,
I have a set of children,
uh, two yellow guys,
and I need to aggregate information from them, right?
And of course, in a multi-set,
um, nodes can have different colors,
node can have different features, so, you know,
could say, uh-huh, I have,
uh, I have two children,
one with yellow attribute and the other one with
blue attribute versus some other node has, um,
three children, two of them with yellow attribute or yellow feature,
and one, uh, with the blue feature.
And then we aggregate this information,
we want the- the new message,
the aggregated information not to be lost.
Somehow we wanna the,
er, to- in this aggregation,
in this compression step,
basically to retain all the information we know about the children, right?
So here we'd wanna say, two yellow and a blue,
and here we'd wanna say one yellow and one blue so that
these two sets- multi-sets still remain, uh,
distinguishable as- as we are aggregating them, uh, to their parent,
so that then we aggregate this parent further to the super parent,
uh, no information, uh, gets lost.
So let's look at, uh,
the neighborhood information- aggregation functions used by the two,
uh, models that we have discussed so far in the class.
First, we'll talk about, uh,
GCN, which uses mean pooling.
It uses element-wise mean pooling over neighborhood node features,
and then let's talk about the max pooling variant of GraphSAGE that uses
element-wise maximum pooling over neighboring, uh, node features.
And let's see what is the expressive power of mean
and what is the expressive power of max, uh, pooling.
So, uh, let's first talk about, uh, GCN,
so the mean pooling when you
average the messages coming from the ne- from the children.
Uh, if we take the, er, element-wise mean,
then in a GCN,
it's followed by a linear function,
and a ReLU, a- activation function.
Um, and, er, what is,
uh, what is the observation?
The observation function is that GCN's aggregation function cannot
disting- distinguish multi- different multi-sets with the same,
kind of, proportion of colors, right?
So for example, um,
this is a failure case.
When you average together messages,
it doesn't- it doesn't matter whether you average
one yellow and one blue message or whether you average two yellow and two blue messages.
At the end, the average is the same, all right?
And this is the failure case of the average.
It- it will combine these two multi-sets,
it will aggregate them into the same,
uh, into the same message.
So it means it will lose information in the case that here are,
um, one and one,
and here is two and two,
because the ratio is the same.
So let me be a bit more precise and give you a proper example.
Let's for simplicity, assume that node colors are represented as one-hot encodings, right?
So now every node- every node, uh,
has a feature vector that simply encodes what color is the color of the node, right?
That is its, uh, feature vector.
And this is just, kind of,
uh, a- a way to illustrate, uh,
this concept and what happens when we do, uh, aggregation, right?
So for example, when you do,
uh, average of, uh, two vectors,
uh, 1,0, and 0,1,
you- you- you get, uh, uh, half and half.
So that's your aggregated message now of these two, uh, feature vectors.
Uh, in this case,
when you have a multi-set, again,
of two yellow and two blue,
here are the corresponding feature representations.
If I take the e- the- the element-wise average of these,
uh, four vectors, I also get half half.
So it means that even if I then apply some non-linear transformation,
and activation, and so on, at the end,
I will get the same output because the aggregation of, uh,
yellow and blue is the same as the aggregation of two yellows and two blues.
Even though I encode them in- with different feature vectors,
so yellow and blue nodes are de- definitely distinguishable because, you know,
one has the first element set to 1,
and the other one has the second,
uh, element, uh, set to 1.
So you see how mean pooling can basically aggregate, uh,
multi-sets that have the same proportion
of nodes of one type of feature versus the other type of feature,
um, into the same representation,
regardless of what is the total number of nodes or what is
the total size of the underlying, uh, multi-set.
So this is the issue with the mean pooling.
This is a failure case of mean pooling
because it won't be able to distinguish a multi-set
of size 2 versus size 4 if the proportion of features is the same in both.
And now, let's look at, uh, um, um, uh,
GraphSAGE, uh, max-pooling, uh, variant of it.
So we- in the GraphSAGE,
we apply our multi-layer perceptron transformation,
and then take, uh, uh, element-wise maximum pooling.
Um, and what we learn here is that maximum pooling function cannot
distinguish different multi-sets with the same set of distinct colors, right?
So what does- what does this mean,
is that all of these different multi-sets will
be aggregated into the same representation.
Why is that the case is because as long as
multi-sets have the same set of distinct colors,
then the me- whatever is the maximum, right?
Maximum will be one of the colors,
that maximum is the same regardless of how many different nodes,
uh, and what are the proportions of colors in the,
um, uh, in the multi-set.
So to give you an example,
imagine I have these three different multi-sets,
I, uh, I encode this colors using some encoding,
then I apply some nonlinear transformation
like an MLP to it because this is what GraphSAGE does,
and let's assume without loss of generality,
that basically now, you know,
these colors get transformed to some new colors,
and we encode these colors with one-hot encoding.
So I- so everything is disti- distinguishable at to this level.
But the problem is that now if you take, uh,
element-wise, meaning coordinate-wise, maximum,
in all these different cases,
you get the same aggregation,
you get the same maximum value,
you get 1 and 1.
So this means that regardless whether the node has,
uh, two children, four children,
or three children, um,
and whatever is the ratio between blue, and, uh, uh,
yellow, in all cases,
the maximum pooling will give me the same representation.
So it means that all this information here gets lost and
all these different multi-sets get mapped to the same representation.
So clearly, uh, maximum pooling is not
an injective operator because it maps different inputs into the same,
uh, output, and that's the problem.
You get these collisions and information, uh, gets lost,
and that decreases the expressive power,
uh, of the graph neural network.
So, let's summarize what we have learned so far.
We have analyzed the expressive power of graph neural networks,
and the main takeaways are the following;
the expressive power of a graph neural network can be characterized by
the expressive power of its neighborhood aggregation function, right?
So the message aggregation function.
Neighborhood aggregation is a function over multi-sets,
basically sets with repeating elements.
Um, and GCN and GraphSAGE aggregation functions
fail to distinguish some of the basic multi-sets.
Meaning these two aggregation functions,
mean and maximum, are not injective,
which means different inputs get mapped into the same output,
and this way, the information gets lost.
Therefore, GCN and GraphSAGE are not maximally powerful graph neural networks.
They're not maximally expressive,
uh, graph neural networks.
So let's now move on and say,
can we design the most expressive graph neural network, right?
So our goal will be to design maximally powerful graph neural network,
uh, among all possible message-passing graph neural networks.
And this will- the way we are going to do this is to-
to design an injective neighborhood aggregation function.
So basically a neighborhood aggregation function that will never lose information
when it aggregates from the children to create a message, uh, for the parent.
So the property will be an injectivity of the aggregation function, right?
Um, so the goal is design a neural network that can model this injective,
uh, multi-set function because that's the aggregation operator.
So, uh, here is a very,
uh, useful, uh, theorem.
The theorem says that any injective multi-set function
can be expressed in the following way.
So it can be- so if I have a set of elements, right,
I have my multi-set function S, uh,
multi-set, uh, that has a set of elements,
then the way I can write an injective function over a multi-set is I can write it as,
uh, I apply my function f to every element of the multi-set,
I sum up these,
uh, uh, these, uh,
outputs of f and then I apply another non-linear function, okay?
So the point is that if I want to have an injective set over,
uh, over a multi-set,
then I can realize this injective, uh,
function by having two functions, f and Phi,
where f I apply to every element of the multi-set,
I sum up the outputs of f,
and then I apply another function,
another transformation, uh, Phi to it.
And this means that,
uh, this is how,
um, a gr- um,
a multi-set function can be expressed,
uh, and it will still be, uh, injective.
So the way you can think of the proof,
what is the intuition?
The intuition is that our f can, uh,
produce kind of one-hot encodings of colors, right?
So f is injective for different node,
it produces a different output, um,
and these outputs need to be different enough so that when you sum them up,
you don't lose any information.
So in some sense, if- if f takes colors and produces their one-hot encodings,
this means that then you can basically by summing up,
you are counting how many elements of each color you have.
And this way, you don't lose any information, right?
You say, uh huh, I have one yellow node and I have, uh,
two blue nodes, and that is kind of the way you can think of f, right?
f takes the colors and- and kind of encodes them as one-hot,
so that when you sum them up,
you basically count how many different colors you have.
Of course, f needs to be a function that does, that does this.
If f does not do this for you,
um, this won't, uh, this won't work.
So f has to be a very special, uh,
function, um, and then,
uh, it will, uh, work out.
So now the question is,
what kind of function f and Phi can I use?
How do I define them?
And we're going to u- to use them to basically define them with a neural network.
We are going to define them using a multi-layer perceptron.
Um, and why would we want to define it just using a perceptron?
There reason is that, uh,
there is something called an universal approximation theorem,
and it goes as follows: So, uh,
one hidden layer uh, multiple,
uh, layer perceptron with sufficiently large, uh,
hidden layer dimensionality and appropriate non-linearity can
approximate any continuous function to an arbitrary accuracy, right?
So what is this saying?
It says that I have this unknown special functions of Phi and f
that I need to define so that I- so
that I can write my injective function in terms of f and Phi,
but f and Phi are not known ahead of time.
So but- so I'm going to represent f and Phi with neural networks.
And then because multi-layer perceptron is able to
learn any function to arbitrary accuracy,
this basically means I can use data to learn f and Phi that
have the property to- to- to create these types of injective mappings.
So basically this means that, um,
we have arrived to a neural network that can model any injective function, right?
If I take a multi-set with elements x,
then if I apply a multi-layer perceptron to it,
sum it up and apply another multi-layer perceptron,
then multi-layer perceptron can, um,
approximate any function, so it can approximate my function f and Phi as well.
So this means now I have a neural network that can do
this injective multi-set, uh, mapping.
And, you know, in theory,
this embedding dimensionality, the dimensionality of the MLP could be very large,
but in practice, it turns out that, you know,
something between 100 and 500,
um, is good enough and gives you a good performance.
So what magic has just happened is that we
said any injective multi-set function can be written as,
uh, um, as, uh, uh,
uh, as a- with two functions, f and Phi.
F is first applied to every, uh,
element of the multi-set summed up and that
that is passed through the function Phi and this way,
uh, the- the injectivity, uh, is preserved.
And because of the universal approximation theorem,
we can model f and Phi with a multi-layer perceptron,
and now we have an injective, uh,
aggregation function, because MLP can learn any possible function, and, uh,
um, and, uh, the other MLP also can learn any function,
meaning, it can learn the function f as well.
So, uh, what is now the most expressive graph neural network there is?
The most expressive graph neural network there- there is,
is called Graph Isomorphism Neural Network or GIN for short.
And the way its aggregation function looks like it says,
let's take messages from the children,
let's transform them with a multi-layer perceptron,
let's sum them up and apply another,
uh, multi-layer, uh, perceptron.
And, you know, uh,
given everything I explained, this is, uh,
injective multi-set aggregation function,
so it means it has no failure cases.
It doesn't have any collisions,
and this is the most expressive graph neural network
in the class of this message passing,
uh, graph neural networks.
So, uh, it is super cool that we were basically able
to define the most powerful graph neural network,
um, out of, uh,
an entire class, uh,
of graph neural networks.
And we now theoretically understand that it is really all about the aggregation function,
and that the summation aggregation function is better than the average,
is better than the maximum.
So, uh, let me,
uh, summarize a bit, right?
We have described neighborhood aggregation fun- uh, uh, of, uh,
function of a GIN, um, and,
uh, we see that basically the aggregation is a summation.
We take messages, we transform them through an MLP and then sum them up.
And that has the injective property,
which means it will be able to capture the structure of the entire computation graph.
Now, uh, that we have seen what GIN is,
we are going to describe the full,
uh, model of the Graph Isomorphism Network,
and we are actually going to relate it back to the, uh,
Weisfeiler-Lehman graph kernel, the WL graph kernel that we talked about,
I think in lecture number 2.
And what we are going to provide is
this very interesting prospect where we are going to see that,
uh, GIN is a neural network version of the WL ker- kernel.
So, um, let me explain this in more detail.
So what is WL graph kernel?
Right? It is also called a color refinement
algorithm where basically we are given a graph G and a set of nodes V,
we assign initial color,
uh, c to each node v. Uh,
let's say we- the colors are based on degree of the node,
and then we are iteratively aggregating,
hashing colors of neighbors to create a new color for the node, right?
So we take the at- at, uh,
if you want to create the color at level k plus 1 for a given node,
we take the colors of the nodes, uh,
u that are its neighbors from the previous iteration, we take, uh,
color of node v from the previous iteration,
somehow hash these together into a new color.
All right, then hash- hash function.
The idea is that it maps different inputs to different, uh, outputs.
Uh, so hash functions are as injective, uh, as possible.
And the idea is that after k steps of this color refinement, the color, uh,
of every node will summarize the K-hop neighborhood structure,
uh, around a given node.
So let me give you an example.
Imagine I have two different graphs, um.
Here they are, they are different,
they are, um, uh, uh, non-isomorphic.
So the way we do this is, uh,
let's say we first simply initialize
all the colors to value 1 and then we aggregate, right?
So the- for example, this node has color 1 and then has three neighbors,
each one of color 1.
So this will be now one comma 1, 1, 1,
and then, you know,
every node does the same.
Now we are going to hash these descriptions,
these, uh, colors into new colors.
And let's assume that our hash function is injective,
meaning it has no, uh,
collisions, then this would be a new set of,
uh, node colors now.
Um, and for example,
in this case, this node, uh,
and that node have the same color because their descriptions,
uh, are the same, right?
They have color 1 and they have three neighbors, each with color 1,
so this particular input got mapped to a new color, uh, number 4.
And now I can then repeat this process,
uh, uh, one more time and so on.
What you should notice is that at every iteration of this WL kernel,
what is happening is we are taking the colors, uh,
from the neighbors, uh, and putting them together, together with our own color.
So if you go back and look here,
this is very similar to the, uh,
graph neural network, where we take messages from the neighbors,
combine it with the v's own message, somehow transform, uh,
all these into a new- and- and, uh,
transform this and call this- that this is the message for node V at the next level.
So this is essentially like a hard coded graph neural network, right?
We take, uh, colors from neighbors,
uh, aggregate them, take our own color,
aggregate it, and then call this to be the embedding of the node
v at the next layer or at the next, uh, level.
So the point is that as- the more iterations we do this,
the farther out information,
uh, is captured at a given node, right?
The farther out kind of the network neighborhood gets,
more and more hops get added to it.
And the idea of this color refinement is that the- if you are doing, let's say, um,
isomorphism testing, then the process continues al- uh,
until the stable coloring is reached.
And two graphs are considered isomorphic if they have the same set of colors.
In our case, the colors in these two graphs are different.
The distribution of them- the number of them is different,
which means these two graphs are not isomorphic and you- if you look at them,
you really see that,
uh, they are not, uh, isomorphic.
So now, how does this relate to the GIN model?
All right, GIN uses neural network to model this injective hash function, right?
The way we can, uh,
write out GIN is we can say, aha,
it's some aggregation over the- the, uh, embeddings,
messages from the children, uh,
from the neighbors of node v plus the color,
the message of the node V,
uh, from the previous, uh, step.
So the way we write this in terms of,
um, uh, GIN operator is to say, aha,
we are taking the messages from the children,
we aggregate- we transform them using an MLP,
this is our function f,
and we summed them up.
Um, and then we also add 1 plus epsilon,
where epsilon is some small, uh,
learnable scalar, our own message transformed by
f and then add the two together and pass through another function, uh, phi.
And this is exactly now a, um, uh,
an injective operator that basically an in- in
an injective way maps the neighborhood information plus the,
um, plus- plus the node's own information into
a unique embedding into a unique, uh, representation.
So, um, if- assume that in- input, uh,
feature is c is represented,
let's say as one-hot encoding,
then basically just direct summation is injective, right?
So if I say, how do I, uh,
have an injective function over a multiset where
elements of a multiset are encoded with one-hot?
Then basically, all I have to do is sum up these vectors and I'll get
a unique representation because every coordinate will
count how many nodes of a given color, uh, there are.
Now, if these colors are not presented so nicely,
you need to transform them with the function f so that it kind of
approximates this intuitive one-hot encoding, right?
So this means that,
uh, uh, GIN, uh, uh,
aggregation GIN type of convolution is composed of two M- uh, uh,
two MLPs, one operated on the colors of neighbors,
one, um, and then the aggregation is a summation plus some, uh,
final MLP that, again,
kind of provides the next level one-hot encoding so that when we,
again, sum up information from the children at the next level,
no information, uh, gets lost.
So you can think of these f's and, uh,
f's and phi as some kind of
transformation- transform- transformations that kind of softly do
uh, one-hot, uh, encoding.
So let's now summarize and provide the entire, uh, GIN model.
Uh, GIN, uh, node-embedding update goes as follows.
Given a graph with a set of nodes v,
we assign a scalar, uh,
vector to each node v,
and then iteratively, ap- uh, apply this, uh,
GINConv operator that basically takes the information from the neighbors,
takes its own information,
applies these func- uh, functions f and phi that are modeled
by MLPs and produces the next level embedding.
And if you look at this, this is now written exactly the same way as the WL, right?
Rather than hash here,
we write out this GINConv.
So this means that basically after K steps of GIN iteration,
CK summarizes the- the structure of the k-hop neighborhood around a given node, uh,
v. So to bring the two together,
this means that GIN can be viewed- understood as
a differentiable neural network version of WL,
uh, graph kernel, right,
wherein WL views node colors.
Um, and, uh, let's say we can encode them as one-hot and use this, uh,
abstract deterministic hash function,
while in GIN views node embeddings which are low-dimensional vectors.
And we use this GIN convolution with these two MLPs;
the MLP phi and MLP, uh,
f that, uh, aggregate information.
You know, what are the advantages of GIN over
the WL is that node embeddings are low-dimensional.
Hence, they can capture the fine
grained similarity of different nodes and that parameters,
uh, of the update function can be learned from the downstream task.
So we are going to actually be able to learn functions, uh,
f and phi that,
uh, ensure, uh, injectivity.
So, um, you know,
because the relationship between GIN and the WL kernel,
their expressive power is exactly the same.
So this now means that if two graphs can be distinguished by GIN,
they can be also distinguished by WL and vice versa.
So it means that, uh,
graph neural networks are at most as powerful or as
expressive as the WL kernel or the WL graph isomorphism test.
Um, and, uh, this is great because now we have the upper bound.
We know that GIN attains this upper bound and we also know that WL kernel,
both theoretically and empirically,
has shown to distinguish many or most of the real-world graphs, right?
So this means that GIN is the- powerful enough to distinguish most,
uh, real-world graphs, which is,
uh, great- uh, which is great news.
So let me summarize.
Uh, we design a neural network that can model
injective multi-set function by basically saying that
any injective multi-set function can be written as a,
uh, app- application of a function f to the elements of the multiset plus a summation.
Um, in our case,
we use a neural network for neg- neighborhood aggregation function, uh,
and rely on the, um, uh,
Universal Approximation Theorem to basically say that MLP is able to learn any function.
So this means that GIN is able to capture the neighborhoods in an injective way,
which means it is the most powerful or the most expressive graph neural network there is.
Um, and the key is to use element-wise summation pooling instead of mean or max pooling.
So it means that sum pooling is more expressive than mean or max pooling.
We also saw that the GIN is closely related to
the WL kernel and that both GIN and WL kernel can distinguish,
uh, most of the real-world, uh, graph structures.
To summarize, the- the- the important point of
the lecture is that if you say about mean and max pooling,
for example, mean and max pooling are not able to distinguish
these types of neighborhood structures where you have two or three neighbors,
all the same features.
Here is where maximum pooling fails because
the number of distinct- k- kind of the distinct colors are the same.
So whatever is the maximum is the same in both cases and this is, again,
the case where both mean and max pooling fail because we have, uh,
um, green and red and they are in the same proportion.
So if you rank, uh, different pooling operators by- by discriminative power, um,
sum pooling is the best,
is most expressive, is more expressive than mean pooling,
is more expressive than, uh, maximum pooling.
So in general, sum pooling, uh,
is the most expressive,
uh, to use in graph neural networks.
And last thing I want to mention is that you can
further improve the expressive power of graph neural networks.
So the important characteristic of what we talked
today was that node features are indistinguishable,
meaning that all nodes have the same node feature information.
So by adding rich features,
nodes may become, um, distinguishable.
The other important thing that we talked about today is that because
graph neural networks only aggregate features
and they use no reference point in the network.
Nodes that have the same, um,
uh, computation graph structure are indistinguishable.
And what we are going to talk about, uh,
later in the course is actually how do we improve the expressive power of,
uh, graph neural networks, um,
to be more expressive than GIN,
and to be more expressive that- than, uh, WL.
And of course in those cases,
it will actually require more than just the message passing.
It will require more advanced operations, um,
and we are going to talk about those, uh, in the future.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 10.1 - Heterogeneous & Knowledge Graph Embedding.txt
So today we are going to discuss, uh,
heterogeneous graphs and, uh, knowledge graph embeddings.
And in particular, we will be focusing on methods for,
uh, knowledge graph completion.
So the topic for today is about, uh,
first to talk about,
um, uh, a concept called heterogeneous graphs.
And, uh, basically so far in our course,
we had been handling graphs that have only one edge type and only one node type.
We just said there is nodes and edges and you know nodes are connected with edges.
So now the question would be,
how do we handle graphs,
directed- undirected graphs that have multiple types of, uh,
connections, multiple types of nodes,
multiple types of links between them.
Um, and this is the notion of heterogeneous graphs.
Uh, heterogeneous in a sense that they contain
heterogeneous sets of entities and heterogeneous set of, uh, edges.
Uh, and what we are going to talk today about in the first part is to discuss about, uh,
relational GCN that takes care about different,
uh, edge types, different relation types.
And then we are going to, uh,
talk about more specifically about knowledge graphs,
which are a class of heterogeneous graphs.
Um, how do we do knowledge graph embedding?
And also how do we do, uh,
a very common task which is called knowledge graph completion.
So let's start with, uh,
heterogeneous graphs and discuss relational GCNs.
So relational graph convolutional, uh, neural network.
So a heterogeneous graph is,
uh, defined by a quadruple,
is defined by a set of nodes, uh,
we'll call them V. Uh,
nodes could have, uh, different types.
Uh, it is defined by a set of edges that connect
different nodes and each edge is a- has a different,
uh, uh, relation type.
So this means that edge is now a triple that says node i is connected to node j,
via relation, uh,
R. And then also each node,
um, is of a different type and I can get the type of the node by,
uh, by looking into this, uh, uh,
function or set, uh,
T. So basically I have nodes and edges where every node is labeled by a type,
and every relation, um,
is labeled, uh, by a type.
So what would be some examples of heterogeneous graphs?
Uh, one very common example is in biomedical, um, uh,
field where you can basically take different biomedical entities, um,
and then relationships between them,
and represent this as a heterogeneous graph.
So for example, in- in this case,
I would have different, uh, node types.
I would have diseases,
I would have drugs,
I would have proteins.
I would have different edge types which is different types of relations between these,
uh, different types of entities.
And then you know, an example of the node here could
be the- could be the disease, uh, migraine.
And, uh, then I also have,
uh, different, uh, edge for example,
it would be that, uh,
a particular drug treats a particular, uh, disease.
And then, um, you know,
how can this- and- and then I can have different types of relations between them,
like causes, uh, uh,
and so on and so forth.
So that is one example, uh,
of a-, uh, of a heterogeneous graph,
the different types of entities,
uh, and different, uh,
types of, um, uh,
uh, relationships between them.
You can also think of event graphs as another example of a heterogenous graph,
where again, I could have different types of nodes,
different types of entities.
Uh, like for example,
a node type could be a flight.
I could have different, uh, edge types,
it would be a destination of a flight, um,
and then I could have,
you know, um, the destination,
different airports like the SFO, uh,
and create different types of, uh,
relationships, uh, between these, uh, different entities.
This is, uh, another example of a heterogeneous graph.
So now if we are given these type of a graph where we
have different types of nodes and different types of relations between them,
then we would like to be able for our model to handle this heterogeneity,
in some sense, to take advantage of it,
to learn that, uh,
that you know, treats, uh,
a given drug treating a disease is a different type of relationship,
than a given protein interacting with another, uh, protein.
So, um, wha- how are we going to do this?
Is we are actually going to extend our, um, GCN.
So, um, um, uh, graph convolutional, uh,
neural network, uh, to be able to handle different types of, uh, relationships.
We will call this our relational, uh, GCN.
Um, and we will,
uh, so far, right, we define the, uh,
GCN on one type of a relation and one type of a node type,
so on simple graphs,
and now we would like to expand it to,
uh, more complex heterogeneous graphs.
Um, the way I want to explain this is first to remind you about
the GCN and then it will become very natural, how do we extend it?
So, uh, the first question when you think about a graph neural network is, you know,
how do we- how does a GCN, uh,
operate and how does it update a representation of a given,
uh, target node, uh, in the graph?
And we have discussed these through this notion of, uh, uh,
message passing and- and computational graph,
where for a given target node,
let's say A, here we see its corresponding, uh, computation graph,
um, where every node gets information from its neighbors.
Notice that perhaps from the previous classes we talked about undirected graphs,
here now the graph is directed,
so every node only, uh,
gathers information along the direction of the edges.
So A gets information from B, C,
and D. But for example,
uh, node B, even though, um,
uh, it has two edges adjacent to it,
it only gathers information from C because C points to it and B points to A.
So that's why, for example here, uh,
the graph structure- the- the computation graph structure,
uh, is like this.
So this is now how you can, uh,
take into consideration, uh,
relations, uh, of edges.
Now, uh, that we have, uh,
re- reminded ourselves of, uh, uh,
uh, wha- how to, uh,
unfold, uh, the graph neural network over a directed graph,
we then talked about what does define a single layer of a graph neural network?
And we defined this notion of a message,
where every me- every node, uh,
computes, uh, the message coming,
uh, uh, from its, uh, children and this message,
uh, depends, uh, both on its, uh,
uh, on its message from the previous level,
um, as well as, uh,
the- the neighbors, uh,
that- that it has at the previous level.
And then the way the aggregation of these messages happens is
that- that the node V we will basically take these transformed messages,
uh, from each of the children and aggregate it in some way.
Uh, combine it with its own message and produce the message,
uh, or the embedding of the node, uh, at the next level.
And in order to add expressivity in terms of feature transformations,
not in terms of capturing the graph structure,
but in terms of capturing the feature, uh,
transformations, uh, we can then also add the,
uh, nonlinearity activation functions like sigmoid,
uh, ReLU, uh, and so on.
Um, and this is how we think of,
uh, graph neural networks.
Now, going back to the GCN,
and a- and a single layer of GCN is defined,
uh, by the following formula, right?
It's basically a node goes over its neighbors u
takes the previous layer representations of the nodes,
normalizes it by the- by the, uh,
parent's n-degrees, sums them up,
transforms them, and sends them through a non-linearity.
So, uh, what does this mean is that, uh,
the way you can think of this is that the aggregation of a- of a GCN is,
uh, uh, average pooling type of operator,
because of a summation and normalization here.
And then the message transformation is simple,
uh, linear, uh, transformation.
So now what happens if we have multiple relation types, right?
What if I have a
graph that has multiple types of relations?
Er, the way I will denote this is that I will use edges of different colors,
um, and, you know, here I labeled them as r_1, r_2, r_3.
Where it basically says, aha,
that A and B are connected according to the relation r_1, while,
you know, uh, B and C, er,
are connected by, er,
a relation, er, r_3.
So that's, er, the way, er,
we are going to think of the input graph now.
And the way we can now generalize a
GCN to the- this kind of heterogeneous graph is to use different network transformations,
different neural network, uh, weights,
different neural network parameters for different, er, relation types.
So it means that when we are doing relation transformation,
when we are doing message transformation,
we are not going to apply the same matrix W for every incoming edge,
but we are going to apply a different matrix W, um,
depending on whether- um,
er, what is the kind of the relation that we are considering.
So if we have three different relation types,
we are going to use three different types of, uh,
W matrices; one for relation 1,
one for relation 2, and one for, uh, relation 3.
So now, if I look at how does the, uh,
neural network, uh, computation graph for, uh,
the node A look like,
what is different now is that these transformation- message transformation operators,
these Ws, now have a color associated with them.
So it means that the message coming from, let's say,
node B and the message coming from node D, uh,
towards node A will be transformed using the same operator, the same W,
because the message is traveling along the relation type, uh, uh, r_1.
While, for example, the message from node C that,
uh, is connected to node A with, er,
relation r_2 will be transformed with a different transformation operator, the red one.
The red one is, er, designed for, er, r_2.
So here, um, you see- uh,
you see the difference, right?
Then, for example, um,
this means that now what- how we are going to write this up is that we have-
we have these transformation matrices W that are also indexed,
uh, by the relation type, right?
So in a relational GCN,
the way we write out the message propagation and
aggregation is the following: we basically say, um,
to compute the message of node v at level l plus 1,
we are going to sum up over all the relation types.
For every relation type,
you are going to look at who are the neighbors, uh, of, uh,
node v that- that are connected to node v according to this relation type
r. Then we are going to take the transformation matrix specific to that,
er, relation r to take the,
uh, messages from the- uh,
from the neighbors that are- er,
neighbors u that are connected to v according to- to this relation type r,
and we are are also going then to do the
me- the embedding of the node v from previous layer,
er, and, um, er, er transform it.
Uh, the- the important difference here is that Ws,
before, they used to be only differ layer by layer,
now what happens here is that we have W for every layer,
for every, uh, relation.
So, uh, another thing I did not explain is what is this C, uh,
er, sub v, r. This is simply a normalized node,
er, in degree compared to that, er, relation.
So it's basically the number of incoming, er, er,
relations of a given type r to the node, er,
v. This is still a- a graph neural network.
It is still- I can write it out in
this formalism of a message transformation and aggregation,
where basically each neighbor of a given- uh,
of a given node v transforms its- er,
its previous layer embedding according to the matrix W,
but now W changes for every, er,
re- based on the type of the relation between,
uh, you and me and, er, you know,
here we transform the- the message,
uh, from node v,
from the previous layer to, the, uh,
embedding from the previous layer into the message, uh,
for the- from- for the- from the previous layer,
and then the aggregation is still,
uh, simply a summation, right?
We take these, um,
transformed messages fro- based on the embeddings from the neighbors from previous layer,
where the transformation is relation-specific,
as well as node's own embedding from the previous layer uh, transformed.
We add all these together to get the embedding,
uh, at the next layer.
So this is called,
a relational, er, GCN.
So, um, basically, a relational graph neural
network or a relational graph convolutional neural network,
where the main difference is that now we have different message transformation,
er, operators based on the type of the relationship between a pair of nodes.
And this is denoted by this, uh, subscript,
uh, r that denotes,
er, the relation type.
So, um, this is how we defined a RGCN.
Um, the point is now, right?
We said that for every relation r,
we need L- capital L matrices, meaning, um,
these transformation matrices are different for
every layer of the neural network- of the graph neural network,
and they are different for every relation.
So the problem then becomes if you ask what is the size of this matrix?
The size of the matrix W is simply the embedding dimension, er,
on the- on the lower layer, um,
times the embedding dimension at the- at the upper layer, right?
So it's basically d, er, ^l and d, er,
^l plus 1 are the embedding dimension at
layer l and the embedding dimension at layer, er, l plus 1.
And these can quickly be a few hundred- you know,
these d's are in the order of a few hundred,
maybe up to a- up to 1,000, right?
So now the problem is that I have one such matrix per
every layer and I have one such matrix per every relation type.
And because, uh, heterogeneous graphs,
and in particular, knowledge graphs, can have hundreds,
can have thousands of different, er, relation types,
then you get- uh,
you can get tens of thousands of these different, er, matrices.
And each matrix can ha- is- is dense and can have quite big size.
And the problem then becomes that the number of parameters
of this model- of this RGCN model, um,
tends to explode because, uh,
it grows with the number of different relation types,
and there can be thousands of different relation types.
And one problem is that the model becomes too big, um, to train,
and then the other problem is that the model has too many parameters,
um, and overfitting can quickly become an issue.
So what I wanna discuss next is, er, two approaches,
how to reduce the number of parameters of this RGCN-style model,
um, using two techniques.
One technique will be to use, uh,
block diagonal matrices, uh,
and the other one will be to use basis or,
uh, dictionary learning as it is called.
So let me first talk about, uh,
use of block diagonal, er, matrices.
So the key insight is that we wanna make these Ws- W matrices,
the transformation matrices, we wanna make them sparse.
And one way to make them sparse is to enforce, to have this block diagonal structure.
So basically, non-zero elements are only along the specific blocks,
uh, of this bigger,
er, matrix, er, W.
Uh, if you think about this,
this will now reduce the number of non-zero, uh,
elements- the number of parameters in each W you have to estimate,
because you only have to estimate that the blocks,
uh, these two green blocks,
and you can basically ignore or assumed these,
uh, empty parts of the matrix are 0.
So if you assume,
for example that W is composed from B, uh, low dimensional,
uh, matrices, low-dimensional blocks,
then the number of parameters is going to decrease,
uh, by a factor B.
So if you said B by 10, uh,
you have just reduced the number of free parameters that you need to learn,
uh, or estimate by a factor of 10.
Of course, what do you lose?
What you lose is now that if you think about this as a transformation matrix,
what you lose is that, um,
embedding dimensions that are far apart from each other,
they cannot interact with each other, right?
So this means that, for example here,
only embedding dimensions 1 and 2 can interact with each other,
but not, um, let's say 2 and 3 because they are in different blocks.
So it may require several layers of propagations
and- and different structures of block diagonal matrices to be able for,
a embedding dimension 3 to kind of talk to embedding dimension 2.
Um, so basically what this means is that only nearby neurons or neurons
in the same block can talk to each other and exchange information with each other.
So perhaps your GN- GCN,
GNN may need to be a bit deeper,
but you have reduced, uh,
the number of parameters, uh,
significantly, which will lead to, uh,
faster training, perhaps more robust model,
less overfitting, um, and so on.
So that's, uh, the technique of,
uh, block diagonal matrices, uh,
where basically you reduce the dimensionality of
each W by assuming a block diagonal structure.
Uh, second idea how to, uh, make this, uh,
RGCN more scalable and its learning more tractable is to- to
build on the key insight which is we wanna
share weights across different relations, right?
We don't wanna consider relations as independent from each other,
but we want relations to kind of share weights,
uh, to also share some information.
And the way you can achieve this very
elegantly is that you will represent the weight matrix,
the transformation matrix W for each relation as a linear combination of basis,
uh, transformations or as of basis matrices.
And you call these basis matrices as your dictionary.
So this is also called dictionary learning.
So let me now, uh, explain.
The idea is that now your, uh, W,
uh, r is simply, uh,
weighted summation over this dictionary of matrices V,
um, that- and I have B- B of them, right?
So basically the idea is that I'll have a dictionary of matrices V.
Uh, and then to come up with a- with a, um,
transformation matrix for, um,
node W, then basically what I have to do is, uh,
have these, uh, weight- weight- weight scores,
uh, important weights, uh,
a, that say how important is a given matrix,
uh, V_b, for a given relation R, right?
And then the point is that these, uh,
matrices V are shared across all the relations.
So I can think of matrices V as my basis matrices or as my, uh, dictionary.
And then these, uh, weights, uh, A,
those are importance weights,
uh, for each matrix.
So basically I'll say, the transformation matrix for,
uh, a given, um,
relation R is some kind of linear combination of
these basis matrices where these linear combination weights are learned for every,
uh, for every, uh, relation, right?
So this means that now every relation only needs to
learn these importance, uh, weights, um,
which is just capital V number of scalars rather than,
uh, uh, a- a large number of different, uh, transformation matrices.
So now basically, every relation
specific transformation matrix is simply a linear combination,
uh, from this dictionary of, uh, matrices, uh,
V. And that's, uh,
an elegant way how to reduce, uh,
the number of parameters because b can be relatively small,
let's say, I don't know, 100,
uh, or 10 but,
maybe you still have,
um, uh, 1,000 different, uh, relation types.
And you basically say, I will represent every of the 1,000 relations as
a linear combination from my dictionary of 10, uh, weight matrices.
And that's a very elegant way how to reduce,
uh, the number of, uh, parameters.
So now that we have talked about this, uh,
scalability issue of, uh,
uh, RGCN with two different approaches,
one being, um, the block diagonal structure of the transformation matrix W,
the other one being that we cast learning of W as a dictionary learning problem,
which basically means we assume there is an underlying set of 10- let's say,
10 different basis matrices which we call dictionary,
and then every, uh, uh,
relation specific transformation matrix is simply a linear combination of the, uh,
of the matrices coming from the dictionary also reduces
the number of parameters makes the method more robust and, uh, speeds it up.
So now that we have RGCM, uh,
what we want to do is also talk briefly about,
how do you define various prediction tasks on the,
um, on the graph- on the heterogeneous graph?
So in terms of node classification or anti-classification,
um, it is, uh, it is, uh,
it is all- all, uh,
kind of as it used to be, right?
So, uh, RGCN will compute the representation or an embedding for every node,
uh, for every node, um,
then- then based on that embedding,
we can then create a prediction head, uh, for example,
if you want to classify nodes into k,
uh, different classes, uh,
then the final prediction head will simply be
a linear transform that will basically take the embedding,
multiply it with the weight vector,
uh, pass it through a non, uh,
non-linearity like a sigmoid and we can interpret that as a probability,
uh, of that given class.
So basically we will have a k head output rather than, uh,
um, h, and then have a softmax on top of it,
so that basically we interpret that as a probability that, uh,
that a given node is of the kth,
uh, given type or kth head.
That's how we think of node classification.
For link prediction, things get a bit more tricky because now links have different, ah,
types and we don't want to simply split links, uh,
randomly as we did it so far, uh,
because some types might be very common and other types might be very uncommon, right?
Some relation types might be very common,
some relation types might be very uncommon.
So what we would like to do is for every relation type,
we would like to split it between,
ah, training message edges, uh,
training supervision edges, validation edges, and test edges.
And then we would like to do this for every
of the relation types and then kind of merge, uh,
all of the message edges for the- for the training will, er,
take all the training supervision edges for each separate relation type into
the training supervision edges and then same
for validation edges and for test edges, right?
So kind of the point is that we wanna
independently split edges from
each relation type and then merge it together, uh, these splits.
And the reason we wanna do this is because this means that even for a very infrequent,
ah, very rare, uh,
relation type, some of its- some of the instances of it will be- will be in the training,
some of instances will be in the validation,
and some will be in the test set.
Because the point would be if you just blindly split the edges,
just by chance it can happen that a very rare edge type does not appear in your,
ah, validation set because it's just too rare and by
chance none- none of the edges, uh, landed there.
So that's why we wanna do this,
what is called stratification,
where we- where we independently split, uh,
each relation- edges of each relation type separately,
and then merge it all, ah, together.
Um, so we have these four different edge buckets,
training message as edges,
training supervision edges, validation edges, and test edges.
We do the splitting for each relation type separately
and then merge it all together into the four sets.
And then, you know, everything still, uh,
applies as we talked about, ah, link prediction.
So to tell more or give you more details about how you formalize link prediction,
you look at heterogeneous graphs.
Imagine, you know, I wanna be able to predict, um,
whether there is an edge or what's the probability of an edge between nodes E and A,
um, and that- that edge is of,
ah, type 3- of relation type 3.
Um, so imagine that this edge is a training supervision edge, right?
And let's say that all edge- all other edges are,
uh, training message edges.
So the way I would now, uh,
use the RGCN to score these edges,
I would take the final, uh,
layer embedding of node D. I would take the final layer embedding of node A.
And then I would have a relation specific scoring function f, ah,
that would basically, um,
take these two embeddings and transform them into a real- ah, into a real value.
So one app- approach to do this would be to use this kind of,
ah, uh, linear- uh,
bi-linear form, where basically I take one embedding,
I have the transformation matrix in-between and another embedding,
ah, so that at the end, basically this takes,
take the embedding of node D, transform it,
and then dot-product it with,
uh, embedding of node A.
And I can interpret these,
perhaps send it through some sigmoid or something like that.
I can interpret this simply as the probability that- ah,
there is an edge of relation type,
let's say r_1 between nodes E and A.
So that would be one way, uh,
to- to actually formalize and,
uh, instantiate, ah, this, ah, problem.
Now, um, how exactly am I thinking about these during training?
So let's assume again that this is the edge,
that is a supervision edge.
Um, and let's think that, ah, all other, er,
edges in the graph are- are the training message passing, ah, edges.
So we wanna use the training message-passing edges to predict, ah,
the likelihood or the existence of this,
ah, training, ah, supervision edge of interest.
What we also have to do in link prediction is that we have to create negative instances.
We have to create, ah, negative edges.
So the way we create negative edges is by perturbing the supervision edge.
So for example, if this is the supervision edge,
then one way how we can do- do it is that we corrupt the tail of it, right?
So we maintain the head,
we maintain node E,
but we pick some other node,
ah, that node E is not connected to with the relation of type r_3.
So in our case, we could, for example,
create an edge from E to B or from,
uh, E to D. So these could be our negative edges.
What is important when you create negative edges,
the negative edges should not belong to
training message edges or training supervision edges.
So for example, edge to the node C cannot be a negative edge because
node E is already connected to the node C with relation type 3.
So this is not a negative- uh,
a negative edge because the edge already exists and it's of the same type as here.
So we don't wanna create this, uh, contradiction.
So we have to be a bit careful when sampling these, uh, negative edges.
Now that we have, uh,
created a negative edge by perturbing the tail,
uh, so the end point of the- of the supervision edge.
We can now use,
uh, the GNN model,
the RGCN, to score- to score the positive as well as the negative edges.
And the- you know, the loss function you would wanna optimize is a standard, uh,
cross entropy loss where basically we want to
maximize the scores- the score of the training supervision edge,
we would like to maximize the scores of this guy,
and we would like to minimize the score of negative edges, like for example,
E to B or E to D. So that's how we- we would write down,
ah, the penalty, uh, as I show it here.
Um, and then, er, use the optimizer stochastic gradient descent to
optimize the parameters of RGCN to basically assign high probability,
high score to the training supervision edges and low score,
uh, to negative edges.
Now that we have the model trained,
now assume we move to the validation time.
We wanna validate, uh,
the our- the- the performance of our models.
So let's assume now that
the validation edge that I'm interested in is this edge between node E,
uh, and node D. And I'm interested whether it's of type r_3.
Then in this case,
training message edges and training supervision- supervision edges basically means,
in- in my example,
all existing edges of the graph are used, ah,
for the message propagation and then at the validation time,
I'm basically using all these solid edges to do the message propagation.
And I'm trying to score the value of this particular, uh,
edge from node E to node D, right?
And again, the intuition here is that the score of
this edge should be higher than the score of,
let's say all other, ah,
edges that are- that are in some sense
negative or don't exist in the data set yet from node D. So for example,
this would mean that the score of this node edge from E to D
has to be higher than the one from E to B in our case.
Um, and- and I cannot consider
these other edges because they are already used as a message passing engages in my,
uh, validation, uh, RGCN.
So notice that it is important that
these sets of edges that are independent from each other,
uh, when we score them.
So how would I evaluate this?
Right, I would get- use the RGCN to calculate the score of edge ED,
uh, according to the relation type 3.
I would then calculate the score of all the negative edges.
So in my case,
only two possible negative edges of type r_3 are E to B and E to F, right?
I cannot do A and C because they are already connected according to the relation 3.
So that would be kind of a contradiction.
So I have only two negative edges.
Ah, and the- the- the goal then is basically to obtain a score for all these three edges.
We rank them and hopefully the- the rank,
the output score of the edge, ah,
ED will be higher than the output score of EF and BF.
And then how do I usually evaluate the calculation matrix?
How do I usually, um, evaluate?
Um, I can do either what is called hits, which would be,
how often was the correct positive edge ranked among
the top K of my predicted edges or I can do
a reciprocal rank which is 1 over the rank of the-
of the- of the positive edge ranked among all other, uh, negative edges.
And then you can do mean reciprocal rank,
and the higher the mean reciprocal rank,
the better or high- the higher the hits score, the better.
So let me summarize.
We talked about relational GCN,
which is a graph neural network for heterogeneous graphs.
We talked about how to define it.
We talked about how to have a relation type specific transformation functions.
Uh, we discussed about, uh,
how to do entity classification as well as link prediction tasks.
We also discussed how- discussed how to make a relational GCN more scalable through, uh,
making matrices W, block diagonal or using
it as a- as a linear combination of basis transformations.
So this kind of dictionary learning approach.
And of course, you can take out GCN and extend it in any way you like, right?
You can- you could think that you have- you know,
you could have RGNN,
you could have RGraphSAGE,
you could have R graph attention networks.
So all these, ah,
basic fundamental GNN architectures that we already discussed.
It's kind of natural to extend them to the- to this, er,
multi-relational case, ah, by- by
basically adding relation specific, uh, transformations.
So it's kind of natural
that- how you can play with this model and make it more expressive,
and richer and so on.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 10.2 - Knowledge Graph Completion.txt
What I want to talk about next is to talk about knowledge graphs, and in particular,
I wanna talk about knowledge graph completion task, uh,
using embeddings and I wanna present you
some very interesting knowledge graph completion methods.
So knowledge graph, uh,
the idea is that you wanna store knowledge about a given domain in a graph form.
And the idea is that we wanna capture entities,
types, and relationships between different entities.
So the idea is that now I will have the nodes,
which I will call entities.
Entities will be labeled with different, uh, types.
Um, and then I will have different types of relationships, uh, between entities.
Um, and in this sense,
knowledge graph is just an example of a heterogenous graph but generally,
we would think of a knowledge graph as something that
captures factual knowledge about a given domain.
So for example, you could take a bibliographic network where you could say,
I'll have different node types like papers,
titles, authors, conferences, years.
And then I will have, uh, different, uh,
relation types like where was the publication done,
what year was it done,
what title does it have,
who is the author of it,
who cites it, and so on.
And this is, for example,
one way to look at the schema of this type of,
uh, knowledge graph would be the following, right?
Paper links to conference,
papers cite each other,
papers have titles, have years- publication years, and have authors.
And of course, that's multiple authors,
multiple conferences, and all these, uh, links to each other.
Another example is something I already discussed would be in biomedicine,
there is a lot of knowledge graphs where we can have
different types- node types in terms of drugs,
diseases, adverse events, proteins,
uh, disease pathways, and so on.
And then we also can have different, uh,
types of relations like has function,
causes, is associated, treats,
uh, is a, type of relationship.
And now we have this knowledge about biology and how, uh,
how, uh, life works encoded in this, uh, graphical form.
And then, uh, of course,
there are a lot of, uh,
knowledge graphs that are actually publicly available and store knowledge about,
uh, other types of entities as well.
So for example, about real-world entities, you know,
Google is using, uh,
Google Knowledge Graph to make,
uh, search results better.
Uh, Amazon is using their product graph to
understand properties of different products and be able f- to,
uh, to search and recommend products better.
Uh, Facebook has a Graph API because they think of the social network interests,
relationships between people's schools they graduated from as nodes in a giant graph.
Uh, IBM Watson in the background uses, uh,
a giant, uh, graph to be able to answer questions and reason.
Uh, Microsoft, uh, search engine and the company is using Microsoft Satori,
which is their own proprietary knowledge graph.
Uh, you know, LinkedIn,
uh, I think, uh, calls its Knowledge Graph,
uh, the- the- the Economic Knowledge Graph and- and so on and so forth.
So knowledge graphs are basically heavily used in
industry to capture background knowledge about a given domain,
to capture relationships between the nodes in a given domain.
And for example, one way, uh,
you can use knowledge graphs is simply to serve information.
So for example, if you go to Bing search engine and say, you know,
what are the latest movies, uh,
by the director of Titanic,
this is a- a knowledge graph grid, right?
You find Titanic, you say who has directed it?
You find that person and you say,
which other movies has this,
uh- uh, person directed?
And you can directly,
uh, surface this information.
And without having this data encoded in a graphical form,
answering this type of query,
this type of question,
would be practically impossible.
Knowledge graphs are also, uh,
very important in question answering and conversational ag- agents.
And here I show kind of the- the system diagram, uh,
for, uh, one of these,
uh, one of these, uh, types of,
uh- um-uh- uh, agent- uh,
systems where basically you- you get in the questions,
you wanna have a conversation with an agent
and the agent will understand what- what are
the entities that- that are contained in the question and how
these entities relate to each other so that it can then come back with a,
let's say, an intelligent, uh, answer.
And you can think, for example,
of knowledge encoded in Wikipedia, IMDb,
and so on as examples of data sources you can use, uh, for your knowledge graph.
Right? So there are many publicly available, uh,
knowledge graph, uh- uh- uh,
knowledge graphs out there.
For example, FreeBase, um,
then there is Wikipedia or Wik data,
uh, there is DBpedia,
uh, and so on and so forth.
And one common characteristic of
these knowledge graphs is that they- they are massive, they have millions,
tens of millions of nodes and edges,
but they are also notoriously incomplete,
meaning a lot of relationships are missing.
And one of the most fundamental tasks in
knowledge graph literature is to say given a massive knowledge graph, enumerate,
uh, or, uh, identify what task- what relations are missing in the graph.
So the question is, can we predict plausible but missing links in the knowledge graph?
So gi- to give you an example, so for example,
FreeBase is a knowledge graph, uh, that,
uh, was acquired by Google and Google uses it,
uh- uh- uh as a basis for Google Knowledge Graph.
It contains knowledge about real-world entities and their relationships.
So it has 50,000 entities,
it has 38,000, uh,
relation types, and 3 billion edges, right?
So- so you know, right,
the num- the cardinality of the number of different relationships,
it's 38,000, right? So it's huge.
And now if you think of this in the context of RGCN,
we would need to learn 38,000 different
transformation matrices for every layer of the GCN,
which clearly becomes intractable, right?
But what is interesting is, for example, uh,
not- almost like 94% of all the people,
person nodes in FreeBase have no place of birth and, you know,
78,000, uh, 78% have no, uh, nationality.
All right? So these knowledge graphs are notoriously incomplete.
Um, and, uh, the question then is,
can we automatically infer what is nationality of a given node,
or could we automatically infer where was a given node,
a given person, uh, born.
And this is what is called, uh,
knowledge graph, uh, completion.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 10.3 - Knowledge Graph Completion Algorithms.txt
So let's talk next about
knowledge graph completion tasks and various methods to accomplish,
uh, knowledge graph completion.
In particular, we'll talk about methods that are,
uh, that right now will have cryptic names,
but it will become, uh, clear: TransE,
TransR, DistMul, and, uh, ComplEx.
So I'll, I'll introduce what these methods are,
and we'll be in particular interested,
what kind of relationships can these methods capture and predict?
So let's think about,
uh, knowledge graph completion task.
Where basically we are given an enormous knowledge graph,
and the question is,
can we impute- can we predict missing information?
Because the knowledge graph is incomplete.
And the way the knowledge graph completion task works is that we are given the head node,
we are given the relation, uh,
type and we'd like to predict the missing tail.
So notice that this is slightly different from
classical link prediction task where nothing is given,
we just say, predict what links are missing in the entire graph.
Here we would actually say, aha,
we are interested in this particular node,
we are interested in this particular, uh,
relation type and we wanna ask,
you know, where is,
uh, this particular author?
Which other, let's say,
genres is this author connected to?
So for example, uh, JK Rowling, right, the, uh,
Harry Potter author is connected to,
let's say, uh, genre science fiction.
So that would basically be,
uh, something we would wanna predict.
So what we are given is the head node,
we are given the relation type and we wanna predict the tail.
Um, the way we are going to do this is we are going to do through node embeddings, right?
So every node, every entity in the knowledge graph will have an embedding.
And we are going just to use shallow embeddings.
So basically it means we will wanna learn an embedding vector for every,
uh, entity type in the, uh,
in the knowledge graph so that we can then do prediction well.
So the point is,
we won't be using GNNs,
um, we'll just assume we are doing shallow encodings.
But you could also, uh,
use GNNs if you would like.
So, uh, knowledge graph representation will be
such but knowledge graph is represented as a set of triples: head, relation, tail, right?
So head is the source,
relation is the type of a relation,
and tail is the endpoint of the relation.
So head could be, you know,
I know Barack Obama and, uh,
relation would be nationality and the tail would be United States.
So the key idea how we wanna do this is that we wanna model entities and the
relations as embedding points and vectors in the embedding space.
And we wanna associate entities
and relations with shallow embeddings, as I mentioned.
So we are not using GNNs just for simplicity to keep our life simple.
Um, and then the idea is that given a true triple: head, relation, tail,
the goal is that the embedding of head,
relation should be close to the embedding of the tail.
Um, so I know this is a bit abstract right now.
So let me give you, um,
explanations how can we embed head,
relation and how do we define this closeness, uh,
that the embedding of head,
relations should be close, uh, to the tail.
And really, all different methods we'll talk
about differ how these closeness and how this,
uh, embedding is produced.
The simplest idea, and the kind of the most natural one, is called TransE.
And Trans- TransE basically has this intuition of translation.
Where basically the idea is that for our triple: head, relation, tail,
we wanna learn the embedding of the head,
we wanna learn the embedding of the tail,
and we wanna learn also the embedding vector r. Um,
and we, and the idea would be that we want it-
we want to find these embeddings in such a way that head plus the, uh,
um, of- plus vector r is- gives me the, uh,
the point, uh, where the t is embedded, right?
So for example, if, uh, um, um,
Barack Obama, uh, nationality United States, then I want,
I want them to be embedded in such a way that if I start with the head,
we start with Barack Obama,
I move according to this relation vector r,
I would end at the point where United States is embedded.
And of course, I don't want this to only work for,
uh, let's say Barack Obama,
I'd like to w- this to work to be true for any person and their,
uh, their country of birth, right?
So for example, if in this graph, I don't know, um,
it would be, uh,
myself here as well,
maybe on a point here.
Then if I would move according to this vector r here,
I'm from Slovenia, so my nationality should be Slovenian.
So the, the country of Slovenia should be mapped somewhere here, right?
Basically I would apply the same translation and, uh,
hopefully the- the- the- the country that is
embedded close to that point is the country of my,
uh, nationality, all right?
So here's, uh, here's, uh, here's the idea.
So the scoring function, uh,
that would measure proximi- that would measure proximity between the head and tail,
would simply be head plus this vector r minus,
minus the tail, right?
It basically says what is the distance between h plus r,
uh, and, uh, the point, uh, t?
And here I could use, let's say Euclidean distance if I would like.
So this is what is Trans, uh, TransE.
Basically it wants to learn these translations
in the embedding space so that you can get from head,
according to a translation that's- that is the relation specific,
to the, uh, to the tail,
uh, of that, uh, relation.
Um, s- I will- I- I won't go into details basically of the learning algorithm.
But essentially the idea is that you wanna have entities and
relations that are first initialized, uh, uniformly, right?
You want an embedding for an entity,
you want to learn the translation vector for a relation.
You s- you- these are all the parameters of your model.
You would, you know, sample a positive example, a positive triplet.
You would, for example, sample ,uh,
what is called a corrupted triplet,
where you would have the head,
the relation the same,
but you would pick kind of a random tail.
You would pick a country that is not the- the- the true country, uh,
of nationality, and then you would say,
let me try to, uh,
find the embeddings, uh,
the values for head, relation,
and tail such that the distance between, uh,
the real tail and the corrupted tail,
uh, the real tail, uh,
and the embedding of h, uh,
plus r is smaller than the distance between the h plus r and the corrupted tail,
which basically means a country that is not,
uh, the true nationality.
And you can basically do, uh, the learning.
So now, uh, that we have done this,
this sounds like a great, very natural idea.
What is cool about it is that now we don't only think
of- of the embeddings as points in the space,
but we also learned this vector r that allows us to move or translate,
uh, between different points.
It allows us to move in a given direction.
Um, and of course the question is,
what kind of relationships is this type of approach able to, uh, learn?
And when I say what kind of relationships,
I really mean relationships with different, uh, properties.
For example, some properties might be symmetric or reciprocal, right?
If, uh, person 1 is a roommate of person 2,
then person 2 is also a roommate of person 1, right? It's mutual.
Um, for example, then there's- there might be relations that are called inverse, right?
If I am someone's advisor,
then that other person is my, uh, advisee, right?
So advisor and advisee are kind of inverse relations.
Whoever is in a relation of advisor,
in one way, advisee is in the other, uh, direction.
So the question then becomes,
can we corre- categorize this type of relation better than sort of relation properties?
And, uh, are these knowledge graph embedding methods like the TransE that I discussed?
Are they expressive enough to model these patterns?
So the- the relation patterns,
uh, we are going to discuss are, uh, four.
First is symmetric and antisymmetric relations, right?
Like, uh, family, roommate are what we call symmetric because they are reciprocal, right?
If I'm your roommate,
you are also my roommate.
It cannot be only one way.
Um, antisymmetric would be, uh,
would be part of hypernym, hyponym.
So it's basically one is with relation,
uh, with the other,
but not the other way.
Uh, inverse, uh, relations would be like advisor advisee.
Where if I'm related to you with one relation,
you are automatically related to me with the- with a different relation,
uh, then it would be, uh, transitive relations.
For example, uh, friend of,
which would be if x is friends with y and y is friend with z,
then, uh, x and z are also friends, right?
So this would be one way of having the notion of transitive relations.
Or if you think about relationship type,
trans- uh, those are also transitive.
That for example I say, my mother's husband is my father.
So, um, you know, that's,
uh, that- that's also saying, aha,
if- if you start with me and you- you
tra- you transform according to the mother relation,
and then you take the mother point and transform
according to the fa- husband relation, uh,
you should arrive to the same place if you take myself and
transfor- me- me according to the father relation, right?
The two, uh, the two,
uh, uh, the two embeddings should meet.
And then another important thing is that there are, uh, 1-to-N relations.
For example, student off, right.
I can, uh, have multiple students or I can have multiple advisees.
So I can have 1-to-N, uh, relations.
So these are different relation patterns we are going to consider,
and what we are going to do next,
is- is we are going to look whether TransE is able to capture these type of relations.
So, for example, let's first talk about, uh, antisymmetric relations.
Uh, what would this mean is,
uh, h is, uh,
related with t, uh,
t cannot be related,
uh, with h according to the same relation,
like, uh, part of,
uh, hyponym is an example.
And- and TransE captures this, right?
Because if I go from head,
um, according to the relation vector r,
I arrive to the tail but now we find that the tail and again,
apply this relation transformation, uh,
r, I don't get back to h. I get to some other point.
So this means that, uh, um, um,
TransE naturally captures this antisymmetry,
uh, of relations that have this property.
Another of the questions is,
can TransE capture inverse relations like advisor/advisee, right?
So if, uh, if h and t are related by r2, then,
you know, there's some other r1 that makes,
uh, tail and the head related, right?
Can we model, uh,
inverse relations? Yes, we can.
Uh, in simply the r2- the- the vector r2 should simply be the negative,
uh, of the vector r1.
So it means that from head according to r1, I move to tail.
Then if I wanna move from tail back to head,
I just move in the opposite direction.
So in TransE, this is naturally captured and naturally doable.
So inverse relationships are possible.
How about composite or transitive relations?
So I, uh, where I say if x and y are related and y and z are related,
then x and z should be related as well, right?
So in TransE you can simply do this by, uh,
by saying r3, uh,
is simply a summation of r1 and r2, right?
So if I go from x via r1 to y- to y,
and then I go from y via r2,
uh, to, uh, z,
then I can basically simply go from x to z according to,
uh, r3 and r3 is simply the vector r1 plus vector r2.
So again, we can do, uh,
composite relationships with TransE, uh, easily.
So how about some relationships types that are- uh that TransE cannot capture?
So for example, interestingly,
uh, TransE cannot capture symmetric relationships, right?
Relationships, uh, um, uh, where it's- uh,
where like family or roommate that are reciprocated, right?
Basically saying if, uh,
h- h- h is related with r to t,
then t should be related to h with r as well and- and the point is this right?
If you go from h according to r to t,
then what you'd like to do is apply the same r to t as well and get back to
h and the only able- way you are able to do this is to make r to be 0,
which basically means that h and t are embedded on top of each other.
But that is not good because h and t are different entities.
They are distinct entities with distinct properties.
So this means that TransE cannot embed and cannot do this kind of,
um, sy- uh, symmetric, uh,
reasoning or it cannot model,
uh, symmetric, uh, relations.
And then another part that, uh,
TransE cannot do is 1-to-N relations, right?
The idea is that if I have a head, um, uh,
and, uh, and, uh, relation,
and maybe this is a 1-to-N relation.
Uh, then basically I wanna- I wanna go from head according to relation r to t1
and I also wanna be able to go from head according to relation 1 to t2, right?
Both exist in the knowledge graph.
For example, student of, right?
I can be a mentor or I can have two students simultaneously.
And again, TransE cannot model this type of relations because again,
the only way for this to work would be that t1 and t2 map into the same vector, they
map into to the same point in
the embedding space so that when you go from h according to r,
you arrive to the same, uh, point.
But again, this is not- this is not good because this would mean that
t1 and t2 are the entities embedded in the same point.
So they are indistinguishable from one another.
But in reality t1 and t2 are completely different entities.
So this is what TransE cannot, uh, model.
So let's look at a different method called
TransR are that will allow us to fix some of these issues, all right?
So what we have seen so far is that the TransE models, uh,
trans- a translation of any relation in the same embedding space.
So the question is, can you design a new space for each relation and
do the translation in a relation-specific space?
And the method that proposed this idea is called TransR,
and it models entities as vectors in the entity space,
so entities as points and models each relation as a vector
in rela- in the relation space together with a relation specific,
uh, transformation or projection matrix.
So, um, let me give you the idea how this works.
So the idea is, as I said,
TransR models entities as points, uh,
and for every relation,
we are actually going to learn two things.
We are going to learn the translation, uh,
relation vector and we are going also to learn this projection matrix,
uh, M for every relation.
So the idea is, right,
if I want to,
uh- uh- what- the way I'm going to now predict relations.
If I say are h and t related, uh,
by relation r,
I'm first going to apply matrix, uh,
matrix M to both points and basically transform them
into a new space and then in this new transformed space,
I'm going to kind of apply the TransE, um, intuition.
So basically I'm going to use this vector to move around.
So, um, right, I basically,
take the original embedding.
I transform it according to the matrix M,
which basically means I scale it,
I rotate it, I translate it,
and then I apply my vector r, uh, in there.
So, uh, you know,
what will this buy me?
For example, what this buys me is that it allows me to model, uh, symmetric, um,
relations because imagine, uh,
again a symmetric, uh,
relation like roommate reciprocated right?
Then basically, I can run a roommate specific,
uh, function, uh, specific, uh, projection matrix,
uh, M that will take two different points and map them into the same,
uh, underlying point, into the relation specific, uh, space.
So it means that two people that are roommates,
um, will be able to be mapped into the same point.
So that now here, uh,
I'll be able basically to say,
uh, no translation needed,
r is 0 and I'm, um,
I have this, uh, symmetric,
uh, relationship, uh, right now.
So this is- what this allows us to do,
which for example, TransE was not able to do.
Um, how about anti-symmetric relations?
Um, again, we can easily do this, right?
Because for example, the translation matrix,
uh, can- can, if I have- if I take two different points,
the translation matrix can- can again transfer, uh,
into this relation specific spa- space in such a way
that the two points don't collide and then the same way as in TransE,
um, uh, we are able to do,
uh, the same thing here, right,
from head according to the relation I get to t,
but from t according to the relation,
I don't get back to, uh, head.
So this is also possible,
so it can model both symmetric as well as anti-symmetric, uh, relations.
Can, uh, uh, um, uh, this TransR also model 1-to-N relations, right?
Uh, like student of?
Uh, and actually again it turns out it can.
TransR can model this because again,
we have enough flexibility in the projection matrix r that can learn how to take,
let's say t_1 and t_2 and map them into
the same point so that when in this relation specific space,
I go from head according to the relation,
I arrive exactly to t_1 and t_2 that are embedded to the same position.
So in the original embedding,
these are two different points but in the, uh,
let's say the relation specific embedding
that is achieved by this transformation matrix M,
I can enforce t_1 and t_2 to be in the same point.
So these 1-to-N relations can naturally be captured.
So, um, this sounds great, right?
It seems we have kind of fixed, uh,
all the issues with,
uh, trans, uh, -TransE.
We can also do inverse relations because, uh,
the same as we were do- able to do them in TransE,
we can do them in TransR, right?
So basically we just, uh, uh,
-the transformation matrices for both relations have to be the same and then,
you know, one translation vector is the reciprocal or the inverse,
uh, of the other, uh, translation vector.
Um, what is the issue?
What trans - what tra- what does, uh,
TransR- what is it not able to do that TransE,
was able to do?
Is one thing: is composition relations, right?
In, in TransR, you cannot do this type of compositional relations, right?
Meaning, uh, remembering, uh,
uh, where you say, a-ha, if x,
x is related by y with r_1 and y is related with z with r_2,
then, uh, x and z are related by r_3.
This was very easy to do in, uh,
TransE because everything was in
the same space and you just kind of moved, uh, around.
Uh, but in TransR we cannot do these compositional relations.
Um, and the reason for this is that each relation has
a different space and we know how to
model from the original space to the relation-specific space,
but we don't know how to model between different, uh,
relation-specific, uh, spaces.
So these relation-specific spaces are not naturally, uh, compositional.
So, uh, this means that the, uh,
compositional relations, uh, TransR,
uh, cannot, uh, do.
So, um, this was second method, uh, we discussed.
So let's now talk, uh,
about the third method as well that,
uh, is, uh, -that is based on what is called bilinear modeling.
Um, and so far,
right, we were using, uh,
the scoring function simply as a,
as a distance in the embedding space.
Like in TransE and TransR we said the head plus relation should be close to the tail.
But, um, now what if we change, uh,
the embedding, uh, the,
the way we're scoring different relations?
What if we change this function n-l, f?
But it won't be, uh, distance anymore but something more interesting.
So for example, uh, this DistMult, uh,
is simply saying let's embed entities and relations as vectors in the,
uh, in the embedding space.
Kind of similar to TransE.
But let's use a different, uh,
scoring function rather than using the distance, uh, as in TransE.
Uh, let's use, uh,
this type of scoring function where we basically just, uh,
have this bi- bi-linear model when we say the score is simply,
a coordinate wise product of h,
r, and t, right?
So the idea is if I have,
uh, h, uh, r and t,
I simply multiply these things, uh,
uh, entry by entry,
sum it up, and that is my score.
And the idea is if, uh,
head relation tail, uh, is true,
then the score should be high and otherwise,
the sh- the score, uh, should below.
The way you can think of
this scoring function is that you can think of it as a hyperplane,
uh, in the embedding space.
So basically, wha-the way you can think of it is you can think of it
as cosine similarity between h times r,
uh, and t. And cosine similarity is simply a cosine of the angle between two vectors.
Um, and what this means is that if the two vectors are orthogonal,
then the cosine similarity will be 0,
and if two vectors are completely aligned, uh,
the cosine, uh, similarity will be, uh, will be 1.
Uh, another thing that is important is that because
this now defines like a hyperplane in the embedding space,
this dot product can either be positive or it could be negative.
And whether it's positive or negative,
this tells me whether the,
the point lives, lies on the left-hand side or whether it lies
on the right-hand side of the, um, of the hyperplane.
And hyperplane is really defined as a vector that is, um, uh,
that is orthogonal to this hyperplane and this vector is simply h times r, right?
So basically h times r, uh,
defines a hyperplane that is orthogonal, uh,
orthogonal to it and then,
uh, the, the tails, uh,
that, that are related to h according to relation r should
fall on the same side of the hyper- hyperplane as this,
uh, um, uh, uh, normal vector is,
and tails that are not related to it should fall on the other side,
uh, of the hyperplane.
So that's, uh, the intuition and the idea how this DistMult, uh, works.
So can it do 1-to-N relations?
uh, yes it can.
Uh, the reason, the reason for this is because we can think again of, of a,
of a, of a hyperplane and if,
uh, if, uh, I have 1-to-N relations,
then t_1 and t_2 should fall on the same side or should fall on the same distance on or,
uh, from the, uh, hhyperplane and that's easy to achieve.
Um, in terms of symmetric relations,
again, we can, uh,
naturally model this, uh,
because, uh, multiplication is commutative, right?
So we can flip the order in which we multiply things and we'll get,
we'll get to the same value.
So because, uh, um,
summation and multiplication are commutative,
um, this is, uh, naturally true.
Uh, however, the limitation are anti-symmetric relations, right?
Like a hypernym, part of where uh, uh and, uh,
the-the idea here is that DistMult cannot model antisymmetric relations.
Um, again, because of the, uh,
commutativity, uh, of, uh, summation and,
and product, it means you get symmetry but you don't get, uh, antisymmetry, right?
Um, h times r times t will always be the same
as t times r times h and that's, that's the issue.
So, uh, we can- DistMult cannot do that.
And then the other thing DistMult cannot do is modeling inverse relations, right?
If, uh, uh, h and t are related with r_2 then they should also be related,
uh, in the different direction with r_1.
Like advisor, advisee type of relation.
Um, and the reason, uh, um, uh,
DistMult cannot, uh, model this is,
uh, because, uh, uh,
for- because of the fact that it does not model rel- uh,
inverse relations so the only way
to model inverse relation would be that r_2 and r_1 are equal.
But semantically this does not make sense because then the embedding of
the relation advisor is the same as the embedding of the relation advisee.
So basically this would mean that if I'm your advisor,
you are my advisee but at the same time,
if I say, are you my advisor as well,
the answer would be yes and am I your advisee,
the answer would be yes as well.
So it will be like two symmetric relations rather than a inverse relation.
So this is where, uh, DistMult fails [NOISE].
Um, and then the last place, uh, where, uh,
DistMult fails is, uh,
composition, uh, relations, right?
And the problem is that DistMult defines hyperplane for,
uh, each, uh, head, er,
relation pair and the union of hyperplanes induced by this,
er, by this composition of relations cannot be expressed by a single, uh, hyperplane.
So kind of, you know, a union or an intersection of
hyperplanes is not a hyperplane, uh, anymore.
So that's why- th- that's the intuition why composition relations,
uh, are also not possible.
So the last method I want to talk to you about is called complEx.
And the idea here- the reason why I want to do this is that we don't
necessarily have to embed points into the simple Euclidean er,
you know, er, real, uh, space.
We can also, um,
have complex vector, uh, embeddings.
We can embed them in complex spaces, right?
So ComplEx embeds entities using in the complex vector space, right?
Where every- every point now has, uh,
two, er- it has an imaginary part and a real part.
And the one concept from, um, com- uh,
ComplEx algebra that is important to keep in mind is
this concept of co- complex conjugate, where you basically say,
if u is a complex number with a real part a and imaginary part, uh, b,
then a complex conjugate of it is simply that it's not plus b times i,
but is minus b times i.
And this, uh, notion of
complex conjugate will be important as we analyze, uh, this model.
So the way you can now think of this is that in ComplEx we can actually use, um,
the scoring function that is simply, uh,
similar to the, uh- to the ComplEx- uh,
to the scoring function in DistMult.
But we are only then taking the real part of the- of the ComplEx function because,
uh, this is- er, this will give me a complex number.
So I wi- want to only take,
uh, the real, uh, part of it.
So let's quickly analyze,
uh, this approach as well.
So can it model antisymmetric relations?
Uh, yes, it can.
Uh, the way- the way we,
uh- we can do this is that basically we can achieve that for,
uh, different, uh, um, uh, relations.
Uh, we get basically high or low value of our scoring function, uh,
basically due to the, uh,
asymmetric modeling of complex, uh, conjugates.
Because here we are using,
uh, the complex, uh, conjugate.
Uh, can we model symmetric relations?
Again, um, yes, yes,
we can because we can simply say the imaginary part of the relation, uh,
to be 0, um, and then, uh,
because everything works in- in- in- in the real part, uh,
we are able to model, uh,
symmetric relations, um, as well.
And then, you know, can we mo- can complex model,
uh, symmetri- uh, inverse relations as well?
Uh, yes, it can.
How? By setting, uh, r- um, uh,
by setting r_2 to be, uh,
com- complex conjugate of, uh, r_1.
Um, and this basically means that, uh,
the real part of the two er, will be, uh,
exactly, uh, the same because what differs is only,
uh, the complex, uh, part.
So that is, um,
uh, in terms of modeling inverse, uh, relations.
Uh, how about composition and 1-toN relations?
Um, because, uh, ComplEx shares this property with DistMult,
meaning it uses the same, uh, scoring function,
then it turns out that ComplEx also cannot model composition relations as well as,
uh, 1-to-N uh,
relations because it uses kind of a very similar, uh, scoring function.
So this basically now concludes, uh,
our discussion of different knowledge graph embedding,
uh, methods for knowledge graph completion.
We- we went through four of them.
We went through TransE,
TransR, DistMult and ComplEx.
And what is interesting is that each of these methods has a bit different idea, right?
Um, the- the two- first two methods embed into the- into the,
um, real, uh, space,
so into the Euclidean space.
Uh, but one uses the translation, uh,
idea for every relation,
while TransR also uses this, er, um,
transformation matrix, er M. And, uh,
here I show different properties or
different relation types that these methods can model.
Um, TransE is the only one that can model composite relations where they can be composed.
Uh, DistMult changes the scoring function, um, and, uh,
and ComplEx actually embeds, uh,
not into the Euclidean space,
but in the complex space.
And the reason why I wanted to show you
these different methods is to see kind of the diversity of options that, uh,
embeddings, uh, allow you,
uh, to work with and, uh,
a lot- give you a lot of freedom how you define your predictive model.
So how do you do now knowledge graph embeddings in practice?
Uh, different knowledge graphs may have
drastically different relational patterns and different properties.
So it is not that there is one method that works best, uh, on all of them.
It really depends on what kind of, uh,
relationship types you are interested in modeling and,
uh, what kind of relations do you wanna predict.
If you really want to predict, uh,
1-to-N relations and model 1-to-N relations, then don't use TransE.
if you really care about composite relations and these kind of compositions,
then use, for example, TransE.
If what you care about, uh, for example,
would be antisymmetric relations,
don't use DistMult, um, and so on.
So it really depends, uh,
on- on the graph and what you wanna do.
In general, you know,
TransR are and ComplEx have the most kind of- are able to model the most,
uh, diverse set of different, uh, relation types.
So what you can always try is try with TransE because it's very,
uh, simple and has a very nice intuition.
And then you can use more expressive model like complEx, uh, TransE.
You- there's also an notion,
a model called trans- RotatE,
which is TransE, but with complex embeddings.
Um, so there is a lot of different approaches and different,
er, uh, ideas here.
There is a paper linked up here that s- uh,
that serves as a very good follow-up, uh,
reading if you are interested in this topic.
So to summarize, we talked about the, uh, knowledge graph,
embedding a knowledge graph completion task
as one of the very important task in knowledge graphs.
And we talked about four different methods.
TransE, TransR, DistMult and ComplEx,
er, models with different embedding space,
and with different type level of expressivity,
allowing us to mi- model, uh,
different types of um, relationships.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 11.1 - Reasoning in Knowledge Graphs.txt
So, uh, welcome, everyone, to the class.
Uh, today, we are going to discuss about, uh,
reasoning in knowledge graphs, uh, using embeddings.
So, um, what we learned,
uh, last week was that, uh,
we discussed knowledge graphs, and in particular,
we defined the notion of a knowledge graph as a,
uh, set of, uh,
nodes and a set of relations between them where, uh,
nodes can have, uh,
different types and also, uh,
relationships, uh, can have different types.
Um, and then we have defined, uh, a task,
uh, we call the knowledge graph completion task.
We're basically given an enormous knowledge graph.
The question is, can we complete it?
Can we predict, infer, uh,
missing, uh, relationships, uh, in the graph?
So for a given head and,
uh, for a given relation,
uh, we wanted to predict missing tails, right?
So basically, for example, we would want to, uh,
predict the genre that a particular author is writing,
uh, books about based on the rest of the structure,
uh, of the knowledge graph.
So what do we are going to do today is we are going to take this idea of,
uh, knowledge graphs, um,
and knowledge graph completion,
but we are going to generalize it and extend it to, uh,
more, uh, interesting and a more,
uh, challenging setting where basically,
what we will be doing is we are going to perform multi-hop reasoning,
um, and logical reasoning in these knowledge graphs, uh, using embeddings.
So the goal is to reason in knowledge graphs and what-
by mean- by reason is to be able to answer, uh,
multi-hop queries or in some sense,
to make complex predictions in these, uh,
knowledge graphs for arbitrary predictive queries.
Um, and we are going to talk about, uh, two approaches.
One will be how do we handle what is called path queries,
and the other one will be about talking about
conjunctive queries and a method called, uh, Query2box.
Um, and in all these cases, basically,
the idea will be how do we embed the knowledge graph?
How do we embed the relations such that we can reason and navigate,
and move around, uh,
in this, uh, in this- in this space.
Uh, that's essentially the idea.
So, um, for the rest of the lecture,
I'm going to use this simple, um,
biomedical knowledge graph that includes,
uh, the following, uh- uh,
four types of entities.
If we will have nodes for, uh, different, uh,
drugs, we are going to have nodes,
uh, for different diseases,
we are going also to have some kind of adverse events or let's say- let's say,
side effects, uh, that these diseases can cause,
and then we'll also have nodes corresponding to proteins,
um, and, uh, that are- that are in,
uh, in our bodies and basically,
proteins that regulate and are part of various kinds of biological processes,
uh, taking, uh, part in our bodies and in our, uh, cells.
So now, let's say we have this kind of miniature, uh,
knowledge graph or a small part of the knowledge graph, uh,
that basically captures how these different entities,
uh, are related, uh,
to each other and then the question would be,
can we answer interesting questions,
uh, over this knowledge graph?
So, uh, to give you, uh,
an example- for example, the question is,
how could I answer complex queries or how would I be able to make
complex predictions over incomplete and massive, uh, knowledge graphs,
and just to give you, uh,
a few examples, um,
you know, you could say what,
uh, adverse event is caused, uh,
by a drug, uh, Ful- Fulvestrant, right?
And the way you could say this is to say, "Hi,
I want to start with the entity Fulvestrant and I want a traverse relationship cause,
and I want to predict what is on the other end of the- uh,
of this- uh, um, uh, Fulvestrant-caused relation."
Uh, and this is in some sense, a one-hop query.
It's basically a knowledge graph completion-type task.
I could then also do path queries where, for example, I could say,
what protein is associated with an adverse event caused by, uh, Fulvestrant, right?
So in this case, I would say,
I want to start with entity Fulvestrant,
I need to traverse over causes relation,
and then I need to cause, uh,
traverse over a associated relation.
Whatever entity is at the end of this,
that is the answer to my, uh, question,
to my query and then we can also ask more complex queries,
uh, where we can also have various logical relations between them.
For example, I could ask,
what is the drug that treats breast cancer,
uh, and, uh, causes headache?
So I could say, uh, you know, uh,
breast cancer- I start with an entity breast cancer.
I say, uh, treated by,
so this is now the drugs treated by
breast cancer and I also want to take a migraine and say,
what are the drugs that cause, uh, migraine,
and then whatever drug, uh,
causes both, um, that's the- uh,
that's the answer to my question.
So, you know, you can write these questions in natural language,
you can write them in this kind of,
um, uh, formula, but you can also write them in terms of the rec- graph structure, right?
So in- in a sense that one-hop query says start with
an entity and make a step along a relation,
path queries say start with- start with an entity,
make a step along one relation,
and then make a step a- along the other relation like causes
and associated, and what does this say is, it
says start with breast cancer and move across the
treated by, and start with migraine and move across,
uh, the relation caused by and whatever you end up- whatever entity you end up with,
uh, that's the answer, uh, to your question.
So you can see how we can basically take this- uh,
let's say, uh, questions,
queries, write them in natural language,
write them in the- let's say,
this kind of logical formula or write them in this type of,
uh, graphical structure, uh, form.
Uh, of course, today,
we are not going to address how do we get from natural language to the formula,
we are only going to kind of discuss once you have the formula,
once you have this logical structure,
this graphical structure, how do you answer, uh, a query?
And an important point over here will be that this-
this- we want to be able to answer these queries over incomplete knowledge graphs.
So it won't be only that we say,
oh, just traverse the relation.
It will be like, the relation is missing,
you have to predict it.
And it will- it can be chains of relations that are missing,
or are not in the knowledge graph,
and we still want to be answered- uh,
able to answer these questions.
So, um, you know,
first, let's talk about one-hop queries.
We already know how to, uh, uh,
answer them because we can formulate
graph completion problems as answering one-hop, uh, queries, right?
In a- in a knowledge graph completion, basically,
what we need to be able to estimate is to say,
is there a link between a head and a tail of relation-type, uh, r?
So this is essentially the same as saying,
is there- is there an answer to the query start
with the head and move along the relation, uh, r?
So, for example, what side effects,
uh, are caused by drug Fulvestrant, right?
In this case, I start with drug Fulvestrant.
This is my h. Uh, r is the- um,
caused by- uh, and now,
I want to basically say, is this particular entity the nth
point of Fulvestrant and, uh, caused by?
So that's, uh, how basically answering
one-hop queries like simple link prediction queries is,
uh, knowledge graph completion.
Now, we can talk about how about answering path queries, right?
We want to generalize this to basically being able to chain multiple relations,
uh, one after the other, right?
So we can say that we have an n-hop path query q that is represented by a- uh,
what is called an anchor node, uh, starting entity,
and then a sequence of relations,
r-1, uh, all the way to r-n, right?
So v is- uh,
v_a is an anchor node,
this is a fixed entity like, uh,
Fulvestrant, as we talked about,
and then answers, uh,
to the query will be denoted in this notation.
So basically, um, I have these kind of double braces so whatever is- uh,
whatever elements I write in here,
these are the answers,
uh, to the, uh,
query q on a knowledge graph G. So the way we could write this in
a graphical form into this kind- in something
we are going to call query plan is basically,
we start with the entity, uh, v_a,
and then we want to traverse our relationship type 1 and the relationship type 2,
all the way across the n relationships, and whatever entities are at the end of this,
um, we are making,
uh, uh, a prediction that this is the correct answer.
So that's a notion of a path query.
Um, to give you an example,
I could ask what proteins that are associated with
adverse ev- events caused by Fulvestrant, right?
So if I write this, then I say,
Fulvestrant is my anchor entity,
my r_1 and r_2 are relation causes and associated.
So my query will be started with Fulvestrant,
go across the causes relation,
and go across an associated relation, right?
So here is my, uh, Fulvestrant.
I want to first go over the green, uh-
The green links, uh,
saying, uh, uh, uh, you know,
what is- uh, what causes, this would be the re- the re- the red links, uh, apologies.
And then from here, I wanna then,
uh, traverse over, uh, uh,
the second part of li- link associated to arrive,
uh, at a given, uh, protein.
So that would basically be the idea for,
uh, path queries, right?
So to answer- to give you an example again,
so how do I ans- how do I find the answer now?
Basically the way I find the answer is that, conceptually,
if- if the knowledge graph is complete,
I simply need to, uh,
traverse it according to this query plan.
So the idea is I start with Fulvestrant.
I- I traverse over causes relations to get to all the side effects,
uh, caused by, uh, this drug,
like headaches and, you know,
brain bleeding, and shortness of breath,
and kidney infection, and so on,
so, kind of, quite serious side effects.
And now that I have this set of entities,
now I need to traverse from them according to associated relation.
And you know why I traverse the edges,
and whatever proteins I end up with,
those are the answers,
uh, to my query.
So, uh, these are the answers to the- to the query about,
you know, what are proteins associated with
adverse events caused by the drug, uh, Fulvestrant?
So that's, uh, basically the, um,
idea how now I can formulate and
answer path-based queries directly on the knowledge graph,
assuming knowledge graph contains,
uh, all the information.
Of course, um, in some sense, this seemed easy, right?
It's just answering queries, seems easy.
You just traverse the knowledge graph according to this plan.
Uh, but the problem is that knowledge graphs are notoriously incomplete, right?
Many relations between entities are missing or are, uh, incomplete.
Uh, for example, we lack a lot of biomedical knowledge, and, you know,
kind of enumerating all facts,
uh, testing all possible side effects,
all possible associations in these graphs,
is too costly and would take far too much time,
and perhaps we will never be able to obtain the entire knowledge, uh, about biomedicine.
So the question then is,
if these graphs are notoriously incomplete, um,
right, we cannot simply hope to traverse them,
um, to get the answers, right?
So the question then is, uh, what do we do?
Um, to give you an example, right,
if I'm missing, perhaps, uh,
a relationship causes by because it's not yet known
that Fulvestrant also causes shortness of breath,
then, for example, if I'm not able to traverse this edge, then, you know, uh,
the li- there is no way for me to discover that, uh, BIRC2,
the- uh, the protein,
is actually an answer to my query, right?
Uh, I would be able to say that the CASP8 is, uh,
is an answer because I go over,
uh, brain leading to get to it.
I know that, uh, PIM1 is also,
uh, an answer because of this path through the kidney infection,
but there is no way for me to find out that this, uh,
BIRC2 protein is also the answer, uh, to my query.
So that's the problem.
So what you could do is to say,
uh, I, actually, you know,
listened previous lecture and I remember we talked about knowledge graph completion.
So let's just go and apply, uh,
knowledge graph completion so that now the graph will be complete,
and then we can simply, uh, traverse it.
The problem is that,
if you do this, then, um,
you don't know how many edges are really missing in the knowledge graph.
So you could basically go and apply your knowledge graph completion,
uh, approach to all possible,
uh, edges of the graph.
So it means that you would wanna take, for,
uh, every triple i- in the knowledge graph,
you'd wanna s- ask your pre- uh,
knowledge comple- knowledge graph completion engine to
assign a probability that that link, um, is true.
Um, the problem then will be that, now,
your graph just got a whole lot of new- this kind of probabilistic links,
and now the graph is getting denser and knowledge graph, uh,
completion or traversal, uh,
will get very expensive because,
if you think about, you know,
starting at an entity, alo- uh,
moving along a given relation,
and then, uh, getting to a set of nodes,
moving to the next relation,
uh, to a new set of nodes,
this will explode exponentially with the length of the query, right?
So with the query length is, let's say,
capital L, then if,
at every step, you have some,
um, uh, non-trivial expansion,
some non- non-trivial number of- um,
of, uh, uh, nodes that are as- that, uh,
al- that allow you to traverse to- uh,
along a given relation,
then this is going to, uh,
increase exponentially in the query length.
And, uh, having big queries,
this becomes computationally quite hard to do,
uh, over, uh, a massive, uh, knowledge graph,
especially if you think about that we have just added
a lot of new edges with different probabilities, uh,
and we nin- we need to keep track over these probabilistic imputing edges as well,
this becomes, uh, a very hard, uh, computational task.
So, uh, here is the idea how we are going to do this and formulate it.
And the way we are going to talk about this is we
are going to call it predictive queries.
And the idea is, right,
we need a way to answer
path-based- path-based queries over an incomplete, uh, knowledge graph.
Um, and we want our approach to implicitly impute
and account for the incompleteness of the knowledge graph.
So we don't want to impute the edges and then traverse.
We just like the method to take care of all this automatically.
So rather than thinking that all we have to do is just traverse the knowledge graph,
we really can think of this as making very complex predictions, right?
It's basically saying predict what entities are a- uh,
an answer to a given query.
So basically we are going to formulate these queries as predictions.
So this will now be our prediction task.
Um, and this means it will be very interesting
because we'll be able to answer any kind of query.
In so far, we talked about path queries,
we are able to answer any kind of path query.
Whatever set of relations you put on the path,
we should be able to answer it.
So it doesn't mean, at the training time,
we have to see the query,
then we train how to answer the query,
and now we are able to give the answers.
The idea is that we can obtain- we can get any query at the,
uh, test time, and we should still be able to answer it.
Uh, and then, you know, uh,
another important thing is that this method needs to be able to impute- to
implicity impute or implicitly complete the knowledge graph for the missing information,
and be, this way, robust to
the missing information or to the nois- in the knowledge graph.
And, really, another way how you can think of
this predictive queries task is that it's
a generalization of the link prediction task, right?
Uh, link prediction is simply starting with the head, uh,
moving along a relation of type R,
what is the tail?
And now we'd like to, uh,
generalize this one-step prediction task into multistep, uh, prediction task.
So this is very important.
We are really taking- reformulating this,
um, task of, uh,
answering queries as traversal over to the knowledge graph
into predicting what entities are the answer,
uh, to the query.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 11.2 - Answering Predictive Queries.txt
So, how are we going to predict, uh,
the answers to a given query on a knowledge graph?
Uh, this is what we are going to discuss next.
And the key idea we wanna do here is we wanna take advantage of
the structure of the underlying- this embedding space.
And the idea is that we wanna embed queries.
Um, and the way you think of this is that really,
this will be a generalization of the TransE method that we talked about,
uh, last time, right?
Last time, we said that the TransE knowledge graph completion method,
the idea is you start with the head,
um- you wanna go from head to tail and way you go from head
to tail is to learn this vector r that moves you from h,
uh, to t. So you say,
what is the scoring function, right?
What is, kind of the- the likelihood, the, you know,
some kind of uh, probability if you like,
that- that, uh, he- uh, that, uh,
if you start with the he- that head and tail are related with the relationship r,
then the way you've write the scoring function is- is
head plus the vector rminus the point,
uh, t, and now,
this is the distance.
Essentially what this is saying is,
what is the distance between h and r,
um, and the point, uh, t?
So another way to interpret this,
uh, TransE is to say,
I will embed the query.
The query is hat plus relation.
And the goal now is that embedding of the query is
close to the entity t that is the answer to the query, right?
We wanna say the- the distance between the embedding of the query and
the entity t is simply the distance between
these two points where another way to think of it is,
I'll start with the head,
that's my anchor node.
I'm going to traverse over this learned vector r that will,
you know, lead me to some point.
This is now the embedding of my query.
And now, my goal is to minimize the distance between the,
um, embedding of the query q and the, uh,
uh, the point t. This is the embedding of the entity that is answer, uh, to this query.
So for example, if I start I know with an entity called
Barack Obama and I wanna move along this vector that corresponds to the,
um, relationship called nationality,
I add h plus, uh,
vector for nationality, I end up at this point q.
And my goal now is that, of course,
there would be some other entities, uh, uh,
embedded, uh, in this space.
I want the distance between the q and American to be small.
Uh, and I want it in some sense, and I want,
let's say the distance between q and New York, uh,
to be larger because the answer to nationality of Obama is- is,
let's say American and it's not,
uh, New Yorker, uh, right?
So that's essentially the idea, right?
So once I have embedded the query,
uh, in this case, embedding the query is very easy.
I start with the anchor entity and add,
uh, uh, relation to it, a vector to it.
Um, then I want the onset entity,
the entity t, to be close- embedded very close to the embedding of the query.
That's the idea of the TransE.
So what we can do now, uh, with TransE,
we can generalize it to multi-hop reasoning, right?
So if I say, uh, starting with the query,
er, defining the query q as, uh, er,
anchor entity and a set of relations,
then it's intuitive how I can generalize this using TransE.
I can start with my embedding of my entity v and then I can simply move,
uh, along the vector of, uh, relation 1,
uh, move along another vector of- for relation 2 and I
chain these vectors together to arrive at some point, uh, in space.
And I will call this point q because this is now
an embedding of this query that started with,
uh, uh, entity and then traversed,
uh, uh, a set of relations.
Right? And why is this elegant is because now,
creating this embedding for the, uh, for the,
ah, query q simply involves a couple of vector additions, right?
I start with a point and then I add,
uh, these vectors, uh, to it.
And now that I have identified the embedding of the- of the q,
I only look at what are the entities embedded close to these points.
Those are my answers that are- basically,
those are my predicted answers, right?
It's a, uh, now I formulate this as a prediction task where I say,
whatever is close to the point q, that's my answer.
Um, that's essentially, uh,
the idea about, um, ah,
how do we now generalize, uh,
TransE to this kind of uh, multi-hop, uh, queries.
To give you an idea, right?
I could say what proteins are associated with adverse events,
uh, events caused by, uh, Fulvestrant?
Then basically, I have my, uh, Fulvestrant.
I have my, uh, embedding space.
Here's, you know, hypothetically,
the- the embedding of the drug, a Fulvestrant,
then I would traverse across, uh,
across a vector that I have learned for the- co- to- for the causal relations- relation.
And hopefully, you know,
the goal is that all these side effects that are caused by
Fulvestrant are embedded close to this, uh, point here.
And now that I'm at this point,
I wanna add the traverse along the relationship associated with,
so I would add the associated with,
uh, relation to it.
Perhaps, you know, this vec- learning vector is like this.
So this is now the embedding of my query and the goal
is that the proteins that are the answer to this predictive query,
uh, they are embedded close to this point,
uh, q, and these would be my answers.
So it simply just asking embed the query and find the entities that are close,
uh- closest to the embedding,
uh, of the query.
So that's the, uh, idea for answering, uh,
multi-path queries, uh, using,
ah, a, uh, extension of TransE.
So what are some,
uh, things to discuss?
Uh, what are some insights?
First is that we can train TransE to optimize the knowledge graph,
uh, completion objective, right?
Uh, basically meaning we can, uh, uh, basically,
learn TransE by learning the entity embeddings as well as the vectors are.
Um, and because TransE can naturally ha- handle composition relations, right?
Last- last, uh, week,
we were discussing about, uh,
different types of, um,
properties of these different graph completion methods.
And we talked about the TransE has
this compositional property where you can- you can, um,
uh, chain multiple, uh,
predictions or multiple relations,
uh, one- one after the other.
And this really allows TransE to be- to,
uh- to be able to answer path bath- based queries.
Uh, for example, if you look at TransR, DistMult, or ComplEx,
they are not able to be used in this kind of
path settings because they cannot handle composition, uh, relations.
So it means they cannot- they are not suitable for answering, uh, path queries.
So the key here was this insight, but basically,
we learn how to move around,
uh, in the- in the embedding space.
And the goal is to se- to embed the entities as well as learn these,
uh, uh, vectors that allow us to kind of strategically move around, uh, given the,
uh- given the query to embed the, uh,
the query and then the entities that are closest to the embedding of the query,
those are, uh, our answers.
So this was now about answering path queries.
The question then becomes,
can we answer more complex, uh, queries,
for example, that also include some logical operator like a conjunction?
A conjunction is an and.
So for example, maybe I get a query that is,
what are drugs that cause the shortness of breath and treat
diseases associated with protein- protein, uh, ESR2?
So the way I think of this is that ESR2 and
shortness of breath are my, uh, anchor entities.
From ESR, I wanna go to as, ah,
across associated with and then TreatedBy.
Uh, from shortness of breath,
I wanna say CausedBy and whatever are the drugs that do both,
uh, I need to take the intersection here.
And those are, uh, the entities that are predicted to be the answer,
uh, to my query, right?
So these are- what- drugs that cause shortness of
breath and treat diseases associated with my,
uh, protein of interest, uh, ESR2.
Um, if you are, let's say a drug development,
a medic- medicinal chemist,
this is a real-world query,
you would like to ask over your, uh, knowledge graph.
So, um, how would we answer,
uh, uh, this type of query?
Let's say if the knowledge graph is complete and we can do knowledge graph traversal,
the way we would do is we start with the first,
uh, anchor entity, uh, ESR2.
We traverse across the associated,
uh, uh, uh, with, uh,
relationship to get the- to the diseases associated with this, um, protein.
Uh, from here, we then move to, uh,
what are the drugs that are, uh,
that treat, uh, these diseases.
Again, we- we move, uh,
we now traverse across the TreatedBy relationship,
uh, to arrive to the set of drugs.
And then we also have to start with a shortness of
breath entity and move across, uh, CausedBy.
These are now the, ah,
drugs that, uh, cause shortness of breath.
And now, we basically have to take an intersection between
this set of entities and that set of entities.
And whatever is- is in the intersection,
that is the answer to our question.
So in- our, um,
the answer to our question would be, uh,
Fulvestrant and, uh, uh,
Paclitaxel, um, uh, drug, right?
So the point is, um, that we have now, uh,
two entities that are answer to our query,
if we think of it as a knowledge graph,
uh, uh, traversal, uh, type task.
And of course, similarly to what I was saying before,
is a given- if some of the links on the path are missing,
which is usually the case,
then the- then a given entity would not be,
uh, will not be able to predict or identify that it is the answer to our query.
So for example, if, uh, we don't know that,
uh, ESR2 is associated with breast cancer,
then the- then there is no way for us to discover
that Fulvestrant is actually the answer,
uh, to our question.
So, uh, again, if the knowledge graphs are incomplete,
knowledge graph traversal, um, won't work.
So the question then becomes,
how can we use embeddings to, uh,
in some sense, implicitly impute these missing relations, um,
and also, uh, how would we even be able to figure out that, you know,
in this case, uh, you know,
that there should be a link between ESR2 and breast cancer?
And the hope is, right,
that our method who will take a look
at the entire knowledge graph will see that basically, uh,
ESR2 here is also associated with, um, uh,
ESR1 and, uh, uh, BRCA1, right?
And we see that there are kind of these strong relations here.
So what this would allow us to do is kind of be able to implicitly impute and say if
breast cancer is associated with these two proteins
who are strongly associated with this third protein,
perhaps there is a missing relationship here, right?
That's kind of what our algorithm,
uh, needs to be able, uh,
to do, uh, implicitly,
uh, through, uh, node embeddings, right?
The- the hope is that other contexts and other relationships in
the graph will allow us to do this, uh, implicitly.
So, uh, going back to our question about how would we now implement logical, uh,
relations like intersection, uh,
an and operator in this, uh,
setting where we wanna answer more complex queries in a predictive way?
The question then becomes,
how do we- how do we in- how do we do this in the embedding space?
And the first insight is that when we have this query plan that I showed it here,
then this in- these, uh, starting nodes,
these anchor nodes, they are single entities.
But if you think about what are these gray nodes,
what do they represent in my query plan?
They actually represent a set of entities, right?
So they represent, let's say, all the, um,
all the diseases that are associated with ESR2
or they represent all the drugs that are caused by,
uh, that cause, uh, shortness of breath.
So the question then becomes how do we repre- do this,
uh, representation of entities in the embedding space because now these are sets?
And how do we then define intersection operators in this, uh, latent space?

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 11.3 - Query2box Reasoning over KGs.txt
So what I want to talk about next is a solution,
uh, to- to these questions.
And in- in particular,
the method we'll talk about is called Query2box,
that allows us to reason over knowledge graphs using what is called, uh, Box Embeddings.
So let me first,, uh,
motivate the setting, and then give you how this method works.
So we said we wanna answer more complex predictive queries on, uh, graphs.
And let's say that we wanna be able to also include conjunctions,
so not just sets, but also intersections.
We wanna be able to take an intersection of two sets in the embedding space.
What we need to be able to do is two things.
We need to be able to now figure out how are we going to
represent a set of entities in the embedding space?
And then, how are we going to define the intersection operator in the embedding space?
How do we quickly take, uh,
two sets of entities and say,
what are the entities that are in the intersection, uh, between them?
So let's see how to do that.
The key inside that, uh,
will allow us to do this is the concept of a box eEmbedding.
What this means is that we're going to embed all the entities,
and the relations in our space as boxes,
as, uh, multidimen- dimensional, uh, rectangles.
And the intuition would be that every box is
defined by two points where it's defined by the center,
and is defined by the corner.
We'll call the center,
center, and we'll call the corner, offset.
So this basically tells me, you know,
what is the offset in, uh, one dimension?
What is the offset in the other dimension?
This basically tells me what is the size of the box.
And intuitively, we want to,
uh, let say, in our case of Fulvestrant,
we wanna embed all of its side effects in a- in a box, uh, that is,
uh- so that in basically, uh,
all the side effects caused by Fulvestrant will be- will be embedded close together,
and they will be enclosed in this,
uh, in this box.
So that's the- that's the idea.
So let me now tell you,
uh, keep kind of building on this.
So, um, the reason why we wanna embed things as
boxes is because intersection of boxes is well-defined.
Right? Basically, what I mean by that is,
uh, if you take two boxes,
and take an intersection,
intersection of two boxes is a box.
It can be an empty box,
right, a box of size 0.
But if there is any non-trivial overlap,
that overlap will be a box.
So it means we can now think about how are we going to define
trans-geometric transformations over these boxes
that corresponds to logical, uh, operators.
Right? So when we traverse the knowledge graph to find answers,
each step will basically, uh,
produce a set of reachable entities,
a set of entities that the query covers, uh, so far.
Um, and we are going to now model this set of
entities with a box that encloses all of them. All right.
So, um, this means that we can,
uh- that box provide a powerful obstruction because boxes kind of enclose,
uh, sets of, uh, entities.
And then we can define geometric intersection operator,
uh, over, uh, let's say,
two, or multiple boxes,
and when- when we intersect them,
the intersection is still a box.
So we- it's very easy again to have this compositional property,
where we can take boxes,
intersect them in arbitrary ways,
but, uh, we always are left of- are left out with a box,
which we can further kind of manipulate,
and change in shape however we like.
So that's why this is kind of cool,
um, and, uh, exciting.
So, uh, what does it mean to be a- to be able to embed with boxes?
Here is what we need to figure out or here's what we need to learn.
We need to learn uh,
opa- uh, we need to learn entity embeddings.
So every embedding, entity will have an embedding,
and it will be a box,
but it will be a- a- a trivial box.
So basically, it will be a point, right?
It will be a box of a- a size 0, or volume 0.
Then we'll have relation embeddings,
which will basically, um,
generalize the notion of params,
where we are going to take- now learn how to move around,
and, er, expand, or shrink boxes.
So the relation embeddings will be like vectors,
that are going to take boxes,
and move them around.
Then intersection operator is something that is- that is new.
Uh, the intersection operator will take, uh,
multiple boxes on the input,
and will output a single box.
And intuitively, this will be an intersection of, uh, boxes.
But we are going to learn this, uh, intersection operator.
Um, so it'll have some parameters so that our,
uh, approach will be even more,
uh, expressive, and more robust.
So how will this work?
Let me just kind of give you a cartoon.
Again, we are talking about, uh, you know,
what are drugs that cause shortness of breath,
and treat diseases associated with protein, uh, ESR2?
Uh, the way we do this,
right, in terms of query plan,
we say a-ha, we start with a anchor entity, ESR2.
We wanna move across the, um,
across the relationship called associated, uh, with.
And the way we are going to do this is that we are going to define
this relation projection operator P. Uh,
here is how it's defined.
We'll have one for every relation.
And essentially what this will do is it will take one box,
and it will kind of move it according to our given relation,
and also, um, expand its,
uh, or shrink its size.
So basically, we will have, um,
this learned, uh, ve- this learned operator for every relation.
It's going to move the center,
and it's going to change the offset.
So basically, it will change the size as well as the location,
uh, of the box.
So I can start with, uh, q,
and then I apply this, uh,
projection r to it to get a new box, uh, q prime.
So that's the projection operator.
So how would this look in our case if I started the ESR2,
and I wanna move across the associated relation?
I would take this trivial box, apply my, uh,
r- r- r associated,
and basically, this would move it, uh,
and expand it to include, uh,
all the- all the diseases associated with, uh, ESR2.
Now, um- now that I have a new box,
I can again, simply kind of follow the query plan.
Now I wanna traverse over a TreatedBy,
which again, I would take this box, apply TreatedBy, uh,
projection operator, the transformation to the box, which would, you know,
further move it, and you know,
expand or shrink it,
depends what the model we learn.
Uh, and then the idea is that whatever the answers to this path to a predictive query,
those are the entities, uh,
inside, uh, the box.
And that's essentially the idea, right?
Now, I can also try to answer the second part of the query,
which is shortness of breath CausedBy.
So I start with the shortness of breath entity,
apply this CausedBy box transformation to get another box.
And now, uh, what are the entities that are,
uh, in the intersection of the two boxes?
Uh, those will be basically the answers to my query.
In our, uh- in our case, uh,
this would be these, uh- these two entities,
uh, fulvestrant, uh, and the other drug.
Right? So that's essentially the idea.
I have one box, I have the other box I'm interested
in, what is the intersection, uh, of them?
So that's, um, the idea about how would I do the embedding grid boxes?
Now, let me talk about how does- how is this intersection operator, uh, really defined?
Uh, one approach would be to simply define it as kind of hardcore,
or just mathematically as intersection of boxes,
almost like as- as in a Venn diagram.
Uh, we wanna be a bit more, um, flexible.
And we wanna do, uh, uh,
and learn a geometric intersection operator,
we'll call it J.
Where the idea is we wanna take multiple boxes as input,
and produce the intersection,
uh, of these boxes.
We'll call it the intersection box.
Uh, and the intuition,
we are going to use this to say the center of
the new box should be close to the centers of the input boxes.
So these are the three input boxes and the centers.
Um, so we would like to take kind of intuitively the intersection between them.
So we want the new center to be kind of a function of these three centers,
and we want the intersection to be kind of close to- to them.
And then the offset,
so the size of the box,
should shrink, since, uh,
the size of the intersection is smaller than the size of,
uh, any of the input sets,
or any of the input boxes.
So formally, we'll have
this intersection operator that will take an arbitrary set of boxes,
and produce a new box.
Now the way we define this operator is we need to say what is happening to the center,
and what is happening to the offset of the intersection.
The way we are going to find the center of the intersection is,
we are going to do this through a learned oper- operation.
Our learning operation will take the following,
it will take the centers of the boxes that are on
the input and then we are going to learn this function, f,
that takes a center of the box and, um,
and transforms it and then we will apply softmax to this, uh, to dysfunctions.
What this will allow us to do is that,
basically the - the intuition is that the center of the intersection would be
somewhere in this green region defined by the centers of the input boxes.
We define the center as a weighted sum of the input box centers,
where W is this weight,
the importance of our- of our given box on the intersection of boxes.
So, W represents, in some sense,
a self attention score on the cen- how much a center of
each input box affects the center of the intersection box.
Uh, and this funny operator is called Hadamard product,
which is basically element-wise product, right?
It's just you - you uh - uh,
take product of cells uh,
that correspond, uh, to each other.
So this is how center uh,
is defined and of course,
f is a function that we are going to learn.
Similarly now, we also have to define the offset.
And the way we are going to define the offset of the intersection is to
take- is we'll take the min- smallest of all the input boxes,
so we take the minimum in terms of the offsets.
And then we are going to also learn this offset function,
f offset, that is going to aggregate and transform the offsets.
So here, the - the intuition is that if we first take
the minimum of the offsets of input boxes and then make the model more
expressive by introducing- introducing this new function f_off for extracting
the representation of the input boxes with the sigmoid function to guarantee shrinking,
uh, of the- of the offset.
Again this f_off will have some trainable parameters uh,
that will uh, that we are going to learn
through the training process uh, of the entire model.
So, uh, you know, intuitively,
the idea is that once I have two boxes on the input,
I wanna- I wanna produce the intersection of the two boxes and in our case,
the intersection of the two boxes, for example,
would be this, uh, shaded, uh, area here.
Uh, and that- and whatever entities are enclosed or are close to this box,
uh, those are the answers,
uh, to our, uh, query.
So now, what we need to do is,
so far I said, oh, um,
entities that are the answer that are included in the box,
they have to be kind of inside the box, right?
In reality, this might be uh, harder,
because, uh, data is noisy, um, uh,
there might be inconsistencies,
contradictions in the knowledge graph,
so we need to define the notion of distance between the box and a point.
And the idea will be that, uh, we are going to define this distance to have two parts.
We're going to define the distance from the center to the point,
if the point is inside the box,
and if it's outside the box,
then we'll take the distance between the center and the corner and the corner,
uh, and the point.
Um, and the point here is that we wanna be able to uh,
account for the distance differently.
We wanna measure distance, uh, uh,
from the center if, uh,
when the- when the point is inside the box in some set of units.
And then we are going to have this, uh,
weight scalar Alpha that will, uh, uh,
be less than one so that basically we- we are going to
penalize the- the- the distance from the center of the box,
uh, more if the point is, uh,
inside than if it is,
uh, outside the box.
Uh, and the intuition is that if the point is enclosing the box,
it should- the box should be, uh,
it should be very close, uh,
to the center of the box so that we get more robustness from our- from our approach.
We don't want the point to be kind of close to the border or the edge of the box,
we want it towards the center of the box.
So that's how we now define the- the distance between the entity and the box.
So now when we have the final embedding of the query,
all we do is basically apply this distance, um,
and say who are the points that are closest according to this distance,
uh, to the center,
uh, of the box.
So now we have discussed two things.
We discussed how we can embed everything, uh,
all the entities as boxes,
how we learn the- the box transformation operation,
the projection operation that is one per relation,
and then we talked about how we learn
these intersection conjunction operator that takes the- as an input,
a set of boxes,
and produces the intersection box.
Of course, our natural question next is,
could I- could I have these predictive queries,
uh, even with a union operator?
So, for example, to be able to answer queries like OR.
Um, and so next what we are going to look at is,
how would you be able to answer what we will call AND-OR queries?
Or what is technically called existential positive first-order queries.
So basically, these are queries with conjunction and disjunction operator, right?
So the question is, how would we be able to dis- are we able to
design a disjunction operator and embed
AND-OR queries in low dimensional vector space and be able to
answer predictive AND-OR queries in the,
uh, space as well.
So the question is,
can we embed AND- AND-OR queries in a low-dimensional space?
The answer is actually no.
The reason for that is that allowing union over
arbitrary sets requires high dimensional embeddings.
So let me quickly give you a sense why we need a lot of dimensions.
Imagine I'm given three queries, q1, q2,
q3, and each one of them has one answer.
Uh, you know, q1 has v1 as an entity,
v1 has answer q2,
v2, and q3,
v3 entity has the answer.
So now, imagine I- I allow an entity operation, right,
which basically means I can take, uh,
any pair of queries and do an OR between them.
So the answer set will be, uh,
the answer set for the first query union the answer set,
uh, of the second query.
So given that we have two queries,
sorry, given that we have three queries, the question is,
uh, can- would we be able to model union operation when we embed them,
let's say on a two-dimensional plane?
And yes, you could, right?
Basically, you say for every individual query,
I can simply put a box around the entity and this would mean, uh,
for a given query,
I want the entity that is the answer to it to be inside the box and that's,
uh, you know, easy to achieve.
We can space these entities apart and put a box around each one of them and say,
uh-huh, the box is the embedding of the query and
the answer to the query is embedded inside the box.
You know, all good.
How about the unions?
Um, in this case,
I can easily, uh, do the union.
This is, for example, how a box of union 1,
query 1, union query 2 would look like so, uh,
q1 or q2, this is how, uh,
q2 or q3 would look like,
and this is how uh,
q1, uh, or q3, uh, would look like.
So it seems it all works for three queries in two dimensions. However-
What if we have four points?
So imagine now I have four queries.
I have four queries each one with one entity as the answer.
Can I now use a two-dimensional space to embed them,
um, and be able to do arbitrary unions between arbitrary queries?
Uh, and it turns out I cannot design a box embedding such that, for example,
in this case, that would represent the union of q, q_2, or q_4.
Because there will always be some other kind of box or some other entity,
um, uh, in- in between them, right?
And if I move this entity away, perhaps here,
then I won't be able to- to- to model the union between v_1 and v_4.
So basically, um, there will always be, um, some issue.
And, uh, what is the conclusion, or if you,
um, if you do the, uh, mathematics carefully,
what turns out is that if you have M, uh,
queries or M entities with non-overlapping answers um,
then you need the dimensionality of order M to handle arbitrary OR queries, right?
So in our case, we said if I have three queries, I need two-dimensions,
if I have four queries I need three-dimensions,
and so on and so forth.
So because real- real world knowledge graph have a lot of entities, for example,
you know, FB15k has 15,000 entities and it's considered a small knowledge graph.
This would mean that if we want to be able to- to uh,
model arbitrary OR operation between arbitrary sets of entities,
this would mean we need an embedding dimension of 15,000.
And uh, that's far uh,
far too much for us to be able to do this.
So, um, it seems, um, hopeless, right?
Since we cannot embed AND-OR queries in low-dimensional space can we still handle them?
And actually, the answer is yes.
And the answer is yes because we can rewrite the query to, um, in uh,
into a different form so that the query is kind of logically equivalent,
but the union operation happens all at the end, right?
So the idea is if I have the original query plan, where I,
you know take two projections,
take the union, take one more projection,
and take- then take an intersection,
I can rewrite this into a different query plan that gives me the identical answer,
it's logically equivalent, but the union only happens at the end.
So why is this beneficial?
Because I already know how to answer this part to get a box.
I know how to answer this- this part up to here to get a box.
So now, uh, write these the answer to my query?
Is simply entities included in the first box and entities included in the second box.
So that's very easy to do because the union occurs,
uh, at, uh, always as- as a last step.
So what is important is any AND-OR query can be transformed into uh,
an equivalent what is called, um,
disjunctive normal form, which is a disjunction of conjunctive queries.
Um, and this means that if I have a query that's now a disjunction of conjunctions,
I can simply first answer these conjunctions,
and then take the OR or the union at the end.
So it means, um,
we can aggregate these unions uh, at the last step.
That is one caveat is that when you do this transformation into disjunctive normal form,
so from the original query to the disjunctive normal form,
the size of the query can increase exponentially.
Um, but in our case,
this is not the problem because answering queries um,
is so uh, is so fast,
it's just kind of uh,
navigating the embedding space.
So this doesn't, uh, doesn't sound like too big,
uh, of a problem.
So now that we have um,
defined the uh, the disjunctive normal form and we have rewritten the uh,
the query as a set of conjunctive queries and then apply the OR operation at the end,
then all we need to do is to change the notion of the distance between the query,
the embedding of the query um, and the entity.
And the way we can write this out mathematically is very intuitive.
We say that um, the query in the disjunctive normal forum,
conjunctive query q_1 or q_2 or q_3,
we can simply define the distance between the embedding of
this query and the entity to be- to be the minimum of
the distances between the- the box distances
between the individual conjunctive queries and that entity.
So basically we say the entity is an answer uh,
to the query if it is inside at
least one of the individual conjunctive query boxes, right?
So that it is close to at least one of the,
uh, conjunctive queries q.
So that's why we have the minimum here, right?
So as long as v is the answer to the one conjunction,
query q then the distance between that query and v will be uh, very small.
And we achieve this by using this minimum uh, based uh, distance.
So, um, this now, uh, er, uh,
allows us basically to- to answer uh,
arbitrary AND-OR predictive queries.
So the way we would go about this,
if- as we are given a query q at the beginning,
we would rewrite it into a disjunctive normal form.
So a, uh, disjunction of conjunctive queries q_1 to q_m.
We would uh, embed this q_1 to q_m using this set of box operations,
meaning uh, projections and uh, intersections.
And then we would simply calculate the box distance between each q_i uh,
from the disjunctive normal form and entity v and uh, take the minimum of these distances.
And then the final score of an entity is simply the dista- is
this minimum base distance between the entire uh, query uh, and uh,
the- and the embedding of a given entity uh,
for which we are interested in assessing how likely is that the entity an answer uh,
to our uh, to our query.
So now we have talked about how the method
works from the point of view of once the embeddings are there,
once the box operators are there, uh,
how do we apply it to answer an arbitrary predictive query?
The question is, uh what kind of training procedure do we use to learn this all?
What does this mean is,
we need basically a simil- a- a setup similar to knowledge graph
completion task so that we can learn all these parameters, right?
Um, when I say parameters,
I mean we need to learn entity embeddings,
we need to learn these relation embeddings,
these box transformations, and we need to learn the intersection operator.
And the way we are going to learn this is basically that we are going to select uh,
or sample or create a set of queries.
For every query, we are going to have a positive set of entities.
This will be our set of answers.
We are going to have a negative set of entities,
this will be non-answers.
And, uh, our goal will be to- to learn all these uh,
parameters in such a way that answers are included
in the final box and non-answers are uh, outside the box.
So let me, uh, uh, summarize this.
So we are going to first randomly sample query q from
the training knowledge graph and
identify an- an entity v that is the answer uh, to that query.
Then we are also going to identify some negative entity,
let's call it v prime,
that is not an answer to the query, right?
So for example, I could say,
uh, I start uh, with uh,
let's go back to our uh,
Barack Obama nationality American.
So, uh, I would say- I would sample a query um, uh, nationality.
This is a simple kind of knowledge graph completion query,
not even a multi- multi-hop.
Uh, I start with Obama,
along with the nationality,
I find American, so that's a positive entity.
And then perhaps I say a negative entity could be some other random uh,
entity in uh, in the graph.
Um, perhaps it could be Paris, uh,
or if I want to be smarter I say I have the true entities
of type Americans, so United States.
So I want to pick another country, perhaps Germany,
as a negative example,
as a non-answer uh, to this query.
Now that um, I,
now that I have sampled the query and the answer and non-answer,
I embed the query using the transformations and then I calculate the score function f uh,
of entity v, which is the answer entity,
and the entity v prime,
which is the non-answer uh, entity.
And then the goal is that now I can compute the gradients, uh,
with respect, uh, to this, uh,
loss function l where the goal is to maximize,
uh, the- the score of the entity v. So basically, uh,
it means maximize the negative distance,
which means minimize the distance,
while minimizing, uh, the distance of the non-answer.
So the lo- the way we write the loss is, we say, uh, s- uh, uh,
f- uh, Sigma is a sigmoid function,
uh, that has value,
you know, between 0 and 1,
so basically, I'm saying I want,
uh, to, uh, maximize, uh,
f for, uh, the answer,
and I wanna minimize,
uh, f for the non-answer.
And now I can take gradient or derivative with
respect to this loss function to then be able to update
both the entity embeddings as well as the projection and
intersection operator parameters in order to be able to learn this, uh, well.
Of course, we are not only to  going to sample one-hop queries,
we are also going to sample two-hop queries,
three-hop queries, intersection type queries, uh,
all kinds of different, uh, query structures,
and that will be, uh,
our, uh, training set.
And then given these queries,
given an answer entity,
the- and non-answer entity,
we are going to, uh, optimize this likelihood,
meaning we are going to find parameters of the model
which is embeddings of entities plus these,
uh, operators so that all these, uh,
will, uh, optimize, uh, the loss function.
So how do we, uh,
how do we instantiate, uh,
a query, uh, in the knowledge graph?
We will assume we are given a query template, and then we, uh,
in- instantiate the query, uh,
will simply be to do the backward walking.
Uh, what I mean by that is we are, uh,
basically start from the initial answer entity,
and then we are kind of going to walk backwards towards the anchored entities, right?
So, um, in our case,
for example, we could say,
let's take fulvestrant as our, uh, answer entity.
So now we are going- now that we have kind of
randomly picked this entity from the knowledge graph,
we're going to kind of move backward, uh,
along the knowledge graph to then get the- the query plan,
the query structure, right?
So we are just at fulvestrant,
then we, um, uh, move,
we say we're- we will need to take intersection of two relations,
relation 1 and relation 2,
so let's pick two relations from,
uh, fulvestrant and move backward.
And then now that we are here,
we moved across TreatedBy.
Uh, now, we can again move one step, uh,
backward along the projection operator, um,
for the second type of relation like, uh, associated.
And, uh, now this will now become my anchor entity.
And now, uh, I do the same thing for the second relation,
uh, here CausedBy and find the second, uh, anchor entity.
So now I have just sampled the query that goes from ESR2 and shortness of breath,
through associated TreatedBy, as well as from shortness of breath to CausedBy,
and the intersection of them is the fulvestrant.
So this is now my sampling, uh, of the query.
And then what I can do is, uh,
find some entity that is not answer to this query and use that as a,
uh, non, uh, answer, right?
Uh, it is important when we do this sampling to be careful, uh,
because the query q must have answers,
uh, on the KG.
Um, and we will take one of the answers as instantiated, uh,
uh, answer for our, uh,
positive example, and, um,
we're then going to basically randomly sample some negative,
uh, examples, non-answers, which are basically other entities, uh,
in the knowledge graph, but we have to be careful that we are not, uh,
by accident sampling some other entity that's also,
uh, answer, uh, to the query.
So that's, uh, essentially the idea.
Um, and the- the- the impressive thing is that we can actually make this,
uh, all work and that now we are able to answer arbitrary, uh,
predictive queries in knowledge graphs, uh,
even though they might be notoriously, uh,
incomplete or, uh, may not have,
uh, uh, all the edges.
And what I wanna do to, uh,
finish the lecture is give you a quick,
um, example of how would this look like.
So, for example, here we are going to take the Freebase 15,000, uh,
knowledge graph, which is a knowledge graph about, uh,
real-world entities, uh, people,
uh, things like that.
And imagine we wanna have a query, uh, you know,
who are all male instrumentalists who play string instruments, right?
Uh, and we are going to, uh,
use our system to learn the embedding of, uh,
all the nodes, uh, all the entities to learn the projection operator,
to learn the conjunction operator.
Um, and then we are going to take this, uh,
high-dimensional embedding and we are going to project it
down to just two dimensions so we can visualize it, right?
So, um, here's the visualization, right?
So here is at, uh,
15,000, uh, different dots,
one corresponding to the embedding of, uh, each entity,
and these are real entities with real embeddings,
and here is our, uh, query plan, right?
We, uh, we wanna answer, uh,
you know, who are male instrumentalists who play string instruments,
so we'll start with string instruments as the anchor entity, go over, um,
the instance of relation to find all st- instances of string instruments,
then we are going to go over,
uh, PlayedBy, uh, relation, uh,
to identify all the- all the people who
play these string instruments, any of them, right?
And then we are going to go from node male to say
instance of to find all the instances of male, uh,
males in our dataset,
and now we need to take the intersection to find all the males
who are also playing any of the string, uh, instruments.
So let me show you how this, uh,
actually works in this, uh, two-dimensional projection.
So, uh, the anchor node for string instrument,
if you would, uh, locate it,
is actually here, uh, the blue one.
Now I will take and apply a box transformation that will say,
uh, what are the instances of the string, uh, instrument, right?
What are all the individual string instruments?
Um, and if I do this,
um, actually notice now,
um, the- the dot- the- the dots will have different types of colors.
True-positive means, this is a string instrument inside the box.
Um, false-negative means this is a string instrument outside the box.
Uh, false-positive would be, um,
other entities that are not string instruments but are inside the box.
And true-entities are non-string instruments outside the box.
And what you see here is that basically,
we are able to perfectly enclose all the string instruments inside the box,
and there are 10- 10, uh,
instances of a string instrument in our dataset.
So we have just identified,
uh, all the string instruments in the embedding space.
Now we wanna take the box of, um,
all the string instruments and transform it to cover,
uh, the- the people,
uh, females and males, right?
Who play these instruments.
Uh, and if we do this, uh, here is the,
uh, these are the entities that are inside the box.
Again, because this is a two-dimensional projection, you know,
this is not a box,
uh, but in high-dimensions it is.
And again, we see that we are able to cover, basically, um,
98, uh, percent of all the- all the,
uh, all the- all the, uh,
instrumentalists of string instruments,
and that, we have a few false positives,
basically a few other entities, um,
that are not instrumentalists of string instruments,
but are still inside the box, right?
So there is a bit of error,
but the point is now we have 472 instrumentalists,
both musicians, female and male,
who play string instruments like a guitar, right?
So now, uh, with this,
let's move to the second part where we take the male,
uh, the node male.
Um, the male- the node, uh,
is here, uh, and now we wanna say, uh,
do the box transformation instance of to
cover all the males in this, uh, knowledge graph.
And if we do this, this is kind of the entities that are inside the male box,
it's, uh, 3,500 of them.
So we basically went from one entity to a giant box that covers,
uh, 3,500, uh, males that are,
uh, represented in this dataset.
Now that we have these two boxes,
we wanna apply the intersection,
uh, operator, uh, to this,
that you take the two boxes and take the intersection between them.
Um, and if you do this, the intersection, um,
has, uh, almost 400 different entities,
and these are the entities that are predicted to be,
uh, male players of string, uh, instruments.
What is interesting, you see that basically,
we are able to almost perfectly answer, uh,
this query even though we were doing all these operations,
uh, directly, uh, in the embedding space.
And this is just one example how we can basically answer, uh,
arbitrary predictive queries, uh,
over these, uh, types of incomplete, uh, knowledge graphs.
So, um, to summarize today's lecture;
uh, we introduced, uh,
um, the problem of a, uh,
logical reasoning in knowledge graphs using embeddings, and in particular,
we talked about how are we formulating
answering queries as a predictive machine learning task.
So we basically, rather than traversing the knowledge graph,
we have defined the task in terms of a binary prediction task that says,
"Predict what entities are the answer to a given query."
And the benefit of our approach is that it is,
uh, very scalable, meaning, uh,
answering queries is very simple because it just requires, uh,
transforming boxes and then checking what entities are inside the box.
Um, and it is also very robust because we
don't rely on individual links of the knowledge graph,
but we really rely on the embeddings and the relationships in the embedding space.
Um, and the key idea for today's lecture was we wanna
embed queries by navigating the embedding space, right?
We have this com- we've- we took advantage of
this compositional property of uh, TransE method,
and then we extended TransE to this notion of
box embeddings so that we can then define the intersection of boxes.
Um, and we learned how to transform boxes according to given relations,
we call this a projection operator,
um, and then we also showed how the,
um, how the union operations can also be carried
out- carried out by rewriting the query into the- a disjunctive normal form and,
um, apply the union operator,
uh, all the way, uh, at the end.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 12.1 - Fast Neural Subgraph Matching & Counting.txt
Uh, today we are going to talk about new,
very interesting, and exciting problem called, uh,
subgraph matching and subgraph counting or, uh,
frequent subgraph counting and frequent subgraph matching.
What is exciting is that we will, uh,
talk about how can you do this with neural networks.
So basically, how can you take a very,
uh, classical, um, uh,
problem, uh, kind of classical combinatorial problem and cast it as a,
um, machine learning problem.
Uh, and this would be the exciting part and we are
going to use embeddings and graph neural networks,
uh, to make this all work,
and we will be able to scale it up and make it accurate, uh,
and kind of swap- ste- side-stepping all the kind of complex,
um, uh, discrete base,
the matching and counting.
So, um, here's the idea for today. All right?
We are given a big graph and we would like to
identify what subgraphs are common in this graph, right?
Like we can think of the graph as a- as a being
composed of a set of building blocks, meaning small subgraphs,
and we can think of a lot of these graphs almost like being composed of
these little pieces as the same way as let say, um, you know what, uh,
once you build something out of Legos,
that object that I know how so- that you built,
it's composed of small pieces that combine it,
uh, all together and create a house, right?
So in some sense, what we'd like to identify is what
are the most common Lego bricks that are,
uh, uh, that, uh, compose the graph together.
So, um, the power here will be that we are able to
characterize and discriminate different networks based on these building blocks.
And the task today will be,
how do we identify and define,
uh, these building blocks, uh, for graphs.
So to give you an example,
you can take a set of molecules,
as I show you here,
and you can represent them as graphs.
and what are common substructures, uh, in these, uh,
And now you can ask what are common, better,
in these graphs, um, and, uh,
this way I'm able to understand,
let's say the structure of these molecules and what are the important substructures.
In this case, for example,
you could identify that this particular,
um, substructure is, uh,
common across all the molecules,
and you know, it turns out that this is actually a very, uh, important, uh,
group that, uh, would tell you whether,
uh, the molecule is acidic or not, right?
So in many domains,
you have these recurring structural components
that determine the function or the behavior,
uh, of the graph similarly as this example in, uh, molecules.
And of course the question is,
how do we, uh, extract,
identify these commonly occurring, uh, substructures?
Um, we are going to, uh,
approach this problem in three different, uh,
kind of as a set of three steps.
First, we are going to talk about subgraphs and motifs,
where we are going to define what is a subgraph,
what is a motif,
and then we'll also talk about how do we identify, uh, significant motifs.
And then after we will be done with the first step,
we'll talk about how can we use
graph neural networks and embeddings to represent subgraphs,
and how can we then quickly identify common subgraphs, uh,
using only the embedding space and no need to do,
um, a very expensive, uh,
discrete type, uh, matching.
So let me first go and define,
um, subgraphs and, uh, motifs.
So, uh, here are two ways to formalize this idea of,
uh, uh, building blocks of networks, all right?
So we are given a network,
we are given a graph with a set of nodes and a set of edges,
and the first, uh,
definition we will be using is called node-induced subgraph,
where basically the idea is that you take a subset of nodes
and all the edges that connect these nodes.
So induced means it's,
uh, determined by the node set.
So the idea is that let's say this G prime on a- on a set of, uh,
nodes V prime and a set of edges E prime is
a node-induced subgraph if node set is a subset of the nodes,
and then edge- edge set is simply, um,
a set of all the edges that the- that the- existing the big gra- uh,
in the bigger graph where both endpoints are part of my, uh, subgraph, right?
So this means that G prime is a subgraph G,
uh, induced by the vertex set,
uh, V prime, right?
So basically, induced means,
it takes- it say- it ta- it means take
all the edges between the- the vertices that you have determined, right?
So, uh, we only get to choose the vertices, the edges.
Uh, the edges are determined by the set of vertices, uh, we picked.
Um, so this is what people say node-induced subgraph or generally,
we just call it induced subgraph.
It's basically a subgraph defined by a set of nodes where we take all the edges,
uh, between that set.
Um, then a second definition, uh,
that is less common is to talk about edge-induced subgraphs.
Here we take a subset of edges and all the corresponding nodes.
So G prime is an edge-induced subgraph, um, er,
simply defined by the subset of edges
E prime that is a subset of the edges E in the entire network.
And then now in- in this case,
the V prime, uh,
the set of nodes is simply defined,
um, through the edges we have selected.
Um, so in-in- in,
um, the terminology we'll be using, usually,
we say this that this is
a non-induced subgraph or just a subgraph because it's determined,
uh, by the set of edges rather than by the set of nodes.
When we say induced subgraph,
we would mean select the set of nodes and,
um, determine the edges,
and when we say non-induced,
it would mean just select the node- the edges,
and then the nodes get automatically, uh, determined.
The- the two ways of formalizing network buil- building blocks,
um, it will really depend,
uh, on the domain we are interested in.
Most often people like to work with induced subgraphs because otherwise if you do edge,
uh, induced subgraphs, the, er,
the number of, uh,
possibilities, uh, explodes.
So especially in natural science domains like
chemistry and so on where we worry about functional groups,
we are going to use node-induced subgraphs.
Uh, in other domains, for example,
knowledge graphs, it is actually often edge-induced subgraphs.
For example, if you think about focusing edges that represent,
uh, local logical, uh, relations.
So now that we have defined these two no- notions of a subgraph,
then, um, you know,
the preceding definitions of subgraphs, uh, uh,
basically say that V prime is a subset of V and E prime is a subset of E,
which mean- basically means that nodes and edges are taken from the original graph,
uh, G. Now, um,
you could also say, okay, what if,
er, V prime and E prime comes from totally different, uh, graphs?
For example, you could- can you somehow define that,
you know, you have two different graphs,
G_1 and G_2, could you somehow say that G_1 is contained in G_2?
Right? G_1 is this triangle of three nodes,
and we can see that in G_2,
this triangle is contained by the subgraph X,
uh, Y, uh, and Z.
So how do we say that one graph G_1 is contained in another,
let's say bigger graph, uh, G_2.
The way we do this is that we need to define the- the, uh,
problem or the task of graph isomorphism,
where the graph isomorphism problem,
uh, i- is the following problem,
you wanna check basically say, yes,
no, whether two graphs are identical.
So the idea is on having graph G_1 on some nodes and edges,
and I have graph G_2 and again,
some nodes, uh, and edges.
And I say that G_1 and G_2 are isomorphic.
If there exists a bijection, basically,
it mea- means there exists
a one-to-one mapping between the nodes of one graph to the nodes,
uh, of the other graph such that,
uh, all the edges, uh, are preserved.
Meaning if u and v are connected in,
um, uh, in the graph 1,
then the mapping of node, uh, uh,
u and the mapping of node v,
uh, is also connected, uh, in graph 2.
So this a and b should actually be, uh,
u and v, so we need- we'll fix that, right?
So this mapping F is called graph isomorphism.
So to give you an example,
if I have two graphs here, you know,
one- one looks like this,
the other one looks like that,
they are isomorphic because if I, uh, map, uh,
these nodes, uh, one to the, uh,
one to the other, as I show it here,
then basically, uh, the edges,
for example, these two nodes are connected here,
they're also connected there,
this- there is this connection which is here, and so on.
So clearly, these two graphs are isomorphic.
I can map nodes from one to the other,
and I'm able to preserve,
uh, all the edges.
In this- in a similar sense,
these two graphs are non-isomorphic
because there is no way for me to map these four nodes, uh,
of the left graph to the nodes of the right graph,
such that if two nodes are connected on the left,
I'd know they are also connected on the right.
Um, so this is the problem of
Graph Isomorphism is checking whether two graphs are identical.
And- and the issue here is that we
don't know how map- how nodes map to each other, right?
It goes back to this idea that ordering or- i- i- no- ideas of the nodes are arbitrary.
So really there is no special order to them.
So we really need to check in some sense,
all possible orderings to determine if one graph is the same than the other graph.
Um, and if you ask, "Okay,
so how hard is this graph isomorphism step um, problem?
It is actually not known whether graph isomorphism is NP-hard.
But we don't know any polynomial algorithm for solving graph isomorphism.
So it seems it's this kind of
super interesting problem where we cannot prove that it's NP-hard,
at same time, we don't know uh,
any algorithm or nobody was able to
determine the algorithm that would solve this in polynomial time.
So it's somewhere in between. Nobody knows.
Still a big open question.
So this is now the notion of graph isomorphism.
So now we can define the notion of subgraph isomorphism.
Where do we say that uh,
G- G2 is subgraph isomorphic to G1 if for some subgraph of G2,
the subgraph is isomorphic to G1.
So um, what they commonly say is also that simply that G1 is a subgraph of G2, right?
And we can use either node or edge induced subgraph definition in this case.
And this problem is known to be NP-hard.
To give you an example, right,
this is G1, this is the graph G2.
I say G1 is subgraph isomorphic to G2,
or G1 is a subgraph of G2.
Because if I use this particular node mapping, right?
A maps to X, B maps to Y,
and C maps to Z,
then these connections between the three nodes in G1 are preserved in G2 as well.
Notice that we don't care about additional connections.
So this doesn't matter because this node- this structure is not part of G1.
And the other thing that is also important to note is
that this mapping does not need to be unique.
It is just enough to find one mapping where I map nodes of one er,
sub er, graph to the nodes of the other graph in a unique way,
in a one-to-one mapping.
So it's- two nodes cannot map to the same nodes.
Right, so in this case,
we have now been able to mathematically define and determine that
G1 is a subgraph of G2 because there exists this bijective mapping,
so one-to-one mapping, so that uh, every- any er,
nodes from G1 map to G2 and if two nodes in G1 are connected,
then their maps, uh their transformations are
also connected in G2 and the other way around, right?
So that's the- that's the important uh, part here.
So now um, we have th- what have we learned so far?
We defined the notion of a subgraph.
We defined the notion of a graph isomorphism problem,
and then we also defined the notion of a subgraph isomorphism problem, right?
Basically saying is wha- is a small graph contained in the bigger graph?
Now, of course, um,
when we talked about subgraphs,
usually we are interested in all subgraphs up to a given size.
Size, meaning the number of nodes if we talk about different subgraphs of given size.
So to show, actually there is a lot of
different subgraphs of a given size and this number increases very, very fast.
So for example, if here I show an example of
all non isomorphic connected undirected graphs of size 4, right?
These are all possible graphs on four nodes, um,
where the number of nodes is fixed,
number of edges can vary,
and the other constraint is that these graphs are connected.
So there is four different graphs on four nodes undirected.
Now, for example, if you say what are
non-isomorphic connected directed graphs of size 3,
if you look at that, there's already 13 of them, right?
It's only three nodes.
But because edges are directed,
I can have edges in different directions,
and this gives me 13 different uh, graphs.
So what does- wh- why is this important?
Because if I have a directed graph and I say,
what are the building blocks of size 3,
then I would need to determine the frequency,
the number of times this particular subgraph number 1 is included in the big graph.
And then I need to determine how often is this guy um,
included and so on.
So the point is that the number of these building blocks,
different subgraphs, increases super exponentially.
So in general, people usually only counted these building blocks up to size 4, 5.
Because even at the level of 5,
there is thousands of them and it's kind of a lot to
keep track of and it becomes a very hard computational problem.
So now that we have defined the notion of subgraphs inclusion,
and I showed you that there are
many different possible subgraphs that are non-isomorphic with a given number of nodes.
Then, now we define the next concept that um,
will be important for today's discussion,
and this is the concept of a network motif.
A network motif is defined as a recurrent,
significant pattern of interconnections in a graph.
So now let's unpack this and determine like this, make it precise.
What do we mean by this?
First is um, we define a network motif as a pattern,
which means a small node induced subgraph.
Then we need to say what do we mean recurring, right?
Recurring means it has to appear multiple times, right?
It has to have high frequency.
It has to be contained many times in the underlying graph of interest.
And then there is another interesting part,
where we say significant,
and significant means that it's more frequent than what we would expect.
And of course, if you say more frequent than what we would expect,
then you need to have some way to say,
okay, but what would I expect?
And this means you need to have a null model,
you need to have a random graph null model.
So you say, aha, what I would expect in the model,
what I see in the reality, is there a big discrepancy?
If there is a big discrepancy,
this- then this subgraph pattern,
this motif must be an important thing so let's surface it out to the scientists.
To give you an idea.
Imagine I'm interested in this particular motif on three nodes in a directed graph,
then when I talk about motifs,
these motifs need to be induced.
So for example, this is not an instance of the motif
of interest because actually this is a triangle of three nodes.
It's not so bad, so
there's no edge in my motif here,
but there is there, for example.
So really the instance is here because this- this particular subgraph of interest,
this motif appears here and there is one-to-one mapping.
So I say, aha, I found this- the incidence of this thing here.
Of course, you know,
there are many other places where this same motif occurs.
So um, the question is,
why do we need this notion of motifs?
And motifs help us understand how uh, graphs work, how networks work.
They help uh, make us- they help us to make predictions based
on the presence or lack of presence uh, of a motif in a data set.
So for example, uh, feed-forward uh, loops,
this is defined as a feed-forward loop motif,
were found to be important, um,
for, uh, networks of neurons for- so basically for brain networks because they,
uh, neutralize what is called biological noise, uh.
Parallel loops are important in- in food webs because it says that,
um, eh, are given predators preying, uh, uh,
on two different uh, species that have a common food source,
and you know in, for example,
in gene control networks,
you have a lot of this type of a single- what is called single input modules
where um, this gene regulates uh, a lot of uh, other uh, genes.
So these are some examples of uh, significance of
motifs uh, for the function of a given underlying network.
So now, uh, let's go and define the two things we discussed.
First is, we need to define frequency,
second, we need to define significance.
So let's define frequency first, right?
Let's say G_Q is a small uh, sub-graph of interest and Gt be the big target graph.
Uh, and then we say that we will define a um, graph, uh,
level, subgraph frequency uh, by the following definition.
We'll say that frequency G- of this graph G_Q in the bigger graph,
G_T is the number of unique subsets of nodes, um, in, uh,
the big graph, for each the sub-graph, uh, um,
of the big graph induced by- by its nodes is- is,
uh, isomorphic to this,
let's call it G_Q, so the query graph.
So to give you- to give you an example, right,
this is kind of a mouthful,
but the intuition is actually quite simple.
Here I have the query graph,
here I have the target graph.
This query graph appears twice in the target graph, you know,
there two triangles, one is here,
the other one is here,
so the frequency would be 2.
Um, here's a different example,
imagine I have this star sub-graph um,
and I want to ask it,
how often does it appear in this graph, um, of uh,
interest? Actually here um, perhaps counter-intuitively,
the frequency will- will be super large because, you know,
the center node will map to the center node,
but then the number of the satellite,
these uh, leaf nodes um,
is huge because I can select any 6 out of 100,
and any 6 I select,
it's a different mapping.
So I basically count how many different ways am I able to
take this graph and map it to the target graphs?
So in this case,
the number of different mappings would be
100 choose 6 because out of the 100 nodes here,
uh, I want to choose different subsets of 6
because the s- the star graph uh, has uh, 6 nodes.
So here the frequency of this um, would be um, would be huge.
So um, this is uh, the graph-level sub-graph frequency uh, definition.
Uh, there is uh, also
a more precise uh, frequency definition ca-
called node level uh, sub-graph frequency definition,
and here the idea is that uh the query comes
with a- uh, comes with the graph as well as with an anchor.
And then we do the mapping,
we say to how many different nodes can this anchor be mapped?
Uh, so that this uh, sub-graph Q is contained uh, in the target graph, right?
So here we are saying,
I want to be able to map uh, the edges of the- of the graph uh, Q to the graph uh, T,
and I want to count to how many different nodes can this anchor uh, node uh, be mapped?
So in our case, for example,
if I have this star graph from the previous case,
and I select a center as- as a uh, as an anchor,
then the frequency of this um, graph in my target graph would be uh, 1.
So there is only one way to map this anchor uh, to my target graph.
For example, if I were to select the anchor as one of the satellites,
then the frequency of the sub-graph would be 100 because there is exactly 100 ways to map
the anchor to one of these 100 nodes such that the
entire sub-graph maps uh, to the uh, to the target graph.
So this is um, the node level sub-graph definition,
where we have this kind of anchor and we are asking how often can we
map the anchor together with the corresponding uh, sub-graph?
Okay. So now uh, that we have defined the notion of a sub-graph frequency, uh,
the graph level and the node level,
the last thing to say is, you know,
is it a problem if the graph is disconnected?
If I have multiple small connected graphs- disconnected graphs.
Solution is very simple,
I can s- simply treat
all these small um, separate sub-graphs
as one giant graph with multiple connected components,
so that is uh, no problem uh, at all,
just to kind of address this point.
So now that we have defined frequency,
we need to define the significance,
and we will define motif significance um, in a way that we compare
it- we p- compare the number of occurrences with some null-model,
with some kind of point of comparison and the idea is that if
a given sub-graph occurs um, in a real graph much more often,
than uh, than in a random network,
then this- then it has a signi- functional significance.
So let me now define uh, quickly how do we generate random graphs?
And the first way to define a random graph is
uh, the model called Erdos-Renyi random graph model.
And this random graph model has- is a stochastic graph model that has two parameters.
It has um, n and a p,
n is the number of nodes,
and p is a probability of an edge.
So how do you generate a graph from this model?
You simply create n isolated nodes and then for each pair of nodes,
you flip a biased coin with a bias p, and if the-
the- the- tai- the coin flip says create an edge,
then you would create an edge,
and uh, right, the generated graph is a result of a random process,
so the more t- the- you can generate multiple graphs and they'll be
different because the coin flips uh, will come up uh, differently.
Even if you set the same parameters,
here, you know, I have five nodes,
so n is 5 and probability of an edge is 0.6 and, you know,
this would be le- let's say three instances of
a random graph generated from this G n, p model.
So um, now the next question is,
can we have a more precise uh, model?
Like- because in this model, all I get to
specify is the number of nodes, and the probability of an edge.
So actually there is a more precise model uh, that is called configuration
model and the goal here is to generate a random graph with a given degree sequence.
So what does this mean is, if I have my real graph Gt,
I want to generate a random version of it.
One way to have a random version of it would simply be to say, you know,
my Gt has n nodes,
so let me generate an Erdos-Renyi random graph with n nodes.
And I will set the value of parameter p such that in expectation
this random Erdos-Renyi graph will have the same number of edges as my uh, G_t.
So I match in terms of number of nodes and the number of edges.
Um, in the configuration model,
we are going to match both the number of nodes,
number of edges, but also the degrees of the nodes.
So basically, we will say I wanna generate
a random graph that has a given degree sequence,
meaning I have nodes 1- n,
and each node has a given degree.
But I don't specify how the nodes connect to each other.
Um, the way I do this,
it's actually quite simple and elegant.
I create n nodes and for every node I create, uh, uh, k_i,
uh, spokes, right? So for example,
node B has degree 4,
so it has four spokes.
Node C has, uh,
degree 2, so it has two spokes.
What I can do now is I represent every spoke,
uh, as a node, right?
I take these spokes and I create spokes as nodes and- and the spokes from,
er, every, er, every, er,
supernode are- are kind of belong to a given box.
What I do now is I go and, uh,
I randomly pair up, uh, the spokes, right?
I basically- I randomly pair up these nodes.
And then I determine, um,
an edge between a pair of nodes if at least one spoke
from one partition links to the spoken the other partition.
So for example here, A and B are connected here because there is a node in,
uh, A that links to a node B.
Uh, of course, what is the issue here is sometimes it will happen that I will have,
uh, multiple spokes linked to each other
and that will only result in a single- single edge.
So I'm going to ignore that.
And of course it can also happen that for example,
these two nodes would link to each other.
So this would correspond to a self-loop.
And I'm also going to, uh, ignore this.
And kind of the reason why I can ignore all this is because, uh,
in practice, the probability of there being multiple edges between, um, er,
spokes coming from the same node or, um,
being a se- generating a self-loop is so- is so small that I can, uh,
ignore it for all practical, er,
purposes and also kind of mathematically,
uh, you can ignore it because it is, uh, so rare.
So basically this means that now I have
a very useful null-model of networks because I can compare
the real network with
a random version of the network that has the same degree sequence, right?
Node have the- nodes have the same degrees as in the, uh, G-real.
And now this gives me another different null-model where basically I create spokes, I
then create a G n, p by randomly connecting the spokes and then join these spoky nodes.
These mini-nodes it to get back,
er, a resulting, uh, graph.
So now that we have defined, uh,
two random models, the configuration model and the Erdos-Renyi model,
uh, generally, we would prefer to use the configuration model.
Now, we need to def- determine,
uh, and define what is motif significance.
And the- the intuition is that motif is already represented in a network,
uh, when we compare it to this random null-graph.
So the idea is the following.
I'm going to pick a sub-graph of interest and
I'm going to count its frequency in the real graph.
Then I'm going to generate a lot of random graphs, um,
that kind of match the real graph in terms of some statistics like the number of nodes,
number of edges, uh,
as well as degree sequence.
And I'm going to count the frequency of the same motif in this,
um, um, uh, random graph as well.
And then I'm going to define next a statistical measure that will
tell me kind of how- how- how big is the discrepancy between the,
um, frequency of the motif in the real g raph versus in the random version of it.
And in the statistic I'm going to define to quantify these is called a Z-score.
So let me explain, uh, what the Z-score is.
So, z- score,um, uh, of a given sub-graph
or a given motif I is- captures its statistically significance.
And the way we are going to do this is simply say, uh,
what is the number of times this motif I appears in the real graph?
What is the average number of times this same
motif I appears in the random versions of the real graph?
So because we have multiple instantiations,
I can compute the average and of course,
I can also compute the, uh,
ra-standard deviation of the frequency of
that motif in these different random instantiations of the,
uh, random graph corresponding to the, uh, real graph.
And the Z-score now,
will basically tell me how much over-
represented or under-represented is the motif, right?
And we are, uh, doing two things.
We compute-we compute, compare
the frequency in the real graph versus in the random graph.
But then we also, um,
divide by the standard deviation,
by the variance of that, uh,
of that count across multiple instantiations of the random graph, right?
So basically, what does this mean is that we somehow
normalize the count based on the natural variability,
uh, of- of the- of the count of that motif.
So now this gives me the Z-score of a given motif I.
And then what people compute and define is called network significance profile,
where basically we- we do it such that
the-the-the sum of the squares of the Z-scores, uh, equals to one.
So basically we normalize, um,
the significance profile where at the ith- ith component of the significance profile,
we take the Z-score of- of, uh,
sub-graph i and divide it by the square root of the sum of squares,
uh, of the, uh, Z-scores.
Uh, notice that Z-score, uh,
Z-score is such that if it is 0,
this means that, uh,
the motif occurs as often in the real graph as in the random graph.
And then, um, if you know the Z-score is bigger than plus or- or, uh, minus 2,
then we would say that a given motif is statistically significant,
appears statistically significantly more often or less often,
than, uh, what we- what happens,
uh, in the random graph?
So, um, and this allows us now to compare
networks of different sizes because the row counts can be quite different,
but the Z-scores are,
uh, size, uh, invariant.
So significant- significance profile is that basically for every sub-graph,
we have to count how often it appears in the real graph,
how often it appears in the random graph.
We need to do this over multiple random instantiations
so that we can then compute, uh, the Z-score.
And we need to do this for every possible sub-graph,
uh, of a given, uh, size.
And then for example,
we can take different networks,
like gene regulatory network,
neural networks of synaptic connections between neurons.
We can take the network of the world wide web,
we can take a social network, um,
or even like a network def- defined based on text,
based on word adjacency and compare, uh,
frequencies and significant prof- significance profiles, uh, between them.
And what is interesting, for example,
here is, these are the 13,
uh, sub-graphs of size,
uh, size 3 for directed graphs.
So these are now, uh,
my motifs and the y-axis here, is the Z-score.
And here are different instances of the same type of a- of a network.
For example, here are three instances of, uh,
uh, web graphs and three instances of social networks.
And you can see how basically they have the same significance profile.
You see, for example, how this triangle of,
uh, um, mutual connections is heavily overrepresented.
You notice how this particular motif, for example, here,
is very, um, is very much under-represented.
And for example, in social networks,
this makes sense because this says, you know,
imagine this is, uh, yourself or myself.
This means I have two friends to whom I have very strong relationship,
but these two friends are not friends with each other.
And actually, social science theory says that what would happen in this case is
either these two people become friends with each other and you end up with this motif 13,
or one of these edges will break because simply it is too- too hard
for you to maintain two separate relationships with two separate persons,
um, rather than, you know, bringing them together.
It's almost like saying you have to go to two coffees every
week versus all three of you going for a coffee and having good time, right?
Like this is much more hard,
uh, to maintain, uh, in practice.
And you see that in, um,
social networks, this motif is heavily underrepresented.
Um, but for example,
you can see that in other types of networks like signaling,
it's actually the feed-forward type loops here that are, uh, over-represented.
So basically you can get inside into the, um, uh,
function of networks by looking at this,
uh, motif, uh, profile.
So let me summarize.
Why did we decide is how do you detect network motifs?
You, uh, first count sub-graphs,
I in the real network.
Then you count the same sub-graph I in the random version of the real network here,
denoted as G_rand.
Uh, G_rand is a null model that has the same number of nodes,
same number of edges,
and the same degree distribution or the same degree sequence as the real network.
And then you assign or compute a Z-score for every subgraph i, where you simply say,
how often did this- this subgraph appear in the real graph minus how
often does it tend to occur in a random graph
divided by the standard deviation of the count,
um, of it in the random graph and,
uh, you know, motifs with high absolute z- scores.
This means they are either heavily over- represented
or heavily under-represented in my graph.
And that's why we say that they are, uh, significant.
So, um, and, you know,
the last thing to say in this case is there are
many variations of this notion of a motif concept.
You know, there are extensions to directed and undirected graphs.
There are extinctions to colored, uh,
nodes, so meaning nodes with different types.
There is also extensions to, uh,
temporal, uh, temporal graphs as well.
So in temporal motifs.
Um, and then there is also a lot of variations in terms of how do define frequency?
How do you define statistical significance?
How do you define under-representation?
And can you kind of count anti motifs as well.
So basically, absence of an edge is important.
And also, uh, how do you do different null models?
So there is a huge and very rich literature and
very active research area in this notion of, uh, motifs.
So to summarize, motifs and sub-graphs are building blocks of networks.
Sub-graph isomorphism and sub-graph counting are NP-hard problems.
Understanding which motifs are frequent or uh,
significant in a dataset gives us insights into the unique characteristics of the domain.
And we use random graph, uh,
null-models as basically as reference points to evaluate
significance of a given motif by computing, uh, the Z-score.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 12.2 - Neural Subgraph Matching.txt
So now that we have defined the notion of
a network motif and a notion of a subnet network subgraph,
and I'm really kind of using these two terms interchangeably, now we need to- um,
and then we also defined what is a frequency of
a given motif or a frequency of a given subgraph in a given graph.
Now, we wanna basically develop
a neural network approach that will allow us to quickly determine whether a given,
uh, graph is a subgraph in a bigger graph.
Um, so to give you- um, the next,
we are going to talk about neural subgraph, uh, representations.
So, what's the problem?
The problem is called subgraph matching.
I'm given a large target graph,
and I'm given a small, uh, query graph.
And what I have to decide whether a query is a subgraph into- in the target graph.
Right? So for example,
here's my query, here's my target graph.
In this case, the answer should be yes because this particular set of
four nodes can map to this particular set of four nodes,
and edges are preserved.
Here, I use different colors to deter- to denote,
you know, that this node can map to that node.
You know, this node maps to this node and so on and so forth.
So query Q, is included in the target, uh, graph.
Um, so that's the, uh,
that's the first- uh,
that's the problem you want to do.
We just wanna say yes, no.
Right? It's not about the co- the number yet.
You just wanna say is query in- included?
Is it a subgraph of the target graph?
Um, how are we going to do this?
Rather than doing this as a kind of
combinatorial matching and trying to check edge by edge if,
uh, query is in- included in the target,
we're going to develop a machine learning approach to it.
So basically we're going to formulate this as a prediction task.
And our intuition will be that we're going to exploit
the sh- geometric shape of the embedding space to capture the subgraph relationship.
Let me, uh, now kind of, uh,
er, unpack what do I mean, er, by that.
So the task is that I wanna do a binary prediction to return
true if query is isomorphic to a subgraph of the target graph,
and otherwise I return false.
So again, in this case,
I give you the query, the target,
and the dashed, uh,
edges repres- lines, represent the node correspondences.
So in this case, just note,
we won't be interested in actually finding
correspondences because that's another challenging problem.
What- uh, and we're not going to talk about it.
All we want is just true, false.
Right? Given query, given target,
return true if query appears as a subgraph of the target, otherwise, return false.
We're just interested in the decision problem in the binary task.
So how are we going to approach this is, er, the following.
This is kind of the high level overview of the approach.
We'll take a big input graph, the target graph,
and we're going to decompose it in a set of- uh,
in- in a set of neighborhoods.
And these neighborhoods will be,
let's say, relatively small.
Um, so we are going to decompose it into a set of neighborhoods.
And then we are going to use a graph neural network to embed, uh, each neighborhood.
We are going to basically apply
graph embedding to create an embedding for each neighborhood.
Then at the query time,
we'll also take the query and we are going to embed the query.
And then what we are going to do is we're going to build a predictor that will say,
given the embedding of the query and embedding of a neighborhood,
you know, predi- predict yes or no.
Yes would mean a given,
um- a given, uh,
um, er, a given query is included,
is a subgraph, um,
of a given, uh, uh, er,
neighborhood versus, uh, uh,
that given query not being a subgraph of a given neighborhood.
So for example, for this query and this neighborhood,
we should predict, uh, no.
For this neighborhood in the query we predict yes.
Here, we also predict yes because it's an, uh,
triangle and an edge,
a triangle and an edge and, you know,
our query is a triangle and an edge, right?
So basically, based on the embeddings,
we wanna make these predictions.
And this can be super fast because as I'm giving a query,
I just kind of run through these neighborhoods and make predictions.
Yes, no, yes, no, yes, no.
That's essentially the idea.
How are we going to do this?
Let's give a bit more detail.
We're going to work with node-anchored definitions, right?
Meaning we are going to have a notion of an anchor and we are trying to
predict whether a given node anch- node anchored query,
um, is a subgraph of a, again,
a given anchor into the target, uh, graph.
So we are working with, uh, anchored, uh,
definitions where we- what means- what
has to do is one anchor has to map to another anchor,
and then all the edges and all the other nodes,
uh, have to map as well.
So that's the first important point,
is we have a notion of the anchor.
Um, the second, um, impor- important, uh,
notion is that we are going to decompose the target graph into a set of neighborhoods.
So in these neighborhoods will be node-anchored.
And what does this mean is essentially we'll pick
a node and then we'll take a k-hop neighborhood around it.
But let's say one hop, two hop.
Um, and then this will create a neighborhood and we are going to embed this neighborhood.
So we'll take the target graph and we are going to create many neighborhoods,
uh, and embed them.
And then for a given query,
the task will be given the query, uh,
in that neighborhood, predict whether query is a subgraph of the neighborhood.
So the entire approach is the following.
As I said, we create a query,
pick an anchored node, um,
uh, and, uh, embed it.
We take the target graph,
decompose it into many, uh,
neighborhoods and embed those neighborhoods.
And now given the embedding of the query and embedding of the neighborhood,
we wanna predict return true or false.
True if, uh, this query, uh,
is a, uh, node-anchored subgraph of the neighborhood,
and return false if, um,
these node-anchored query is not a subgraph of the node-anchored, uh, neighborhood.
So that's, uh, how,
uh, we are going to do this.
Um, so now, of course, the question is,
how do we use embeddings to make predictions?
Right? In terms of the, uh, uh,
creating the embedding of the neighborhood,
we believe- we can use our standard graph neural networks.
So kind of not too much,
um, uh, uh, importance there.
I'm kind of going to skip that detail.
There are some interesting kind of architectural details,
but just a standard graph neural network for embedding graphs,
uh, would already be good.
So now we need to decide and kind of r- I
wanna talk a bit about why do we pick anchored neighborhoods?
Why not non-anchored?
Um, you know, recall that the node level frequency definition says,
you know, the number of, uh,
nodes u in G_T for which some subgraph of G_T is
isomorphic to the query and the- and the isomorphism maps node u,
uh, in G_T to node v in the queue.
Right? So basically the anchors have to map plus all the edges,
uh, and the remaining nodes can also map.
Um, the point is that we are going to, uh,
create this anchor, the embeddings,
because we can then create basically a graph neural network,
um, for- um, around each node, uh,
u and each node v. And this way basically create embeddings of the neighborhoods.
So that's the reason why.
So we will use these embeddings to decide if neighborhood of u is
isomorphic to subgraph of neighborhood of v. And, um,
we not only predict if there exists a mapping,
but we can also, um,
in some cases be able to corres- to identify
corresponding nodes because we know that u corresponds,
uh, to v. So we'll also find the correspondence of, uh, anchors.
So how are we going to decompose G_T into neighborhoods?
Basically, for every node in the target graph,
we are going to obtain a k-hop neighborhood around the anchor.
We can simply pre- do this with- using breadth-first search.
Usually, you know, our parameter,
k, will be around,
you know, maybe three, maybe four.
So basically, we go three hops out,
we go four hops out.
And we can- and this way, we can, um,
decompose G_T, the target graphing to a lot of different neighborhoods.
And now, as we have created the neighborhood,
we simply apply our graph neural network embedding of the- of the anchor node,
um, v, to map that anchor node into the embedding space.
And then we can also do the same procedure,
uh, to the query to obtain neighborhoods in the query graph.
Um, and then we are going to embed these neighborhoods, as I said,
using a GNN by computing simply the node embedding for every anchor,
um, in its corresponding neighborhood.
Now, what is the- the cool part and actually,
the most important part of this lecture is this notion of
an order embedding space.
So, you know, what we talked so far was kind of
clear and you have perhaps- perhaps heard about it,
but you haven't heard about this topic.
This is now super cool.
So order embedding space.
Let me explain what we mean by this, right?
So, um, we map graph,
let's say A to point Z_A in a high-dimensional space,
let's say 64-dimensional embedding.
Um, and we are going to assume that the embedding space is non-negative,
so all coordinates are either 0or, uh, positive.
And then what we would like to do is we'd like to capture
partial ordering transitivity in the embedding space, right?
Then we are going to use this notation to say that,
you know, the- the left node is- is- is less than,
equal than the right node, um,
if all coordinates of the blue node- of
the left node are less or equal to the- all the coordinates of the right node.
So for example, in our- in this- in this case,
what this means is we have this transitive, uh, relation.
Because intuitively, when I say all the coordinates to be less,
it really means, um,
a given point has to be to the low- lower left of some other point, right?
So if- if this point is lower left
of that point and that same point is lower left of another point,
then also the first point is to the lower left,
uh, of the, uh, third point, right?
So here, you know, um,
this particular point is to the le- lower left,
uh, of, uh, that particular point, right?
So basically, what we want is we wanna, uh,
have these relationships of being to the lower- to the lower left,
which means in any- any part of the- of the space, right?
Basically, all the coordinates have to be less or equal,
which means you have to be embedded to the lower left of something else.
So- and this is called order embedding because
this partial ordering, this transitivity, uh,
is captured by this relation,
are you embedded lower left of something else.
So, you know, why- why- why should you care, right?
Why is lower left so cool and so important?
The point is that, uh,
lower left is so important is because it captures subgraph, uh, relations, right?
Imagine, uh, for example,
uh, the case here is that I have, uh,
the target graph, I have the neighborhood,
and I have different- different queries, right?
Then in my case, imagine that this is, uh,
the node anchored, uh,
neighborhood that I embedded here.
And I have two- two anchored queries,
Query 1 and Query 2.
And now, because basically the point is the following,
because Query 1 is a subgraph of the neighborhood,
Query 1 should be embedded to the lower left of the neighborhood,
while Query 2 is not a subgraph,
so it should not be embedded to the lower left, right?
So here, this notion of a subgraph relationship is preserved
because the Query 1 is embedded to the lower left of, uh, Query 2.
And simply by comparing the positions of the embeddings of this,
uh, anchor nodes, we can determine that, you know, uh,
Query 1 is a subgraph of, uh,
anchor node t, while, uh,
Query 2 is not a subgraph of anchor node,
uh, uh, t. So that's the cool part.
We can very quickly read from the embedding whether one is a subgraph of the other.
Um, you know, why does this work?
Why do we care about this, uh,
transitive, uh, partial ordering,
uh, in the embedding space is because subgraph isomorphism relationship,
um, can nicely be encoded in
this order embedding space where the order is defined by this relation,
are you lower left of somebody else.
And the reason is because the- the order- order relations,
so the lower left, uh,
relation is transitive and subgraph isomorphism is also transitive.
It has this property of anti-symmetry,
which is also, um,
encoded in the order embedding,
is that if G_1 is a subgraph of G_2 and G_2 is a subgraph of G_1,
then G_1 and G_2 are- are isomorphic.
They are the same. So if one point is to the lower left
of one and the other one is of the lower left of the first one,
then the points are on the same location,
so the two graphs are isomorphic.
Transitive would mean if G_1 is a subgraph of G_2,
G_2 is of G_3,
then G_1 is a subgraph of G_3 as well, which again is,
uh, encoded by the, uh,
you know, the subgraph, uh, relation.
And the last one is this notion of closure under intersection that,
uh, the trivial graph of one node is a subgraph of any node.
In our case, it would be the embedding at the coordinate origin,
at 0, 0, 0, is a subgraph of every other embedding.
It is to the lower left of any other- of any other embedding.
So basically, this order embedding space defined by this relation is one point to
the lower left of the other has
all these properties that the subgraph relation, uh, also has.
So, um, the reason now that- why we are interested in it,
as I- as I said and here I show,
uh, more- more formally, is that,
you know, the order embedding space captures transitivity, right?
In a sense that if, uh,
first point is the subgraph of the second and the second is of the third,
then first is also a subgraph of the third.
We have this notion of anti-symmetry, uh,
that if one is to the le- lower left of the other,
and the other is to the lower left of the first,
then they are- they are equivalent, they basically overlap.
Um, and then the last one is this, uh, closure, uh,
under intersection, uh, illustrated here,
uh, on the- on the right.
So basically, order embedding space defined by this lower left relation,
captures the same type of patterns,
properties that the subgraph, uh, relation has.
And that's the important part and the cool part of the order embedding space.
So now we are going to actually learn the embeddings of these,
uh, anchored neighborhoods such that the subgraph relation is preserved, right?
So we are going to use a GNN to learn the embeddings of neighborhoods,
basically to learn the embedding of the anchor node,
to preserve this order embedding structure,
to preserve the subgraph structure.
So the question is,
what kind of loss function should we use so that the learned,
uh, embedding operator reflects the subgraph relationship?
Um, and we are going to design a loss function based on what we call order constraint.
An order constraint specifies the ideal order of the embedding,
um, lower left property that reflects
a subgraph, uh, relation.
So this specify this, what we call,
order constraint to ensure that
subgraph properties are preserved in the embedding space, right?
So basically, what this means, uh,
here is- it's written in mathematics,
but basically it says that if a query is a subgraph of the target,
then every coordin- every embedding coordinate of the query should be less than the,
uh, embedding, uh- every coor- embedding coordinate of the target.
Right. So if Q is a subgraph of T,
then the embedding of the anchor node, uh,
in T should be to the, um, to the,
uh, greater and equal than the embedding of the, uh, query q.
So the relationship is- is this, right?
This is the query, that's the target,
so the anchor node, uh,
from the query should be embedded to
the lower left of the anchor node of the target because,
uh, que- this is, uh,
anchored subgraph, uh, of the target.
So that's basically what we mean by order constraint,
is that you have to be to the lower left.
Now, um, GNN embeddings are learned using,
uh- by minimizing what is called a max-margin loss.
So basically what we are going to do is to define
this notion of a loss where we are saying,
okay: so how much, um, is this,
uh, constraint, um, violated?
So basically here we say this is the maximum of zero and the,
uh, and the difference in the coordinates, right?
So if coordinate Z_t is always larger than the coordinate of q,
then this difference will be negative,
so maximum of negative in 0 is 0,
so the violation will be 0.
But if the subgraph relation is not preserved,
which means that along the given coordinate q is to the right or to the top of Z,
this means Z_q is greater than Z_t,
then this difference will be positive and then maximum
of 0 and a positive number will be a positive number,
so this E is the margin.
It will be basically, um,
the amount of violation of
the order constraint between a given query and a given, uh, target, right?
So here there is no violation,
while in this case there is violation because q is a subgraph of t,
but q is not embedded to the lower left, uh,
of t. So, um, eh,
according- al- along the first dimension,
this difference will be positive,
so the entire maximum,
uh, will be positive, uh, as well.
So this is now how we've arrived,
and what is important here is that now, um,
this, uh, loss, this, uh, penalty, uh,
E is differentiable, so we'll be able to, uh,
back-propagate this penalty into the graph neural network, uh, architecture.
So the embeddings are learned by,
uh, minimizing this max-margin loss,
so we have this E that determines the- the amount of, uh,
order constraint violation between a given graph, uh,
and a target, and we call this,
uh, penalty, this violation the margin.
So we wanna learn the correct order embeddings so- so that, uh,
the- the penalty is 0 when G_q is a subgraph of, uh,
G_t, and the penalty is greater than 0 when G_q is not a subgraph of G_t, right?
So we want penalty of 0 when, uh,
one is a subgraph,
so it has to be embedded to the lower left,
and then the penalty will be zero,
and if it's another subgraph,
then we want this penalty to be high because G_q
should not be embedded to the lower left of, uh, G_t, right?
So to learn this,
um, uh- this, uh,
embedding function, we need to construct training examples, right?
Of G_q and G_t, where, uh,
you know half of the time G_q will be a subgraph,
and the other time- half of the time it won't be.
Um, and then right when we are going to do the training of the embedding neural network,
we are going to, uh,
make it such that for positive examples we wanna minimize the penalty,
and for negative examples we wanna,
uh, maximize the penalty.
So here is how- how you can write this all out, uh,
by another kind of, um, uh,
huge- huge loss, uh,
type expression where we again say,
um, if, uh- if,
uh- if it will be, uh, um,
for positive examples, I want this to be zero.
So, um, you know,
I'll- I'll get some Alpha and for negative, uh,
examples, uh, uh, I- this will be greater than 0,
so this- I'll- this,
uh- this expression will be smaller.
So I'll wanna be able to uh,
uh, minimize this, uh, maximum.
So, um, now how do I generate training examples?
Is by simply picking an anchor node,
and then doing a breadth first, kind of,
a probabilistic breadth-first search around it,
and this means that I'll have the- I'll generate
the query that will be a subgraph of a given- of a given neighborhood.
And then to generate a negative example,
I can, you know, corrupt the query by,
you know, perhaps removing a node,
ending an edge, removing an edge,
uh, things like that, right?
So, uh- so that it is no longer, uh, a subgraph.
So, you know, how many of these training examples do I
choose so that I can then train my embedding neural network?
The idea is, as I train this, I wanna,
uh, at every iteration sample new training pairs.
Uh, the benefit is that at every iteration the model,
uh, will see, uh, different,
uh, subgraph examples, and it will improve performance,
and it will avoid, uh, overfitting.
How deep do I wanna make
my Breath-First Search training example sampling a neighborhood sampling?
Um, it's, kind of, a trade-off between the runtime and performance.
The deeper- the deeper I go,
the longer the runtime, but usually,
uh, the more, the better embeddings I get.
So usually we would use the- the depth between, uh,
3 and 5, also depending a bit,
uh, on the data set.
So, um, you know,
how do I now apply this when a new query arrives?
When a new query arrives,
I- I- the query has an anchor.
I simply embed the- the anchor,
um, and then, uh,
the procedure is that basically for every other,
uh, target, um, anchor at the
neighborhood of the target graph, I,
um- I compare the embedding of the,
um, anchor neighborhood with my,
uh, anchored query embedding.
And if the- if the query is embedded to the lower left of that, um, neighborhood,
then I say that, uh,
query is a subgraph of the neighborhood,
and otherwise I would say, uh,
that it's, uh, not a subgraph, right?
So basically, um, I can quickly check this by simply asking,
is one embedding to the lower left,
uh, of the other embedding?
So let me summarize, uh, this part.
So um, we talked about neural subgraph matching, which is, uh,
a way to formulate subgraph matching as a machine learning problem, uh,
and this way sidestep the NP-hard problem of sub- subgraph isomorphism.
Uh, basically given a query and given a ta- target graph,
we embed the query,
we embed the node anchored neighborhoods of the target graph,
and we train our neural network embedding function such that, um,
it embeds subgraphs in,
uh- to be in such a way that they are located to the lower left of each other, right?
If q is a subgraph of t,
then q is embedded to the lower left, uh,
of t. So this is- and at the training time of this embedding neural network,
we force it to obey this subgraph relation,
and this means we can then very easily and quickly find
out whether a query is a subgraph of a given, uh,
target neighborhood t. So basically embedding graphs with this order embedding
property or order embedding space allows us to test subgraph isomorphism very,
very quickly by simply just comparing the coordinates.
Basically saying, is the query embedded to the lower left,
uh, of the target?
And given the properties of the subgraph isomorphism,
uh, operator or re- relation,
we see that we can perfectly encode it into the order embedding space,
which means that actually all this is possible, um,
and we can, uh- we can do it,
and it, uh, works well in practice.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 12.3 - Finding Frequent Subgraphs.txt
So far, we have been
talking about the problem
where we are given
a query graph and we
want to predict
or identify a way
that a given query graph is
a subgraph in a larger target
graph.
Now, we are going to generalize
this approach we just
talked about to the problem
of finding frequent subgraphs.
So let me tell you
what this problem is.
What we would like to do is to
talk about frequent subgraph
mining.
So first, we discussed
in the previous lecture
about subgraphs and motifs
and we defined them.
And then we talked about the
problem of a neural subgraph
representation and be
able to quickly say
whether a given query graph is
a subgraph in a bigger target
graph.
Now, we are going to
further expand this
to frequent subgraph mining.
So here is the problem, the
idea is that given a large graph
we would like to identify
what are the building blocks?
What are the
frequently occurring
subgraphs that are part of
this bigger target graph?
And this is called the
frequent subgraph mining
where basically we
want to identify
these frequently occurring
building blocks of a big graph.
And generally, using
combinatorial approaches
the way you would approach this
problem is that, by the finding
most frequent size-k
subgraphs requires
solving two challenges.
First is that, we want
to enumerate or consider
all size-k connected
subgraphs, right, and then
each of these connected
subgraphs of size-k acts
like a query which we then want
to identify in the target graph
to count its occurrence.
So basically this
means that, in order
to identify frequent subgraphs
of a given size we first
have to identify what is
the universe of subgraphs
of a given size.
And then for each
of these subgraphs
find its frequency in
the given target graph.
So this is how traditional
approaches to this would work.
So for example,
if I'm interested
in connected subgraphs
undirected of size-3
then there are two possible such
subgraphs that are shown here.
I generated them so this is step
1, and then for each of them
I would want to ask, what is the
frequency of them in the larger
target graph?
And here basically I
would need to count them.
So for example, for this let's
say motif subgraphs of size-3
here these different
green polygons
show the instances
of this given motif,
so its frequency equals to 3.
So this is a hard
combinatorial problem
because just knowing if
a certain subgraph exists
in a given larger graph is a
hard computational problem,
right?
This is essentially a
subgraph isomorphism problem.
The frequent subgraph
counting is even bigger,
it's kind of even a harder
problem because first we
want to know the frequency
of a given subgraph
and then we want to
find the subgraphs that
are the most frequent out of all
possible subgraphs of a given
size.
So because the number of
subgraphs of a given size
increases
super-exponentially, you
are basically battling two
exponential algorithms.
First is the counting
and the second one
is the number of possible
things you have to count, right?
We saw how the number of motifs
or how the number of subgraphs
increases super-exponentially
with their size.
So traditional
combinatorial approaches
can only handle subgraphs
between size, let's say 3,
to 5, 6, maybe 7.
Because the number of them
increases so drastically
and then just counting
each one of them
to determine their
frequency is computationally
expensive as well.
So what we are going to
do in this lecture is
we are going to talk about
how can we use representation
learning and graph
neural networks to solve
this hard combinatorial problem?
As I said finding
frequent subgraphs
is computationally hard because
there are two problems that
are both exponential.
First is you get a
combinatorial explosion
in the number of
possible patterns,
possible subgraphs
of a given size.
And then given one of
these possible subgraphs
you have to count it, you
have to find its frequency
in the big target graph
so that then you can say,
here are the let's say
10 most frequent when
you kind of sort them by
their decreasing frequency.
So both of these by
themselves are hard.
So we are going to
elegantly sidestep
these hard
combinatorial problems
by using representation
learning and show how
we can tackle these challenges.
The combinatorial explosion
will be of possible subgraphs
will be attained by organizing
the search space cleverly.
And then the problem of subgraph
isomorphism and subgraph
counting will be tackled
by graph neural networks.
So let me give you a bit
more details about how
we are going to do this.
So for counting, we are actually
using the graph neural network
to predict a frequency
of a given subgraph.
So basically what
we are going to do
is we are going to take a big
target graph and embed it.
And then when a small
query graph comes
we are going to
predict its frequency.
So that rather than
directly counting it
we are just going to
predict its frequency.
So that will be the
first innovation.
And then the second
innovation, now,
rather than predicting the
frequency of all size scale
let's say k's 5, 10,
20, connected subgraphs
because there is super
exponentially many of them,
we are actually going to
develop a search procedure that
will start with a
small subgraph and grow
it node by node until it
reaches a desirable size,
and this way we will also
try to grow it in such a way
that its frequency
will be higher.
The important point
here is because we
are talking about
frequency subgraph mining,
we are only interested in
the most frequent subgraphs
and the most frequent
motifs and not
the frequency of all the motifs.
So that's why the search
procedure will be useful.
So let's look into
this problem more,
let's try to
understand it better.
So the problem set up for
the frequent motif mining
is the following.
We are given a
large target graph
G sub T and a subgraph
size parameter k,
as well as the number
of desired results r.
And the goal is to identify
among all possible graphs
on k node out of them that have
the highest frequency in G sub
T, right?
So I want to find r most
frequent subgraphs on k nodes
in a given target graph G sub T.
And here for the
frequency definition
we are going to use what we
call the node level frequency
definition, where this is the
number of nodes u in target
graph T, G sub T for which
some subgraph of G sub T
is isomorphic to our query and
the isomorphism maps node u
to node v.
And perhaps the best way
to understand this is,
imagine I have my
target graph, it's
a kind of a star with
100 spokes and imagine
I have a query graph that
is a star on 6 spokes.
And imagine that I say this
is the anchor I care about,
then the frequency of this
given query in the target graph
will be exactly 1 because
the anchor is going
to map to the center node
and that's the only way how
to map this query graph
to the target graph
while also mapping
the anchor node to one
of the nodes in the
target graph, right?
So rather here having G
sub Q frequency of 100 to 6
it will have just frequency
of 1 because there
is only one way to map this
anchor node to the underlying G
sub T. So this
frequency definition
is more robust to this
combinatorial explosion
when like I try to
illustrate here.
So that's the frequency
definition we care about.
So now the method that
we talk about is SPMiner
and it's a neural network model
to identify frequent motifs.
And the way this is going
to work is the following.
We are going to give
an input graph G sub T
and we are going to
decompose it into
node anchored neighborhoods.
And then we are going to use a
graph neural network to embed
each of these neighborhoods.
And this is exactly what we have
been talking in lecture 12.2,
right, where we talked
about how do you basically
take the graph, decompose
it into node overlapping
node anchored neighborhoods--
and then use the encoder,
use the graph neural network to
basically embed every dot here
as a different neighborhood
and you embed it
in the embedding space.
So this will be
the encoder part.
What is new in this
part of the lecture
is the last part
where we will have
what we call a search procedure
to find frequent subgraphs
by growing our motif.
So we are going to start
with a trivial motif of two
nodes and an edge
and then we are
going to iteratively
grow it while trying
to preserve its frequency.
We are going to say, how
do I grow this motif so
that its frequency remains high?
And when I reach the
desired size of the motif,
the size-k I'm going to stop
and say, here is the motif,
this is its predicted frequency.
So the point is that,
rather than trying out
let's say all possible
graphs on 10 nodes,
we are going to grow
a graph on 10 nodes
iteratively while trying
to maximize its frequency.
That is the idea,
this is the overview.
So now let's dive deeper
into this lecture.
So the key idea of SPMiner is
to decompose the input graft
Gt into a lot of
small neighborhoods.
And we are going to embed these
neighborhoods into an order
embedding space.
And the key benefit of
order embedding space
will be that we'll be
able to quickly predict
the frequency of a
given subgraph G sub Q.
So essentially rather
than taking G sub Q
and trying to match it in many
different places in the G sub
T, we are just going
to encode G sub Q
and then predict its
frequency in the G sub T.
That's the kind of
the idea is that we
have a very fast
frequency predictor that
will use graph neural networks.
So here is how we are
going to do this frequency
prediction, right?
So the idea is that we take
a set of subgraphs, these
are our node anchored
neighborhoods G sub
N of big target graph G sub T.
And the idea will be that
we are going to estimate now
the frequency of our given
subgraph of a given target
graph G sub Q by counting
the number of neighborhoods
such that their embedding
satisfies the order embedding
property.
So basically we are going to
say, for a given graph G sub Q
we are going to embed it into
some point in the embedding
space, and we are going to ask
how many neighborhoods of G sub
T are to the upper
right of it, right?
Basically with
this notation I try
to say how many
neighborhoods N sub
I are there whose
individual coordinates are
all greater than the embedding
coordinates of my query graph
Q.
And this is exactly
a consequence
of order embedding
space property
that we have discussed, right?
So intuitively when my G sub Q
arrives, I'm going to embed it
and I'm going to say
its frequency is simply
the number of node
anchored neighborhoods
that are embedded into the
top right of it, right?
So number of node anchor
neighborhoods whose embedding
coordinates are greater
than the embedding
coordinates of my query point.
So basically there is this
what we call a super graph
region, where basically all the
points, all the neighborhoods
in the red shaded region
corresponds to neighborhoods
in G sub T that contain G
sub Q as a subgraph, right?
So basically this will be
now our frequency estimation
and the benefit
will be that now we
have a super fast subgraph
counting frequency estimation
method because when a new
motif arrives we just embed
it and then determine
how many points
fall to the top right of it and
this can be done super fast.
So now that we know how
to estimate the frequency,
now let's talk about
the search procedure
that is actually going
to find the motif.
So the way we are going to
do this is the following.
We are going to randomly pick
a starting node u in the target
graph G sub T and then we
are going to basically expand
the neighborhood around this
target node u in the G sub T
to get the query
graph, and we are
going to use the
graph neural network
to estimate its frequency.
And we are going to
grow it in G sub T
while trying to keep the
frequency as high as possible.
And the way you can
think of this as that,
we are going to start
with an individual node u
and its embedding will
be somewhere all the way
to the lower level because an
individual node is a subgraph
of all neighborhoods, right?
Because the neighborhoods
are composed of nodes
so this individual node is a
subgraph in all neighborhoods,
right?
So here the point is
that each dot here
represents a neighborhood
in the target graph
that contains the
motif pattern, right?
And whatever is in the
red shaded three regions.
These are the neighborhoods
that contain that motif.
And of course, initially
all neighborhoods
contain the motif because
the motif is a single node.
So now that we have a
partially built motif S
we want to grow it
until it reaches
size-k such that its
frequency of this size-k motif
will be as large as possible.
So the way we are
going to do this is we
are going to do it through
an iterative procedure that
is going to grow the motif
iteratively meaning node
by node and edge by edge,
by basically traversing
the neighborhood of this
chosen node u in G sub T
and adding neighbors
of that node.
The neighbors of neighbors
to S and at the same time
we are going to use the order
embedding space to now take
S, embed it, and ask what is
the frequency of that bigger
subgraph, right?
So the way you
can think of it is
we are going to start
with the individual node
and we are going now to say,
an individual node and that's
our S, and then we are going
to add one neighbor of it to S,
so now S will be bigger.
And for every possible way
to add this new node or this
to the S we are going to ask,
what is the best node to add
to the S so that the frequency
remains as high as possible?
And we can use this kind of
gritty search procedure that
is going to grow our motif
node by node until it
reaches the desirable size.
And now of course,
what we need to decide
is how do we decide what
node to include next, right?
How precisely do we grow this
underlying motif, node by node?
And the way we are
going to do this
is that, we are going
to grow it with the goal
to maximize the number
of neighborhoods
that are in the red shaded
region after the k steps,
right?
So basically in the end,
we want to reach out
some point which that
describes a graph of a given
size-k such that the
number of neighborhoods
that are to the top
right of it, meaning
that are in these red-shaded
regions is as high as possible.
Because whatever are
the neighborhoods
to the top right of it,
these are the neighborhoods
that our red dot
is a subgraph of,
that's the order
embedding property that we
have worked so hard to establish
in the previous lecture.
So that's the idea.
So right then we'll be growing
this motif node by node
until it reaches the
desired motif size,
until it reaches size-k.
And then we are
going to terminate,
and what we are
going to return, we
are going to return whatever is
the motif we terminated and we
are going to return its
predicted frequency, which
is the number of neighborhoods
in this red shaded region.
The reason why we are--
just kind of
noticed that, we are
only interested in
the subgraph's motifs
that have high frequency.
So as we are building it we
are kind of greedily deciding
how to grow that motif so that
its frequency will remain high.
So to now answer the most
important question, which
is how do I decide which
node to pick and add
to my current subgraph motif S?
And the way we are
going to do this
is, we are going to define
the notion of total violation
of a subgraph, let's
call it G which
is the number of
neighborhoods that
do not contain this subgraph G.
So this is the number
of neighborhoods
that do not satisfy my
order embedding property.
Which basically it would be the
number of neighborhoods where
at least one of the
coordinates in the embedding
space of the neighborhood
is less than the embedding
coordinate of my graph G.
So this is the same thing,
this G is this cubed.
And this means that basically
minimizing the total violation,
means maximizing the frequency.
And you can then use many
different search heuristics
but one possible such
heuristic is greedy, right?
Basically, at
every step you want
to add a node to the subgraph
S that results in the smallest
total violation.
Basically, you want
to add a node that
will keep the number
of neighborhoods
to the top right
of that subgraph S
to be as high as possible.
And you can think of this,
right, we start with one node,
we add the second node,
we add the third one,
we add the fourth one, and
you are kind of dancing,
as you are building the motif
you are kind of moving up.
And the goal is to reach a
motif of a given size that
has the largest number
of neighborhoods
in the red shaded region,
so in the region that
is up and above from it.
And that's essentially the idea.
I can show you some
experimental results
to show that this really
works quite remarkably well.
Where for example, we can
say let the ground truth
be the most frequent top
10 most frequent motifs
in a given target graph.
And we are going to do these
four motifs of size 5 and 6
because this is what
kind of this brute force
exact enumeration counting is
able to do in a couple of days,
right?
So then we can ask is, what
are the frequency of the top 10
most frequent motifs
that this search
procedure is able to identify?
Where basically we pick a random
node u in the target graph G,
we kind of grow the motif
around it and at the same time
as we are of growing
that motif we
ask what its frequency as
predicted with the order
embedding space?
And then this graph
tries to illustrate this,
these are the top 10
most frequent motifs
just rank ordered, and the
y-axis is their frequency.
And exact is the
ground truth, right,
so this would be the frequency
of the most frequent motif,
the second most frequent, third
most frequent, fourth, fifth,
all the way down to tenth.
Now, what we also
did in our case,
is we use this neural
network order embedding
space based method to identify
top 10 most frequent motifs
that it finds.
And let's compare
their frequency
to what the exact
counting finds.
And of course, the frequencies
drop as the rank of the motif
gets higher, that's okay.
But you notice, for example
that SPMiner is basically able
to identify the top in this
case, top eight motifs perfect
out of the top ten.
And then the other two
are their frequency
are just a bit lower so these
two were not exactly identified
so their frequencies are lower.
Here is top eight are
perfectly identified here,
for size 6 it's
actually top nine that
are perfectly identified.
And these are some traditional
approximate searching methods.
You see that they fail
much, much, much worse.
And perhaps, if
the mfinder method
finds the top 10th,
the motif at rank 10,
its frequency is only 5,
raised to the times 1,000.
Our SPMiner is able to
find the tenth motif that
has the frequency of around
let's say, 15,000, right?
So much, much more accurate.
So this is for small
motifs, you can also
do this for very large motifs
because the search procedure
is very cheap.
So you can identify large motifs
for example, motifs of size 14,
17, 20, that still have
very high frequencies.
And you can do this in two
different real world networks
and really find large
motifs with high frequencies
computationally
very, very cheaply.
And the motifs you find
tend to be much, much
more frequent than what
kind of random search
traditional kind of
heuristic-based baselines
can do.
So to summarize this
lecture, here we
talked about
subgraphs and motifs
that are important concepts
that provide insights
into the structure of
large graphs, right?
And the frequency of
these subgraphs or motifs
can be used as features
of nodes or graphs.
And they can kind
of also tell us
what is the organization of
networks, what are the building
blocks of a given graph?
And I talked to you about how
it is computationally hard
combinatorially to
identify frequently
occurring subgraphs of a given
size in a big target graph.
And we covered neural
approaches for prediction
of subgraph isomorphism
relationship.
We developed this notion
of an order embedding space
that has a desirable property
and allows us to quickly say
whether a given graph is a
subgraph of a bigger graph,
or it allows us to
quickly say, what
is the frequency of a given
subgraph in a bigger target
graph.
So that prediction happens
super quickly and is accurate.
And then the last
thing we did was
we talked about this neural
embedding guided search that
starts with a small
motif, iteratively
grows it n node by
node such that it
allows us to identify
a large motif that
also has a high frequency.
And we saw that these methods
are extremely practical,
they are extremely fast
and lead to high accuracy
of identified motifs and
identified subgraphs.
So with this we have
finished the treatment
of subgraph identification,
subgraph counting,
and frequent subgraph mining.
Thank you very much.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 13.1 - Community Detection in Networks.txt
So today, we are going to talk about,
er, community detection in networks.
So basically, what community detection is,
is essentially clustering a set of nodes,
uh, in a graph.
So, uh, this is what we are, uh,
going to do today and the clustering we are going
to do will be based on the net structure,
er, of the network.
So, let's start, uh,
by defining this pro-, er,
problem of community detection and then I'm going actually to talk to you about-
about some social science research that explains why do we see these,
uh, clusters in networks,
why do they make sense,
and they actually inform the development of the methods,
er, to be able to do this.
So, um, when we write,
er, or when we draw, er, networks, let's say,
on a- on a piece of paper, we often, er,
would like to think of them to have kind of the following,
uh, the following structure, right?
We think of them as having these, uh, clusters,
these communities where the- the edges inside the cluster are more,
um, are more than the edges, er,
spanning, uh, across, uh, the clusters.
And this is kind of how intuitively we like to think of networks,
in particular social networks perhaps where
these types of clusters could correspond to social communities.
We think of them in terms of- in biological networks.
We can think of these as some kind of, uh, functional units,
modules that are there to do a certain task, um, and so on.
So, we'd like to think that the networks are not just kind of messy objects,
but that they have this kind of clustering, uh, org- organization.
So the question is,
how did this picture, uh, arise?
What kind of evidence do we have that these types of structure
actually appears in networks and what could be some reasons for it?
So, um, here is one way to think about this from the viewpoint of, er, social networks.
So right now what we'll do is we're going to do a bit of
a dive in into classical social science and classical,
er, social network analysis.
So the question is,
how does information flow through networks?
Think of a social network, right,
where basically we have people connected in a social network and
there are different links over which the information flows.
And let's call these links short and long.
Um, and let's call them, you know,
short as the ones that kind of are- are a very kind of, er, local.
These are strong friendships,
and let's also have these long links that you- which we can think of as
links to our acquaintances and kind of colleagues and people we,
let's say, meet, uh, less often.
And the way we can think of this is that, you know,
we have these short, er, short, er,
strong links and we have these weak, er,
long links, er, to other parts,
er, of the network.
So, uh, there was a seminal work, er,
back in 1960s, uh, by Mark Granovetter,
who's a sociologist here at Stanford University,
and this is I think the most,
er, cited and the most impactful,
uh, social science paper ever.
And this is really about his research during his PhD,
where he was asking about,
how do people find information through personal contacts?
Um, and, um, what- he was looking at this,
how people find jobs.
So he was asking people who were on the job market to basically- and have found a job to,
er, to tell him, where did you hear about this job?
Which, er, which person did you tell- tell you about- about this job opening?
How did you apply, things like that.
And, uh, what was interesting is that, um,
contacts through which these people heard for job openings and
found jobs were more often acquaintances than,
let's say, close friends, right?
So it wasn't that you would, uh,
hear that people heard about new job openings
from people they have very strong relationship with that, you know,
they spend a lot of time together,
but they heard about new job openings through these,
um, acquaintances, people that would- they would kind of meet less, uh, more rarely.
Um, and this is very surprising because you
would imagine that it is actually your friends,
people you have strong, er, relationships with,
whose- whose best interest is to help you,
who are closest to you,
who are the most motivated to help you find,
uh, a job, to connect you,
uh, with the job openings, and so on.
But what turned out was that it was actually
casual acquaintances who- who- through which,
um, through whom, uh,
the- these job seekers found about, er, new jobs.
So what this means is if we go back to the- to the picture that we had before,
it was that- basically,
it was this- these types of,
let say, acquaintance links,
these weak links, uh,
to- to have, uh,
to be the ones through which people heard about the jobs and
not necessarily these kind of strong, uh, interpersonal links.
So this means that acquaintances are more,
uh, were more helpful in this process.
And the question was, why?
And Granovetter gave these two perspective on a friendship or on a link in the network.
So the first struct- the- the first perspective on a link in the network is structural.
It's about, what is the structural role of a link in the network?
So you can think about,
what is the structural role of a friendship in a social network, right?
We can think about this particular edge being this kind of, um,
connecting different parts of the network,
while this particular other edge being very well embedded,
uh, inside the network, right?
Having all these common friends and being a really strong, uh, relationship.
So that's kind of structural.
And then there is also the interpersonal, er,
view which is about, er,
friendships being either kind of strong or weak, right?
Having a very strong relationship with someone or having a much,
er, weaker relationship with,
uh, uh, each other.
And, uh, what Granovetter shows is that structural and interpersonal,
uh, relationship of the link,
they- they are connected.
They- they are related to each other.
So, uh, let me explain how.
So first, uh, point is that Granovetter makes the connection between the
social and the structural role of an edge and shows that they are related,
they are correlated, they are connected.
So first is the structural point, right?
The structural, um, edge from, uh, er,
from the point of e- edge being in the network, um,
he basically say- shows that structurally embedded edges,
so edges, uh, that,
uh, that happen among co-, er,
in the- in the densely-linked parts
of the network where there is a lot of common friends,
that these edges tend to be strong edges like inter-personally strong.
And that these long-range edges that kind of connect the different parts of the network,
they tend to be socially weak, right?
So, um, what does this mean?
This means that, um,
there is a relationship between the strength of
the relationship and the structure of the relationship.
So strength means, uh, strong links, er,
between people who have a lot of friends in common,
and weak links connecting different parts of the network, right?
So weak friendships are ge- generally the ones that
connect different parts of the network and strong friendships are the ones that- that,
uh, where you have, um,
a lot of friends in common, and so on.
And what I want to say is,
strong friendship is not defined by having a lot of friends in common.
You can ask someone who are your strongest friends,
and then if you check in the social network,
you will see these are the people you have a lot of friends in common.
So these are the structurally strong, uh,
structurally well embedded relationships are socially strong and structurally,
um, uh, kind of di- the relationships that connect different parts of the network,
they are socially weak.
So those are- those are two connections,
two important aspects of a relationship.
So social strength of a relationship and
a structural role or position of the relationship,
that's the first point.
And then, the second important point that
Granovetter makes is this notion of information flow.
And he says that long-range edges allow you to gather
information from very different distant parts,
uh, of the network.
Uh, and this means that now you have access to
all these distant information that can come to you and can be very useful to you, right?
And, er, he says that structurally embedded edges are
heavily re- redundant in terms of information access, right?
So if you think that, I don't know,
this person A is, uh,
is an edge in the network and they are looking for, er, jobs, yes,
they have these three strong, uh,
social relationships with these three- three other people,
but essentially, these relationships are very redundant, right?
Whatever, uh, one of these people knows,
they can either tell them to node A directly,
or perhaps, you know,
they are going through casual conversations sa- tell it
to this other node who's going to tell it to node A.
So in terms of information access,
these edges are heavily redundant.
Well, for example, this weak long-range
edge to some other part of the network to this node B,
this is a very useful edge because it gives you access to
all this information from these other, uh, community.
So this is the second point,
is that strongly embedded edges are information redundant and it
is weak edges that give you access to very dif- distant parts,
uh, of the relationship,
uh, of the network.
Um, and this is important because if you look how social networks evolve, um,
they evolve such as the communities form,
in a sense that net- tightly-connected clusters of nodes naturally form.
And the reason for this is that, um,
the networks tend to evolve using triadic closure.
What that means is that,
if you have two friends in common,
you know, A has a friend B,
and A has a friend C,
then sooner or later A and B are going to meet and they are going to create,
uh, a link with each other.
Another important aspect is that we like to
connect with others that are similar to us, right?
So if I am, I don't know, a computer scientist,
or maybe I like soccer, football, right?
Maybe I play football with node B,
probably I may- play football with node C as well,
so they already will have something in common so they can,
uh, form that relationship, right?
So this is, um,
an important thing is that because we like to connect to others that are similar to us,
so if I'm connected to two other people,
they likely share some similarities,
some aspects to myself.
This means that they're alw- also going to share that similarity,
so they'll be more likely to bond,
uh, and create, uh, an edge.
So it means in networks,
these are the edges that,
uh, that ha- that like to form.
So for example, uh,
A to B is a likely edge, um, and, you know,
to- also A to C because there is a fr- a common friend,
uh, B, uh, in between.
So these are examples of edges that, uh, uh,
that are-, uh, happen to in social networks, uh, a lot.
So now, you know,
why is- why are some reasons that these types of edges happen?
Because if you remember early on we were talking about clustering coefficient, right?
The fraction of your neighbors that are connected with each other.
And this is essentially a metric of this notion of a triadic closure.
Because if we have this simple network structure,
uh, here, uh, on the right,
then you can say, you know,
if B and C have a friend A in common,
so why should B and C link to each other?
First is, they are more likely to meet each
other because they have this common friend,
uh, A, and maybe he invites them both,
uh, for the coffee or something.
Second thing is B and C are more likely to trust each other because again,
they have this common friend that they both trust so they say,
"Oh, you have this guy, uh,
as a friend, I- I like him,
you like him, obviously we have something in common."
And then there is also another important point,
which is A has incentive to bring,
uh, B, and C together, right?
Because from A's point of view,
it is hard or costly,
it requires effort to maintain, uh,
two relationships, uh, together,
uh, for, uh for, uh, Node A.
So if there is a triangle,
so if A links to B, B links to C,
then all of them basically can go and hang out together because they are all friends.
So these are reasons for, uh, triadic closure.
And that is an important aspect,
is that actually social science research shows that being embedded,
having a lot of the strong edges and a lot of triangles around you,
um, is, uh, is, ah, is, ah,
kind of, um, is- is- is easier, warmer, um,
it's, ah, this is what,
ah, what humans, ah, seek for.
And actually, for example,
there is a famous study, um,
of, uh, teenage girls who have,
for example, low clustering coefficient.
They are, uh, they are found to more likely contemplate suicide.
So basically right, it is hard to not have friends who are friends with each other.
So it's good to be well embedded in social clusters.
That's kind of the social structures, uh,
we humans, uh, seek.
So now, uh, this, uh,
Granovetter explanation was a theory from the 60's, right?
Basically what he said was the following.
He said, edges that are embedded in the network,
so edges that have high clustering coefficient,
edges that have a lot of,
uh, friends in common,
edges that uh, uh,
appear in dense parts of the network,
these edges tend to be socially strong.
And edges that connect distant parts of the network,
they tend to be socially weak.
Um, and this is a prediction he made,
um, based on his research,
but it was left untested for,
er, 40 plus years.
And actually it was only in 2007 that this theory, uh,
was actually, uh, kind of validated on real large-scale data.
And the study is based on cell phone,
uh, data, uh, from one of the European countries.
So where you say the strength of a relationship,
strength of an edge will be the number of phone calls,
um, that- that we make in a given period, right?
And the idea is that people who are strong friends with you will make
a lot of phone calls with and people you are more acquaintances with,
maybe you call them for Christmas, uh, and, you know,
wish them a Happy New Year, for example,
that would be, you know, once in a year type of, uh, phone call.
So the prediction from Granovetter would be that strong edges should be, uh,
these edges with lots of phone calls should be clustered together in, uh, small pockets,
and then these edges that, uh, that, uh,
that span across different parts of the network,
there the number of,
uh, um, uh, phone calls, uh, will be smaller.
That's the prediction. The way- the way, uh,
this was operationalized was first to say,
um, the notion of, uh, you know,
how, uh, how strong or weak is the edge structurally,
uh, and you quantify this by the metric called edge overlap.
And simply you ask,
what is the fraction of neighbors that edge endpoints have in common, right?
So for example here,
the edge overlap is 0 because these nodes i and j have no friends in common,
while here, for example,
this is an overlap of 1 because they have all these- all these neighbors,
each of them, and they are both mutually connected to all of them.
So here we would say this edge is very well embedded, uh,
it is structurally strong,
and this edge is structurally weak, right?
It kind of connects to different parts of the network that don't know each other.
Right? And then, um,
what Granovetter would say that when edges have low overlap, the, um,
the number of phone calls will be very small and when edges have high overlap,
the number of, uh, phone calls, uh, will be very high.
Here is now the, uh, the result.
Uh, this is the edge strength in terms of number of phone calls.
Uh, and this is the neighborhood overlap.
Uh, and this is the real data.
And what you see is that the strong- the more the phone calls,
the higher the overlap,
which means that highly used links,
links where a lot of phone calls happen have high overlap, right?
So links where there is a high number of,
uh, phone calls, uh, look like this,
and links where there is a lower number or phone calls have smaller overlap,
so they are structurally, uh, like that.
Um, and then here what, also, uh,
they show is an example of a null model
where they would basically keep the network structure the same,
but randomly reassign the edge strands.
And then you see that there is no,
uh, relationship, just two, uh,
and this is expected and it's just kind
of a safety check to make sure that this analysis makes sense.
So what this means is that, you know,
the bigger the overlap,
the stronger the edge personally.
To give you a picture that is even more telling
is that here is a part of this phone call network.
And this is the strength of the edge.
Red is very strong and yellow,
green is- is weak in terms of number of phone calls.
And what you see is that these red edges
tend to happen in these kind of small clumps here.
And you see that these long edges,
they are mostly yellow,
which means there is a small number of phone calls that happens over them.
And just to show you the contrast, now,
I will show you the picture of the same network,
but I'm going to randomly shuffle,
uh, edge strengths across the same structure.
Um, and what you will see is that now all of a sudden,
this is how the network, uh, looks like, right?
So if I randomly assign edge strengths according to the real edge strengths,
you see how now these long-range- long-range edges,
all of a sudden start to look, uh, red.
And why is this? This is the reason because there is a lot of
edges- strong grid edges hidden in
these clusters and they jus- you just don't get to see them.
Uh, and these all these long-range edges, they are all yellow.
They all have very small numbers or phone calls in them.
But if you now take these edge strengths and randomly permute them,
randomly assign them over the edges,
then this is how the network would look like.
Like all of a sudden, uh,
you see that there is actually a lot of long-range,
er, red edges that we don't- just don't see in, uh, practice.
So what does this mean?
Uh, this means that,
uh, edges that are strong, uh,
interpersonally appear in clu- in densely knit clusters,
and edges that are weak connect different parts,
uh, of the network.
And, uh, another experiment to validate this is to say,
what if I take my network and I remove edges one by one, uh, uh,
by the decreasing, um, sorry,
by the increasing number of, uh, phone calls, right?
First I will remove edges with one phone call,
then two phone calls and so on.
And then here is how I'm removing edges.
And here he is, uh,
how does my network fall apart into different connected components, right?
So as I'm removing edges,
the network will start to get disconnected.
And what you notice is that if I remove uh,
first the edges that have low number of phone calls,
the- the network is going to fall apart into multiple disconnected components,
um, much faster, if I would,
for example, remove, uh, highly, um, uh,
edges that are highly used because those edges are embedded in these- in these clusters.
So even if I remove them,
I don't disconnect the network.
But if I disconnect- if I remove these weak edges,
then the network gets disconnected, uh, faster.
The largest connected component is smaller,
so the red line is below, uh, the black line.
Um, um, an- and then if I do something
similar according let's say now out to the edge overlap where I say,
let's remove edges with low overlap first,
you see how now the largest connected component super quickly, uh, gets destroyed,
which basically the intuition is that if I remove the slow overlap edges,
very quickly, uh, I get these separate islands,
separate disconnected, uh, components.
So, um, you know,
why- why did we spend, uh,
uh, these 20 minutes discussing about all these?
Like what did we learn?
What we learned is that
this conceptual picture of networks that I try to sh- that I show here,
um, is actually well-grounded in sociological,
uh, processes and social science.
What we have also found out is that the interpersonal strength of a relationship and its,
uh, overlap, it, uh,
embedding in the network,
uh, are related, right?
So we- we showed,
uh, that these type of, uh,
nodes, uh, and these types of edges that have high overlap,
that have a lot of friends in common,
these tend to be interpersonally strong connections
and that edges with low overlap or zero overlap,
that kind of connect different parts of the network,
they tend to be weak,
they tend to be, um, you know,
um, don't have high, uh, strength.
So, uh, this is now, uh,
very cool because this intuition can serve us to identify this type of, uh,
bridge connections and be able to, uh,
separate and be able to identify,
uh, the clusters, uh, in the network.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 13.2 - Network Communities.txt
What we have seen so far is social science explanations,
why clusters, uh, appear in networks.
Um, and what do we are going to do next is to start talking about
methods that build on the intuition I have just explained and actually,
uh, identify, uh, these clusters in networks.
So, uh, let's start with what we have learned so far, right?
Like Granovetter's theory suggests,
that networks are composed from tightly connected,
uh, clusters, or sets of nodes.
Um, and the connections inside the clusters are strong, uh,
interpersonally, socially, and these connections across the clusters are weak,
are more like acquaintance uh, relationships,
we can call these clusters also communities,
we call them groups,
we call them modules.
It is kind of all the same, right?
And, uh, we are going to use the word network communities to
refer- to refer to sets of nodes with a lot of internal connections and few,
uh, external ones, meaning a few to the rest,
uh, of the- of the network.
So now the question will be given- given a, uh,
a network, how do we find these,
uh, densely connected groups of nodes?
Ideally, we would like uh, uh,
then these densely connected groups of nodes to then
correspond to real underlying, uh, social communities.
So let me give you a few examples of this.
So for example, one, uh,
most famous and most used social network is called Zachary's Karate Club network.
And Zachary was also a PhD student and he was studying,
uh, social relationships in a university karate club.
Um, and this is the social network of that,
uh, uh, university karate club.
But what was interesting is that during the study,
the conflicts led this karate club to split into two groups.
And what is interesting is that actually here the split is
denoted that circles went and created a new karate club,
and the squares, uh,
remained in the old karate club.
And what is striking is that basically this network kind of split in the middle, right?
Like the circles went on one end and the squares went on the other.
So it's not that, you know,
a random set of nodes will decide to form a new club.
But it's kind of one part of the network separated out- itself out, right?
And actually, the- the- the way you can automatically find this,
uh, this split is to simply say,
how do I split the nodes of this network into two sets
so that the number of edges that crosses between the two sets,
meaning goes from one set of nodes to the other, um, is minimized.
All right, So basically this was one of
the first examples that basically communities happen based on
the underlying network structure and that you can predict which humans are going to join
one team and which humans are going to join
the other team based on the social relationships between them.
Perhaps that is kind of obvious and well known to us today.
But, uh, back in '60s,
this was, uh, totally miraculous, right?
It wasn't at all expected that it's actually social connections that
make- that play such a big role in how groups and communities form.
Another example of groups and communities is in a very different part of the world.
So it is in, uh,
online advertising where basically if you think you are a Google or a Facebook, um,
you have advertisers who say,
who- where do they want their ads to be shown.
So you can create this bipartite graph where you put, uh,
advertisers at the bottom and you put,
uh, queries or keywords,
uh, on the other side.
And then here I'm kind of showing the adjacency matrix where a dot means that
a given advertiser wants its ads to be shown when a given query,
a given keyword, a given person with a given interest, uh, shows up.
And what you'll find out is that there are
these well-defined dense clusters where, you know,
groups of advertisers, um,
bid or advertise on common keywords or on common, uh, interests.
And you know, here is one example of, uh, you know,
the gambling set of advertisers who are gambling on people,
who are advertising on people interested in gambling queries.
So it allows you to identify
micro markets, subgroups in this type of online advertising space.
Another example is, you know,
imagine you take a network of, uh,
NCAA University football teams
and you connect two teams if they play games with each other.
Uh, and here's a visualization of the network.
And the question is, is that any structure in how teams, uh, play each other?
And if you run these community detection methods,
you would actually identify the- the groups.
Here they are visualized, right?
So notice this network is the same as that network, right?
So this is just visualize the network.
This is what the visualization comes up with.
It doesn't seem that is any real structure here, right?
Maybe you say, oh, there seems to be something here and maybe there is a cluster here.
I don't know, maybe there's something here.
It's very hard to say.
But after you apply a community detection method,
it will actually go and identify,
um, the clusters and here they are.
And what is interesting and- right,
is that they actually exactly correspond to these conferences in which, er,
teams are organized where teams play each other
inside the conference more than with other conferences.
And we even have a couple of themes that are-
that are not part of any community because they
are part of this Independence conference
and they just kind of play the two different teams.
So this is how, for example,
we can extract structure of them out of the network,
even though initially the structure is not obvious.
To give you another example now is to say,
how do we formalize this, right?
How do we identify the sets of tightly connected, uh, nodes?
And the way we are going to do this is that we are going to specify,
um, a metric that we call modularity.
And we are- and this metric will measure how well
a network is partitioned into communities.
So given a partitioning of the network into groups of nodes,
let's assume that right now somebody gave- gives us this partitioning.
We are going to compute this modularity score q,
and if we have the modularity, uh, score defined,
then what we are going to do later is we are going to say,
can I search over,
can I find a very good, uh,
set of partitioning such that my modularity score will be as high as possible.
So that's what we are going to do later.
But now, let's assume that, uh,
groups are given to us, um,
and we want to estimate how good of a clustering are we having.
And the way the modularity operates is the following, we say,
modularity Q will be proportional to the summation
over all the groups where for every group,
I want to ask how many edges are there between the members of the group?
How many edges are within the members of the group S?
Minus how many edges would I expect between this,
h, group S, um,
in some null, uh,
random null model, right?
And if the group S has much more edges
between its members than what is expected at random,
then we have found a strong significant cluster.
And now you know what is the total modularity of
the network is the sum over these modularity scores,
uh, of individual clusters.
Right? So because we have these expected number of edges within a group s,
we need a null- we need a null model, right?
We need a random graph null model.
So now if we go back to our subgraph mining,
we talked about Erdos-Renyi as an example of a round- of a- of a model.
Um, and we also talked about configuration model as an example of a null model.
So let me now tell you, uh,
and remind you about the configuration model,
right, which we already talked about,
I think two lectures ago when we talked about, uh, subgraph mining.
The idea is the following;
given a real graph G on N nodes and M edges,
we want to create,
uh, a random network G prime.
And we will refer to this random network as a rewired network because essentially it
will mean that we every node keeps its degree number of connections constant,
but connects to random nodes rather to- than
to the ones that they are really in- connected to in the network.
So this means that our network,
we'll have the same degree distribution,
the same degree sequence,
but it will have random connections.
Um, and we are going to consider graph g as a multigraph, right?
We'll allow multiple edges to exist,
uh, between the nodes.
So now you can say,
I have these nodes,
they have these spokes.
These are kind of these rough edges.
And now I wont to randomly connect,
uh, these, uh, endpoints.
And of course, maybe between a pair of nodes,
I will allow multiple edges because perhaps both of these two end points,
randomly, you know, by chance decide to
connect to these two end points so it'll be kind of a double-edge.
But for the purpose of this, uh,
discussion right now, that's completely fine and okay.
So then, you can ask yourself,
what is the expected number of edges between a pair of nodes i and j,
where node i has degree, uh,
k_i, and node j has deg- degree k_j.
Uh, and the way to derive this equa- er,
expression is the following.
You say, um, what is the total number of,
uh, edge endpoints?
Basically the spokes.
Number of spokes is 2 times n, right?
Every edge has two end points.
Every edge gets cut in half.
If I have m edges,
then I have 2 times m, uh, end points.
This is why we have this guy- this thing here.
Then what is K sub j, right?
K sub j is the degree of, uh, node j.
It's the number of spokes it has.
So now I say, um,
for every node- for every spoke of node i,
I randomly pick another spoke.
So node, um, er, k_j,
accounts for k_j divided by 2m fraction of all available spokes.
Right? Because this guy could also decide to link to itself or whatever else, right?
So now basically I say out of these k_i different tries, different random, um,
end points selections, uh, for, er,
each one of them has the probability k_j divided by 2m to connect to node j.
So now, um, if I multiply these together,
I basically say that the expected number of edges between I and
j is simply the degree of i times degree j divided by 2m.
Right? So basically what this means is that the probability or the expected number
of edges is simply equal to the product of the degrees,
uh, of the nodes, uh, uh,
um, that we are interested in.
All right? So, um,
we have this very elegant relationship, uh,
about the expected number of edges between a pair of
nodes under this random configuration model,
where nodes keep their degree,
but the edges are assigned, uh, randomly,
which basically means these endpoints get randomly connected uh,
with each other and we're leaving this kind of,
uh, multi graph, uh, world.
Right? So now that we have this, uh,
expected number of edges between, uh,
i and j, then I can say, you know,
just like as a- as a- as a- as a side,
uh, calculation, I can say, okay,
so what is the total expected number of edges in this,
uh, graph g prime?
So basically I'm saying,
let's sum over, um, all the nodes i,
all the nodes j. Um,
so all pairs of nodes and ask what is the expected number of edges between them?
And, of course, I have to multiply this by one-half because when I go over all pairs,
it means I will count every edge twice because i, j and j,i will be counted twice.
So if I work out these summations explained here, basically,
I- I get it's a summation of the,
uh, uh, the degrees, uh,
times another summation over the degrees, uh,
the sum of the degrees is two times n. The sum over
the degrees is two times n. And here I have divide by,
um, 1- 1 over 2 times n that comes from here times one-half.
So I get, um,
2 times m times 2 times m divided by 4 times m. Uh, you know, uh,
4s cancel out, one m cancel out and I'm left with, uh,
m. So this means that this mo- this un- this model,
both the degree distribution and the total number of edges, uh, will be preserved.
And the expected number of edges between a pair of nodes is de- determined, uh,
by this formula, k_i times k_j divided by 2m.
So now let's go back to the modularity.
So in modularity we said,
we have a number of edges between the group s, uh,
minus the expected number of edges within, uh,
the group s. So notice that we are only interested in the edges inside the group,
and we are not explicitly minimizing for the number of edges that cross between groups.
This is kind of implicitly accounted for, uh, in modularity.
So how do we write this out now,
given what we learned about configuration model as our null model?
So we are going to write the following.
We are going to say modularity, uh,
of our group, uh,
S in, uh, graph, uh,
G is simply a sum over the- uh,
all the pairs of nodes,
uh, in the group.
This is, uh, where the,
er, that pair of nodes is connected.
This simply counts number of edges between the groups.
And the second theorem says, ah-ha,
for every pair i, j,
I'm going to multiply their,
uh, their degrees, divide by 2m.
So this is the expected number of edges between a pair of nodes.
Right? So this is basically now saying,
what is the real number of nodes minus
the expected number of nodes over all the pairs of nodes in a given group?
And now I sum this up over all the groups, little s, uh,
from the partitioning of nodes into grou- into communities
into groups, capital S. Um, and this, uh,
factor 1 over 2m is, uh,
is a normalizing constant so that
our modularity metric Q will have range from minus 1 to 1.
Right? If all the edges are inside the group,
um, uh, uh, uh, uh, group s,
and, uh, uh, somehow we would expect very little edges,
uh, inside that group,
then modularity will be- will be very close to 1.
And, uh, if we have
kind of an anti-community where we ex- where we have no edges between the- uh,
uh, between the group.
But based on the degrees of those nodes,
we would expect a lot of edges,
then the value will be negative, will be minus 1.
So as I said, the modularity can take,
a value from minus 1 to 1.
It is positive if the number of edges within the group
exceeds the numb- the expected number of edges.
And in reality, in practice, uh,
if the modularity Q is greater than let's say, 0.3, 0.7,
this means that the graph has a significant community structure that we have identified,
uh, really strong, uh, clusters,
if our modularity is in this range.
So let me recap.
Uh, modularity Q is defined over an undirected graph and our partitioning of nodes s. Um,
and it is intuitively defined as a summation over all the groups,
number of nodes between the members of the group,
minus number of, um,
expected number of, uh,
edges uh, between the members of the group.
Uh, given- using the configuration model as a null model,
we then instantiate modularity using the following formula,
where we basically say for every group s,
take pairs of- all pairs of nodes from the group.
Ask whether a given pair is connected.
So this summation, we now count,
uh, the number of edges uh,
between the members of the group, minus, uh,
degree of one node, the degree of the other node,
divided by twice the number of edges in the network.
So this is the expected number of edges between i and j,
um, uh, under the configuration models.
So these difference tells us, um, er, er,
how- how many more- what is the difference between
the true number of edges and expected number of edges,
uh, inside a given group.
And now we sum this up over all different groups and we normalize it.
So this is how,
uh, we can, uh, write it.
Now that we have the modularity score,
we basically have an objective function.
So now the question is,
can I identify clusters' communities by maximizing this modularity score,
by basically maximizing, uh, this metric?
So the question will be, uh,
that we're going to address next is, if I, uh,
search over these groupings,
can I- how do I find, uh, uh,
uh, sets that have high modularity score?

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 13.3 - Louvain Algorithm.txt
So the algorithm that allows us to identify
high modularity score communities is- it's called uh,
the Louvain Algorithm, from the
University Louvain in Belgium.
That's why, uh, this name.
So it is a very scalable and very popularly- very popular algorithm.
Kind of it's a de facto thing you would use if you want to
partition the network into a set of clusters.
So Louvain algorithm is a greedy community detection algorithm that uh, scales uh, um,
ah, n log n,
where n is the number of nodes in the network so it can uh,
um, scale to large networks.
It supports weighted graphs, uh,
and it provides us hierarchical communities.
So it doesn't only provide us clustering at one level,
but it provides us kind of clustering of clusters.
So we get this notion of a tree or a dendrogram,
how individual nodes join into groups and how
these groups can then be further joined into the super groups uh, and so on.
Um, and as I said,
it is very widely used,
there are fast implementations available, uh,
It works very quickly,
ah, and works well in practice.
It finds uh, clusters,
communities that have a high modularity.
The algorithm operates in two phases,
where basically we wanna greedily assign the nodes to communities to maximize modularity.
Ah, and each uh,
iteration, each pass contains two phases.
So in phase 1,
modularity is optimized by allowing only local changes of nodes to communities,
so basically nodes get a chance to change community memberships.
And in phase 2, these identified communities are aggregated into
the super-nodes to build a new super-node network so that then another- um,
another part of community detection can be run.
So the idea is the following, the input network.
Uh, we start with an input network,
we optimize in phase 1 the modularity of it,
so we find clusters,
then we join these clusters into super-nodes to create this aggregate network.
Now we have a new network over which we can again run phase 1 to cluster it further.
In this way, we find clusters, clusters of clusters,
and so on, and we hierarchically uh, cluster the network.
Um, so now let me explain how phase 1 works.
Phase 1, we start by putting each node in the graph into a distinct community.
So each node says,
I am my own cluster,
I am my own community and each node i,
for each node i, the algorithm performs two calculations.
It computes the modularity when putting
this i into a community with some other neighbor j.
So basically we take the node i and say,
what if I put node i and put it into the community of node j?
Would that increase modularity?
And the idea is that i,
now for a given node i,
I try to put it into the commu- the same community
as every single of its neighbors is already a member of,
um, and whatever increases the modularity the most,
that's where I put i,- that's the community I put i into.
Um, and this phase 1 uh,
would kind of iterate all of the nodes until
no movement is possible that increases modularity.
So, th- the first phase stops when the local maxima in modularity is obtained.
Basically means no individual nodes can
move to a different community to improve modularity.
Um, note that the output of
the algorithm depends on the order in which we consider the nodes,
uh, in practice, it turns out that the ordering is not important.
So you fix some random ordering and that's how you process nodes.
And then for every node, you say,
what if I join you with the community with neighbor 1, neighbor 2,
neighbor 3, see which of these joinings increases the modularity the most,
and you move the node there.
If none of the moves increases modularity,
then you just don't move i to the community of node j.
So, um, what is elegant is that this modularity gain, right?
This Delta modularity where you say,
what if I take node i and move it from
the current community D to the new community C. Um,
this can be computed very efficiently.
And the way you can compute it is that you split it out into two terms.
One term is how will modularity change when I take node i and I
take it out from the cu- current community D?
And then how is the modularity going to change when I take
the same node i and put it into the new community C?
So the way you can think of picture is that, you know,
I'm right now looking- looking at node i,
let's say node i is assigned to some community D,
I pick some of its neighbors,
maybe I pick this uh, node uh,
here that belongs to the community C. So I say what happens if I
take i and kind of move it to the community C?
So first is, I have to compute the Delta modularity by taking i and moving it out of D,
and then I need to compute the Delta modularity for merging i back into the community uh,
C. And summing these two terms will give me the Delta modularity,
the increase or decrease in modularity when I move D uh, from uh,
when I move i from D to C. So let's derive
this Delta modularity when I move i into the community C. First,
we need to derive the modularity of uh,
community C as it is.
Let's call this Q of C. And the way we
derive this is we need kind of two sufficient statistic.
One is the sum Sigma inside which is the sum of the links, number of links,
or some of the link weights between the members of C,
um, and then um,
Sigma_tot will be the total number of links that all these nodes have, right?
So this is only inside the members,
and this is inside plus outside, right?
So here, I'm only summing degrees, uh, uh,
I'm only summing edge- counting edges that are inside the members of the C,
and when I compute the total,
I also account for all the edges.
So I just sum up the degrees of the- of wherever the edges go.
So now I have these two quantities, Sigma_in and Sigma_tot,
so uh, now I can rewrite modularity in terms of Sigma_in and Sigma_tot, right?
So here is modularity of a given community C,
it's this 1 over 2m we had before,
double summation over all pairs of nodes in C,
whether they are connected minus the normalized product of their degrees.
Um, if I distributed the summation inside,
I see here that this is basically now the number of edges between the uh,
members of the- of the group.
So this is Sigma_in divided by 2m,
and what I have here is a summation of the degrees of the nodes uh,
in C. Another summation of the degrees of nodes in C divided by 2m squared.
So that is uh,
Sigma_tot divided by 2m uh, squared.
Right? So uh, this is a- and- and this square
here comes because I take this 2m and distribute it inside.
So this means now that QC is large uh,
when most of the total links are within the community um,
and uh, um, very few cross to other uh, communities.
So now that we have defined, ah,
computed the modularity of C,
now we need to say how does this modularity change,
when we take this node I and put it into C. And, uh,
here we are going to use this, ah,
notion of, ah, um, uh, k_i,
in which is the number of edges node- that node i has,
to other members of C. And k_i is now the total,
uh, degree of node i, right?
So basically these two, um, uh,
these two, ah, terms, uh, are, ah,
equivalent or analogous to a Sigma in and a Sigma tot, right?
Is the total degree versus degree to oth- or the number of edges to other members,
uh, of C. And now,
ah, if I write this out, right,
so the, uh, the, uh, the,
the modularity I had before was,
um, uh, modularity of, of C,
that we have already defined plus the modularity of this,
let's say, ah, isolated community i so, ah,
it has- i doesn't have an edge with itself,
so with 0 minus, ah,
k_i times k_i with 2m,
so it's k_i squared.
Uh, and now after I have moved, uh,
in to the community,
see this node i, what do I get is the, uh,
sum of- number of edges inside Sigma in increases by k_i,
in, and the, uh,
total number of edges,
now increases bu- or total sum of the degrees now increases for the degree of node i.
So this is how I can write it, ah,
write it all, ah, all,
all, ah, all together now.
And then, right here is we said Delta modularity after I move i into community C is,
you know, after minus before.
Ah, this was- this is after, right?
I have the sum of the, uh,
the in deg- the degrees inside the community is increased by this theorem,
the total number of degrees is increased by the degree of node i,
so this is the after modularity.
This is the before modularity we have defined,
and now if I simplify this,
I basically- this is the- this is the expression,
um, the expression I get.
And what is nice, it depends on this sufficient statistic Sigma in and Sigma tot,
as well as the degree of node i,
and the number of edges that i has, ah,
to the nodes of community C. So this was the,
the term that we have just derived is this one.
Um, and then we can analogously determine,
er, derive a similar expression that says,
how does modularity change when I take node i outside community D,
sum these up together and I get a Delta modularity.
So, now that I know how to compute Delta modularity quickly, ah,
I basically iterate between,
ah, for every node I try to see,
is there an opportunity to increase modularity if I move it, uh,
to the, to a different community,
I compute this Delta modularity, um,
and I move the node into some new community, C prime, ah, based on,
uh, based on the- in a greedy way,
based on how the overall modularity, ah, is increased.
And this is essentially how the first phase of this,
ah, Louvain algorithm will operate.
So now, ah, now that I have,
ah, reached a local, ah, optimum,
I have moved, ah,
nodes to different communities until the modularity stopped increasing.
Then I moved to phase 2 which is called the restructuring, right?
So now, I wanna take these clusters communities obtained in phase 1.
And I can- I wanna contract them into a super-node,
ah, and, uh, create a new network,
a next level network,
where super-nodes are connected if there is at least one edge between the nodes,
ah, of the corresponding communities,
um, and the, ah, weights,
ah, of the edges between two super-nodes is
the sum of the edge weights across all edges between their corresponding communities.
And now I will have a super graph.
And I simply go and run,
ah, phase 1, ah, again.
All right, so the idea is,
I have my original network,
I run phase 1 to identify clusters.
Now I contract each cluster into a super-node.
I connect two clusters,
if there is at least one edge, ah, between them.
And now this will be a weighted network where the,
the edge weights are denoted here,
so this will be the total number of edges between C_1 and C_2.
And this would be the total number of edges, ah,
between the members of,
ah, C_2 ah, and so on.
And now that I have the super graph,
I simply apply my,
ah, phase 1 algorithm again.
So the way this will work is,
ah, you know, to summarize,
I have my original network,
I pick a node and, ah,
initially I put every node into its own community.
Um, and then I ask, ah, a node,
what if I move you to the same community as your member node 2 is up.
How would that change modular- modularity?
What if I move you to the community of node 3,
how would modularity change?
What if I move it to a community of node 5?
How would, ah, modularity, ah, change?
So, as I, ah, as I do this, ah,
moves, um, I then decide to move it to wherever the modularity changes the most.
And after the pas- the, the phase 1, ah,
finishes, this is the assignment of nodes to communities.
Now, I create this, ah,
phase 2 where I create a super graph,
where I contract all the nodes of the same community into a super-node,
and then this the, ah,
the edge, ah, the self loop is the,
the to- the total number of- twice the number of edges inside, ah,
the cross edges are how many edges from one cluster point to, ah, another cluster.
This gives me a super, super network,
super-node network, and now I simply apply, ah, phase, ah,
1 again, by clustering this, you know,
here I would get the following two clusters,
again I contract into super-nodes.
So here I have now basically,
ah, the two communities.
So what this means is that this original graph can be first split into two communities,
the violet and green one.
And then, you know, the,
the green one can be super,
ah, ah, ah, ah,
again split into two more communities and the violet,
into two more communities.
And then each of these four communities is denoted in this network.
So we get this, ah, hierarchical, ah, structure.
Um, and just to give you an example,
if you look, for example,
at a, ah, Belgian,
ah, mobile network, right?
Eh, Belgium is a,
is a place where,
uh, there are two, ah,
people speak two languages,
there's a strong French community,
and there's a, a strong Dutch speaking community.
And if you look at the social network,
ah, of phone calls,
you see very nicely how the- basically the,
the country splits into two, uh,
separate parts where, you know,
French speak to French,
Dutch speak to Dutch,
and there is relatively little connections, ah, between the two.
So to summarize, uh,
we have defined the notion of modularity,
which, ah, gives me the overall quality of the partitioning of a graph into communities.
Um, and now, ah, then we talked about, ah,
the Louvain algorithm for modularity maximization,
where basically it is a greedy strategy,
where we start with every node belonging into its own cluster,
and then we merge- we remove nodes between
clusters so that the overall modularity keeps increasing.
After no moves are possible anymore,
we join the clusters into super-nodes,
and again, repeat the clustering,
and this way we get this kind of nice hierarchical,
ah, community, ah, structure.
Um, Louvain works really well in practice,
scales to large networks and, ah,
people, ah, really like, ah, to use it.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 13.4 - Detecting Overlapping Communities.txt
So now I wanna talk about the second method about, uh, community structure,
and we'll call this, uh, BigCLAM,
is the name of the method and this is for detecting
overlapping communities in networks, right?
So so far we made assumption about this type of structure of the network.
So like we have these clusters,
each one has a lot of connections and then a few connections across.
But in many cases, you know,
people- humans, we belong to multiple social communities at once.
So you can have these social groups,
uh, that overlap, right?
So how could we extract this type of overlapping community structure?
To give you an example,
this is actually a Facebook network of one of the students,
uh, uh, in my group.
These are his friends and these are collection- connections between his friends.
Um, and you can ask, you know,
what is the community structure of this network?
What kind of communities does this PhD student of- uh,
uh, from my group, uh, belong to?
Um, and you know, if you look at this, perhaps you say,
there is- I see one big group here,
maybe I see another one here,
maybe there is something down here.
What is actually interesting if you look at it and you ask the student to- to, uh,
put his friends into different social groups,
uh, here are the social groups he came up with.
He basically said, I- I- I have friends from four different social groups,
uh, and here are the social groups.
And of course, some friends belong to multiple of these social groups.
Um, and then you can ask the- the student to label these groups.
And he said, "Oh, these are my friends from high school.
These are the friends from an internship I did in a given company.
These are the Stanford friends I play basketball with,
and these are these other Stanford friends I play, uh, squash with."
Uh, and of course there are some people here, right?
That are both- that went to the student to the same high school,
they are now at Stanford and they play both basketball and squash with him.
And, you know, that have some from the same high school
who only- who did the internship with him at this company,
but are not currently at Stanford, right?
So we see how these, um, communities overlap differently.
Another thing that I wanna notice you in this picture is that
nodes are either solid or they are kind of gray.
Um, and the point here is what I'm trying to show is that for- for a given- uh,
for a given network,
uh, we can- for this given input,
we can ask our community detection method,
our BigCLAM to identify the clusters.
And the big- BigCLAM identifies these four clusters,
and then we, uh, color the nodes.
The node is full if it was assigned to the correct cluster,
and it is gray if it- if it was not assigned to the great- to the correct cluster.
Uh, and what is amazing here is basically that only on the network structure alone,
only on this unlabeled network structure,
we can assign nodes to the correct clusters,
to the correct social communities without knowing anything about them.
Which is remarkable and super impressive, right?
It's a super hard task.
So, uh, this was one example,
another example I wanna show you is in, uh, biological networks.
This is a protein-protein interaction network,
and if you identify functional modules here,
you see how these functional modules,
uh, overlap with each other.
So this is, um, very interesting to think
about overlapping community structure in networks.
So just to contrast what we have been talking about so far,
we talked about networks having
this kind of clustering structure where dense clusters, few connections across.
If we think about this in terms of the graph adjacency matrix,
then the adjacency matrix would look like this,
where basically you have one cluster, you know,
the orange one, you have the green cluster,
a lot of connections.
And then these areas of the adjacency matrix means, you know,
an orange node linking to a green node and there is, uh,
less connections here because we assume there is less connections across the two clusters.
In the overlapping case,
it actually- the situation gets much more tricky.
If you have two overlapping clusters,
then these red nodes here are in the overlap.
And if you think of it from the, um,
um, adjacency matrix point of view, this is the picture, right?
You have one cluster,
you have the other cluster.
And here in the overlap,
you have even more connections because these connections kind of,
uh, come either from one cluster or,
uh, the other cluster.
And the point here wouldn't be to say, oh,
there is this set of people that are super strongly connected with each other.
The point is to say, oh,
there are two clusters that overlap in the set in the middle.
So now, how are we going to detect,
uh, this type of structure, uh, in the network?
How we are going to extract communities.
Um, our plan of action is the following.
We are going in step 1,
define a generative model for graphs that is based on node community affiliations,
and we'll call this model Affiliation Graph Model.
And then we are going to define an optimization problem that, uh, will, uh,
optimize the log-likelihood of,
uh, the model to generate, uh, the graph.
So what we are going to learn today is an instance of, uh,
generative modeling of networks and how to feed a generative model to a given graph.
So this is, uh,
kind of very useful and very fundamental.
So first, let me define the model.
The model will be the following.
Uh, we will have a set of nodes at the bottom,
and a set of communities at the top.
And a node will be connected to a community if a given node belongs to a community.
So in this case, you know,
blue nodes belong to A alone, green nodes, uh,
belo- uh, belong to B alone,
and then the red nodes belong to both.
So they are connected both to A and B, um,
and this is how we are going to de- describe
the community structure of the- of the network.
Now, we wanna build a generative model which says, you- I'm given,
uh, uh, an assignment of nodes to communities,
I want to generate the edges of the network.
How do I generate the edges?
And we are going now to describe the generative process,
how to turn the, uh,
affiliations of nodes to communities into edges of the network.
Our model will have a set of nodes,
a set of communities,
and a set of memberships of nodes to communities, right? Like this.
And then we are going to assume that each community,
C up here, has a single parameter,
P sub C. And this parameter will have the following, uh, role, right?
So basically we'll say given the- given our- our model,
given affiliations in this community parameters,
we wanna generate the edges of the underlying network.
And we are going to say that nodes in community
C will connect to each other by flipping a coin with, uh,
bare ground with a probabili- that is going to create
an edge with probability C. So the way to think of this is,
you know, this pair of nodes says, oh,
we belong to the same community A,
let's flip this coin P sub A, and, uh,
if the coin says yes,
we are going to create a connection between us.
Why? For example, these two nodes here,
they belong both to A and B.
So they are going to flip the first coin and if the coin says yes, they'll create a link.
Um, then they'll flip the second coin and if the coin says yes,
they are going to create a link.
So basically if at least one of these two coins says yes,
they are going to, uh, link to each other.
So this means that long- nodes that belong to
multiple communities get multiple coin flips,
which means they are more likely to,
uh, link to each other.
So in some sense, if the- if they are unfortunate the first time,
they might- might be luck in creating
a connection next time for the second community they have in common.
So one way you can write out the probability that U
and V connect with each other is through this formula.
And let me just explain it.
So basically what this is saying is this is the probability that, um,
at least one of these coin flips with biases,
P sub C comes up heads, right?
So basically we are saying, let's go over to
all the communities that nodes U and V have in common,
for each community we flip a coin of each probability C. So 1 minus,
uh, P sub C is the probability that, uh,
the coin says, let's not create an edge.
Now multiplying this says,
what if you are super unlucky and for every coin flip,
the coin flip comes up and saying no edge.
So this is now the total probability that all the coin flips say no edge
and 1 minus that is that at least one of these coin flips came up,
uh, uh, positively and the edge is created.
So basically we say a pair of nodes is connected if at least one of these coin flips,
uh, was successful, and that's kind of the formula for this.
So what did we learn so far?
We learned that if you give me the model,
uh, specified by the nodes,
communities, the memberships, uh,
and the parameters, uh,
then we know how to generate a- a network.
Um, this model is super flexible because it allows, uh,
us to- to, uh,
express a variety of community structures.
We can create this type of traditional one that's- two dense clusters.
Um, with this type of structure.
We can create overlaps by saying this orange nodes belong to both communities,
so they are in the overlap or we could even have a hierarchical structure,
where we say, you know, ah,
um, violet nodes belong to A,
uh, green nodes belong to, uh, C, ah,
and then all the nodes belong to B so that there is this kind of A and B are
embedded or included are subgroups of the bigger group, ah, B.
So, uh, that's the way, ah,
the flexibility of this model to give you some, uh, intuition.
So what we know to do so far is,
how do we go from the model to generate the network?
What we'd like to do next is to go the other way around, right.
Given the network, we'd like to say, what is the model?
So essentially this is called maximum likelihood estimation,
where basically we'll say, we'll define the following reasoning.
We say, assume there is some model that nature uses to generate graphs.
And we will assume that AGM is the model that nature uses to generate graphs.
Then we are going to say,
let's assume that our real-world graph was generated by AGM.
So we are going to say,
what is the model F that is most likely to have generated my data?
So in our case we say, given a graph,
find the most likely, ah,
model AGM, ah, that have generated the data.
So what do we mean by model?
It means, we need to decide that the number of communities,
we need to assign- decide how nodes belong to communities,
and what is every parameter P_C of every community?
So we say, what are the values of these parameters such that
this graph that I observe in reality
would most likely have been generated by my model?
So now, as I said,
basically we want to fit our model to the underlying graph.
And the way we do this is using maximum likelihood estimation.
So basically given a real, uh, graph,
we want to find the parame- the model parameters
F which maximize the likelihood of our graph.
So the way I wri- say this is to say,
given some model parameters,
I wanna generate a synthetic graph.
And then I wanna, ah, ah, I wanna say, uh,
what is the correspondence between the synthetic graph, uh, and the real graph?
So in some sense, I wanna maximize the probability
of my model F generating the real graph, right?
So basically, I wanna be able to efficiently compute this, ah,
likelihood that F generated my data G and then I need to- a way to,
uh, optimize over F to maximize this probability,
to maximize this likelihood.
So let me now first tell you how likelihood is defined over a graph, right?
So I'm given a real graph G and I'm given a
model F that assigns a probability to every edge.
And I want to genera- ah,
compute the likelihood that my model F has generated the graph
G. And the way I can do this is to say, that is a graph.
Here's an adjacency matrix.
This is my input data.
I have my model and my model assigns a probability to every edge that can happen, right?
Every- the model says,
A has a self-loop with itself with,
ah, you know, probability 0.25.
Of what- node 1 links to node B,
uh, to node 2 with probability 0.1.
So now I can ask,
what is the total probability that this kind of
probabilistic adjacency matrix would generate a given graph G?
I can simply do this as a- as a product.
Uh, and I go, ah-ha,
I wanna go- I wanna go over all the edges of G. I wanna go over
all the entries of this adjacency matrix where there is a value 1.
And I want these coin flips to land up heads.
I want them to- I want to multiply these probabilities together because I want,
uh, you know, this will be,
uh, with, ah, will have value of 1 with probability 0.25 and so on and so forth, right.
Then I also want to go over all the entries that are missing.
So basically, over all the edges that don't exist in the network.
And here I want the- the- the coin to land on the tail.
I want no edge.
So I go over all the non-edges of the network and I multiply 1
minus the probability of edge occurring, right?
And this is how likelihood of a graph under a given model is defined.
Basically, I go over, ah, all,
ah, all the, ah,
all the edges of the graph.
I see what is the probability that the model assigns to this edge.
And I want the product of these probabilities to be as high as possible.
And then I want to go over all the non-edges of the graph,
over all the 0s.
And I wanna multiply together 1 minus the probability that the edge is there.
And again, I want these 1 minuses to be as high as possible.
So our likelihood will be high,
where the model will assign high probabilities to edges that appear in the graph.
This is this part,
and it will assign low probabilities to edges that don't appear,
ah, in my, ah,
graph, uh, G, right?
Because this is- this goes over nodes and edges of the G. And this
now gives me the likelihood that model F generated the graph,
uh, G. So now, um,
the- the question becomes,
how do I find my model?
How do I find parameters of F?
And the way I'm going to find parameters of F is that I'm going to what we call,
relax or extend my model a bit to account for membership strengths.
So the way I'm going to do this now is rather than saying that every community C has a,
um, has a parameter.
I'm going to, uh,
assign a membership to have a strength.
So this would be the strength of our membership of node u to community A, okay?
And, uh, if the strength is 0,
this means, node is not a member of that community.
Um, so now, how are we going to write out, ah, ah,
the probability that node u and v link to each- to each other, ah,
if they are a member of a common community
C. And the way we are going to parameterize this is to say
it is the strength- is a- it's a product of the strengths of memberships.
And memberships can only be a non-negative,
so 0 or more.
So it's a product of the memberships.
And, uh, I'll take the negative value of this,
uh, exponentiate it, and take minus 1 of that.
So this now means that this should be a valid probability,
it will be between 0 and 1.
Um, and the higher,
the stronger the memberships,
the bigger the number here will be.
So E to the minus negative would be something close to 0.
Which means the probability of link,
uh, will be higher, right?
So basically, if membership strengths are low,
the probability of the edge is low.
If membership strengths are both high,
then the product is going to be high,
which means the probability is going, ah, to be high.
So this is how we parameterize it.
So now, uh, nodes v and u can connect via multiple communities, right?
They can have multiple, uh, communities in common.
So the way we are going to do this is using the same formula as we had before.
We are going to say the probability that u and v linked to each other is that they
create- they link to each other
through at least one of the communities they have in common.
So this is again P_C that we have des- derived here.
And but now I say, out of all these communities they have in
common at least one of these coin flips has to come up,
uh, heads, has to create a connection.
And this is uh, very nice because it, uh,
allows me to simplify this expression by quite- by quite a lot, right?
So now this is what we had on the previous slide.
Probability of a link is parameterized the following way,
where P_C is, uh,
parameterized this way by the product of the strands of, uh,
node u and node v belonging to the common community
C. So now if I write it- if I write is all out, it- this is, you know,
it's 1 minus product or our common communities,
1 minus- 1 minus,
uh, e to the minus product of Fs.
So here is how it simp- simplifies this to,
1s kind of uh- uh,
subtract each other out.
Then what I can do, I can take this product and, um, uh,
distribute it inside and the product of, uh,
exponentials becomes a sum of the exponents.
So here it is and why is this elegant?
Because this now means that this is simply a dot product of- of- of vectors,
of community memberships of nodes u and nodes v. So
I can simplify this whole expression into simply saying probability is, uh,
1 minus e raised to the dot-product of the community membership vector for node u times
the community membership vector for node v. So
this is super cool because it's a very nice type expression.
So now that I have defined the probability of an edge between a pair of nodes
based on their vector representations of the community memberships,
now, you know, how will my graph likelihood look like?
It's a product over the edges times the product of 1 minus,
uh, probabilities of non-edges.
And I simply, you know inserted this in, this is here.
When I insert it here again, these, uh,
1s will, um, cancel out and I just get the exponent.
Now, if I take this,
uh- uh likelihood and further simplify it a bit,
I can write it in the following form, right?
It's basically rather than directly optimizing the likelihood,
we like to optimize the log-likelihood.
And the reason why we optimize log-likelihoods is for numerical stability, right?
Probabilities are small numbers,
product of small numbers are even smaller numbers and then
numerical instabilities and imprecisions started to play a role,
so you never work with raw probabilities,
you always work with logarithmic probabilities,
a logarithm of the probability.
So if I apply now, uh, a logarithm here,
and it is a valid transform because logarithm is a monotonic transformation.
So when I maximize the max- maximum is attained at the same position,
regardless of whether I- whether I pass this through
a logarithmic transformation because it's, um, monotonic.
So if I apply a log and the log products become summations,
so I get this summation over log 1 minus the first term plus a summation,
over the log x- the second term and log and exponent again cancel each other out.
So here only the summation survive.
And this is now our objective.
We'll call it the log-likelihood that we tried to, uh, optimize.
So we have simplified it, uh, a lot.
So how do we, uh,
optimize, uh, this likelihood F?
We simply start with random memberships, uh,
Fs, and then we iterate until convergence.
We basically, uh, update the membership F of
node u while fixing the memberships of all other nodes.
Um, and we can do gradient descent,
where we take small changes in F that lead to the increase in the log-likelihood.
Uh, and that is um- uh,
all- all we do.
And, uh, just to say one more is when we compute
this derivative- partial derivative of log-likelihood,
uh, here is the expression.
What we find out is that this depends,
it is a summation over the neighbors of node, uh, u.
And, uh, you see how we're now multiplying the,
uh- the- the- the,
uh, community membership vectors.
We multiply it with, uh,
community membership vector of node v. But
then the term that is very expensive to compute is
the second term because here the summation goes over non neighbors of node u.
And because usually nodes have small number of neighbors, uh,
this means that this summation goes over- over the entire network, right?
Basically this plus this is the entire network and this is too slow,
uh, in practice to do.
But we can efficiently approximate these, uh,
computing this expression by realizing that this blue part,
um, is, uh, slow to compute.
But we can expand it, um,
and uh- uh, make it,
uh, to compute much faster.
And basically the idea is very simple, right?
If you wanna sum, over, um, uh,
a set of nodes that are not neighbors of a given node,
then you can write out this summation the following way.
You say, let me sum up over all the nodes in the network,
but then only subtract the contributions from
the nodes that are actually neighbors of this node u. All right?
So this now is the summation over all nodes that are non-neighbors of node u.
It's basically all the nodes minus the u itself,
minus the neighbors of node u.
Uh, and the point is because mo- most of the nodes have low degree,
you can cache this part,
you just compute it once,
and then you only have to update these terms.
And this way, you can make your method, uh, scale, uh,
much- much faster and run uh- uh,
to be able to be applied to much bigger graphs.
So let me summarize,
uh, the part of this lecture- this part of the lecture.
We talked about generative modeling of networks.
We talked about how we can fit a generative model to
the underlying network using log-likelihood and gradient descent.
In our case, we have defined our generative model of networks,
uh, that is based on node community memberships.
So what this means is that by feeding our model to the underlying network structure,
we can then, uh,
get the co- node community memberships out of that model.
So essentially we are doing community detection by
fitting a generative model to the underlying,
uh- to the underlying, uh, graph.
So BigCLAM is a method that- that defines a model,
uh- affiliation graph model based on the- based on
the node community affiliations and then generates the underlying network structure.
So what we then did is defined the fitting procedure that says,
given that the underlying network structure and
the assumption that the network was generated by the BigCLAM,
what is the optimal assignment of nodes to communities such that,
uh, we- we get the underlying,
uh, real network.
Um, in this procedure of model fitting,
we defined the notion of log-likelihood, um,
and then said, you know,
what is the most likely model to have gen- generated this data?
And we were able to feed the model, uh, to the data.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 14.1 - Generative Models for Graphs.txt
So for today's class,
what we are going to talk about is generative models for graphs, right?
So far we were working on the assumption,
or we talked about methods where, uh,
given- where we said the graph is given and we wanna do some modeling,
learning, prediction, community detection on top of it.
What we are going to, uh,
look at for the next two lectures is a different problem.
It is a problem about how do we,
um, generate a graph, right?
How would we generate a synthetic social network?
How would we generate a synthetic economic network?
How would we generate a synthetic communication network, right?
How are- what are some of the processes that would
allow us to generate these types of graphs, right?
Um, and the way we can, uh,
we can do this is that we wanna generate realistic graphs using generative models, right?
So the idea will be that we are gi- given a,
uh, graph generative model and, uh,
we wanna be able then to, uh,
generate a synthetic graph that is somehow similar or,
uh, matches, uh, the real graph.
So, uh, this is what we would,
uh, like to do.
So why do we wanna do graph generation? Why should we care?
Uh, first is that we wanna understand how real graphs form, right?
So these- these generative models can provide us insights
about what kind of processes take place,
let's say in social science, in- in humans,
when we generate- when we create our own,
uh, social networks and social connections.
Uh, it allows us to make predictions in a sense, you know,
how do we expect this graph to evolve, uh, in the future?
Synthetic graphs are also very useful because they, um,
allow us to understand, um,
various kinds of processes,
uh, that lead to, uh,
generation of, uh, new graphs,
and then we can use them as datasets for understanding corner cases
and understanding the performance of various kinds of, uh, graph methods.
Um, and the last is about anomaly detection in a sense that we,
many times, require a null model,
a model that would say, uh-huh,
this is what I would expect.
This is the network I would expect.
Let me compare it now to the real world network and see what the differences are, right?
So we want these type of null models to then be able
to have a reference point for what is expected,
and then whatever your data we have,
we can kind of compare it to that, um,
expected, the reference point to the,
uh, to the- to the null model.
So, uh, the road-map for graph generation and for the lecture today is the following.
First, I'm actually going to talk to you about properties of real world graphs, right?
Because when I generate a- a graph,
I wanna feed some properties,
I wanna match some properties of the real network,
and the question is, what properties are
interesting and what properties should we try to match?
And then I'm going to talk about, uh,
the second part of the lecture about traditional graph generative models,
uh, where basically each comes with a different set of assumptions.
These models are relatively simple,
but they come up with a lot of insight and they really
allow us to kind of understand the network formation.
And then, uh, next week, Tuesday,
I'm going to talk about,
uh, deep graph generative models,
where we wanna learn the graph formulation process from data,
uh, and this is what we are going to cover in the next lecture.
So today's lecture won't be kind of, uh, machine-learning flavored,
it will be more about traditional graph generation and a lot of cool insights,
and then next lecture we'll be thinking about
graph generation as a purely machine learning process.
And of course, there is valuing both.
There is valuing formulating things as
pure machine learning problems where you say I wanna predict the graph, um,
and there is also valuing generating graphs through
these traditional models because that comes up with a lot of theory,
a lot of insights, and, uh,
much more kind of inductive bias into
underlying processes that could be taking place in these networks.
So, uh, the plan is to ask first,
what are some of the important properties of networks,
uh, that, let's say,
these generators should match or that we can
compare the generated network and the real network?
And in particular, we are going to characterize graphs,
we are going to measure graphs according to these four different properties.
First, we'll call degree distribution, clustering coefficient,
connected components, and, uh,
the shortest path lengths.
Um, and, uh, we are- we have introduced some of
these notions already at the beginning of the class,
so let's do a quick, uh, recap.
So first, degree distribution.
Degree distribution simply says,
what is the probability that a randomly chosen node will have a given degree?
Um, and simply this is just a normalized count, right?
You say, uh, it's,
in some sense, a normalized, um,
histogram where you say, uh-huh,
for every degree k,
I wanna ask what fraction of nodes has that degree?
And I can simply plot degree versus fraction of nodes with that degree.
And I can think of this as normalized histogram,
or if I ignore this uh- uh- uh,
factor N, then it's simply a histogram of the count versus degree, right?
How many nodes have a dif- different- have a given degree.
That's what is called degree distribution.
The second thing we already talked about is clustering coefficient,
which is all about trying to understand how connected are the neighbors of a given node?
Um, and in particular,
we say clustering coefficient of node i is simply defined twice the number of
edges between its neighbors divided by
the degree of the node times the degree of the node minus 1.
So clustering coefficient will have value between 0 and 1,
and the reason why it is this way is basically saying,
I have k neighbors,
so k times k minus 1 divided by 2 is k choose 2.
It's number of possible pairs of neighbors you can select.
So this is the- the total number of edges that is possible between a pair of nodes-
a- a- a pair of neighbors of a given node with
degree k_i and e_i is actually the actual number of,
um, edges that- that occur, right?
So if I have no edges between the neighbors,
then clustering coefficient is 0, if I, let's say,
have three out of six possible 1s,
then clustering coefficient is,
uh, one-half and if I have all the six,
uh, edges between the four neighbors that are possible,
then my clustering coefficient is 1.
And this is now defined,
uh, clustering coefficient of a given node.
Now if you wanna do the clustering coefficient of an entire network,
what people usually do is they simply compute
the average clustering coefficient across all the nodes,
or they define, uh- or they actually plot
the distribution of clustering coefficient, uh, values.
Um, so those are, uh,
two ways how to characterize this notion of how many
triangles- how often friend of a friend is also a friend, right?
You can say basically, how likely am I to see this kind of edges between,
uh, pairs of nodes that have a friend in common?
And in social networks in particular,
clustering coefficients tend to be very high because of
this triadic closure that we have
discussed in the last lecture about community detection.
Um, so these were the first two,
degree distribution, clustering coefficient.
The third one we will look at is connectivity, and in lecture 1,
we dis- we dis- defined this notion of the,
uh, connected components in the graph.
And we said that lets- one thing,
how we can simply characterize connectivity is to ask,
what is the largest connected component size, right?
What- what fraction of nodes,
um, belongs to the largest connected component?
So if I have some graph that may not be connected,
I'm asking what fraction of nodes are in the largest connected component,
where connected component is simply a pair of- uh,
a set of nodes that can reach each other,
let's say, on an undirected network.
Um, the way I find connected components is by simple, uh, breadth-first search.
And I would say that, you know,
giant component exists if- if this largest component is large.
I know more than half of the nodes belong,
uh, to this largest, uh, connected component.
And then the last thing that- the way we are going to characterize networks is
through what is called shortest path lengths or the notion of diameter.
So mathematically, we define the diameter of the network as
the maximum shortest path distance between any pair of nodes in the graph.
So you take- essentially to compute it,
it would- it would mean take any pair of nodes and find the shortest path between them.
Um, now the problem with this maximum is that you can have
a long string of edges somewhere and that will severely increase your,
uh, diameter of the graph.
So for real networks,
you wanna use this- some more robust measure of the diameter,
for example, average shortest path length, where you say,
let me go over all pairs of nodes,
i and j, that are connected,
and let me, uh- let me compute the average,
um, shortest path length between any pair of nodes.
Um, and, uh, you know,
many times if the graph is disconnected,
then the shortest path length between two nodes that
are in different components is infinite,
so you would only do this over the largest connected component,
or you would, uh,
ignore the pairs of nodes that are not reachable from, uh, each other.
But the idea is that you wanna get some sense of how many hops
it takes to get from one node to another on average,
uh, in this, uh, network.
So these are the now four different ways,
how do we mathematically, uh,
empirically characterize properties of a given network.
And just for the, uh,
case- for the kind of- for the rest of the lecture,
I'm going to use one realistic large-scale network that we are going to characterize,
and then we will say, can we come up with
a generative model that could generate this network?
So for example, what we are going to use is we are going to use, um, uh,
network coming from this chat application called Microsoft Instant Messenger.
So this was like a WhatsApp plus Slack, uh, type
application before all these- all these other services existed,
you know, uh, it has- er,
it had- at that time it had,
uh, around 250, er,
million, er, monthly active users,
about 180 million actually exchange- engage in
conversations that were more than 30- 30 billion conversations over a month,
so about, uh, 1 billion conversations a day,
and you know, uh,
hundreds of billions of exchanged, uh, messages.
So what we can do with this now is we can take this communication,
um, data and we can represent this as a graph.
So the way we are going to represent this as a graph is we're going to put, uh,
we'll have people, um,
and we will connect to people if they exchange at least one message, uh,
and here is actually the dots,
here it represent the geographic locations, uh,
of the users of these Microsoft Instant Messenger,
and basically what you see is that they come from or- all over the world,
um, except from North Korea, right?
It seems that, you know, there was no users in North Korea but everywhere else,
uh, there are some users of this thing.
Um, so in total,
we'll now have a network of 180 million people with
1.3 billion undirected edges, uh, between them.
So now let's start characterizing this network.
Let's start measuring it and say,
how does this communication network of,
you know, people all over the world exchanging,
uh, short messages look like.
So the first thing we can do is we can characterize the degree distribution.
So what I'm plotting here is simply the degree of the node.
This is number of different people
a given node exchange of messages with in a given month,
um, er, times and on the y-axis is the number of,
uh, such nodes with such degree.
So I- I plot the count,
so it's probability times the total number of nodes,
so it's kind of an unnormalized, uh, histogram, right?
Um, what do you see?
You see this super, funny, strange,
however you wanna call it, uh,
interesting, um, histogram, right?
It seems that basically that is this huge number, right?
Like notice these dots here of nodes that have the degree very close to 0.
So basically we have a lot of people that only communicate with,
I don't know, 1 to a couple of different nodes,
and then it seems that we have
all these other nodes here that have very large degrees but,
you know, there's very,
very few of them, right?
For example, you know, you have maybe only one person with- that talks to 2,000
other people and the- and the person or the bot or whatever this thing was,
uh, that talks to the most people was about 6,000, right?
So it means out of 180 million people,
there is one node whose maximum degree is only 6,000, right?
Out of 180 million possible people to talk to,
these bot only talks to, uh, 6,000.
So what is very interesting is that this histogram
is kind of very much axis-aligned, right?
It seems a lot of people or a lot of nodes with super small degrees and,
you know, just one node, er,
one- one count for every degree that is- that is,
you know, higher than just, you know,
this, uh, super small number.
Um, so this does not seem- kind of this plot does not reveal too much,
but what you can do is you take the same data,
but you just plot it differently.
The way we are going now to replot this data is we
are going to replot it on- on logarithmic,
uh, scales, on logarithmic axis.
So what we did now it's exactly the same data,
the same counts, the same degrees,
but now this axis is logarithmic.
Right here is 10, here is 100,
here is 1,000, and now all of a sudden you see this very nice shape, right?
You see that basically I have majority of the nodes.
Yeah, this is 10^7,
so that's, you know, 10 million,
um, 50 million nodes that, you know,
have degree 1, 2, 3.
So large majority of the nodes have degree less than 10,
and then you see how here at the end we have very few nodes.
You know, maybe 1, 2 nodes,
um, for each degree above,
uh, several hundreds and you know,
the largest degree here I said is, uh, 6,000.
So now you see this beautiful pattern appeared that before it was not obvious at all.
Like when I plot this on linear scales,
just the histogram, it's just axis-aligned.
I- I don't even know how to interpret it.
So right here, now on same data,
just the axis are different,
I'm basically plotting log degree versus log count.
I see this, uh, very nice, uh, shape, uh,
this kind of what is called a heavy-tailed or a power law, uh,
distribution that, uh, people like to,
uh, call this types of shapes.
So the second thing is how about clustering coefficient?
If we take this network and compute clustering coefficient,
we find out that the clustering coefficient of this network is, uh, 0.11.
So it means that, you know,
11 percent of the,
uh, of the neighbors,
uh, are connected with each other, um,
and that may seem a little,
but it's actually a lot,
and the reason why this is a lot is because you have
these super large degree nodes, um, right?
That have a lot of possible connections between the friends, um,
and the- um, actually it turns out that 0. 11,
uh, is- is quite a lot.
Right now, it seems like, "Oh, you know,
the clustering coefficient is between 0 and 1."
You know, 0.11 is kind of closer to 0 than closer to 1,
so it seems that clustering of this MSN network is low,
but because we will have null models,
we are actually able to establish that the clustering is actually quite high,
so let me now tell you why- why 0.11 is high and,
you know, why is it not low even though it seems kind of a tiny number.
So this is what we are going to, uh, figure out.
So this was clustering.
Now, uh, this- the third metric was connectivity, right?
How big is the largest connected component?
And what I'm plotting here is the, uh,
size of the connected components: how many nodes belong to
a given component versus number of components of such a size.
What you see is that
the largest connected component has about 200 over more than- uh, right?
This is 10 to the 8th is,
uh, 100 million, um,
so this is, you know, uh,
almost like close to 200 million,
so pretty much 99.9% of all the nodes belong to the largest connected component,
so the network is connected,
and of course then we have, you know,
some small number of very small components of, you know, 20,
30 nodes all the way to, uh,
let's say 1 million of nodes of- that are isolated,
so connected components of size 1.
So this means that our graph is well connected with a- with a very small number of,
uh, small- small- very small isolated islands,
but the giant component is definitely there and 99.9% of all the nodes, uh,
belong to, and then the last thing I want to mention is the shortest path length.
So here is the distribution of, uh,
shortest path lengths between different pair- uh,
be- across pairs of nodes.
So I ask here how many pairs of nodes,
uh, are at shortest path Distance 5.
You know, how many are at shortest path Distance 10 and so on,
uh, and here the y-axis is logarithmic.
So notice that most pairs of nodes are reachable, uh,
with each other in around, um,
5, 6, uh, 7 hops.
So it means actually that the average shortest path length of
this giant network of 180 million people living all over the world, uh,
is only 6.6, and it turns out that you can reach
90% of the nodes of this network on average in less than eight hops.
So it takes you eight friendships to get to,
uh, 90% of the nodes,
and just to give you another way,
how you can think of this is to say,
"Let's start breadth-first search at a given node."
Right? So at zero steps,
it is one node.
Now this node has degree 10,
so at Step 1, there's 10 other nodes I can reach, right?
Then these 10 nodes have further connect to other nodes,
so at Step 2, I am at 75 nodes,
and notice how the-, uh,
how the number- how the number of reachable nodes increases,
and here for example at Steps, uh,
6, 7, and 8,
I'm- I- I can reach, you know,
28 millions, 80 million,
52 million, and then again,
you see how this quickly, uh,
decays down where, you know,
the- the- at hop Distance 25,
I can still reach, uh, three more nodes.
So notice again basically this kind of shape here
demonstrated by this single node, uh, example.
So this is, um,
what I wanted to show in terms of the shortest path length, right?
It's even though our network is massive,
the- the amount of, um,
the- the shortest path length tend to be very, very,
very small, and this is what is called,
uh, the small world phenomena, right?
That basically, even though the network is large,
the diameter, the shortest path lengths are actually quite small in this networks.
So what we have seen so far is that there are,
uh, key properties of this messenger network, right?
The degree distribution is heavily skewed.
Average degree is- degree is about 14,
clustering coefficient is 0.11, um,
connectivity has one giant component with 99.9% of the nodes,
and the average shortest path length is 6.6,
and as I give you these numbers, right?
You should be asking yourself,
are these values expected, are they surprising.
You know, is there- should I make fuss about it or,
you know, we just forget about it.
Like is it interesting?
What does this teach us, right?
And in order for us to determine whether- whether this is
interesting is that we need- we need a null model.
So basically what we are going to look at next is
different null models on which we can
make these same measurements and then be able to say,
"Ah-ha, you know, messenger network has high clustering coefficient.
Messenger network has I don't know,
um, low- low- low,
uh, average path length."
Or something like that because we will have this reference point.
So what we are going to talk about next is talk about this reference points,
uh, these, uh, types of models.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 14.2 - Erdos Renyi Random Graphs.txt
So now let's start talking about these null-models,
generative models for graphs that can serve as reference points.
So the first- ah, we are going to talk about is the Erds-Rnyi ah, random graph model.
Basically mention- named after two famous Hungarian mathematicians who
developed a lot of mathematical theory around the random graph generation, er, processes.
And random graph model-
Erds-Rnyi random graph model is kind of the simplest way to generate the graph, right?
We have two variants of it.
One called G_np and the other one called G_nm.
G_np says it is an undirected graph on n nodes where
every edge appear iid with probability p. So basically for every pair of nodes,
I get to flip this biased coin p,
and I get to observe whether the edge is created.
And then a different version of this model is called G_nm,
where it's an undirected graph with n nodes and m edges picked uniformly at random.
What's the difference between the two models?
They both have the same number of nodes, um,
they both have the same number of edges in expectation,
ah, and they both have edges placed at random between the nodes.
So the point being because in G_np the edge creation is stochastic,
you know, in expectation I will have n times- n times p number of edges.
But because of some variance of this random process,
I may have a bit less or I may have a bit more while in G_nm,
I always have the same number of edges.
So now with this,
ah, simple generative model,
let's look at what kind of networks does this model produce, right?
So basically, the point is that these models are stochastic.
So it means that these random graph models do not uniquely determine the graph, right?
The graph is a result of a random process.
So we can have many different realizations for the same n and p value.
So here for example, I show you different random graphs, um,
generated by, you know,
n equals 10 and p equals 1 over 6.
So what we wanna do next we're gonna say,
okay, if I take this um,
G_np model and I generate random graphs from it,
you know, what would be the degree distribution?
What would be clustering coefficient?
What would be the average path length?
What would be the connectivity?
Do I get the giant connected component?
And what is beautiful about G_np,
the Erds-Rnyi random graph model,
it's that- it is that it is kind of so
simple that you can mathematically analyze its properties, right?
You can mathematically derive these quantities using algebra alone,
you don't have to do simulations and measurements.
So it's kind of the simplest model,
but it's very- leads to very rich, er,
mathematical literature, um, and very rich mathematical analysis.
So there are kind of- and it's an entire sub field
that studies random graphs arising from this model.
So, er, first, you know,
what would be the degree distribution of the G_np model?
The degree distribution of G_np model would- is binomial.
So if you diff- you say y here is like an argument, you know,
let P of k denote the fraction of nodes with a given degree k. Then if
you see how many nodes do I- would I have in the given degree?
You say, ah-ha, out of n minus 1 possible members of a given node,
I get to sample or pick k of them.
So n minus 1 choose k is the number of times I can select k nodes of out of n minus 1.
And then, you know, because I'm saying my deg- my node has to have degree k,
it has to have k edges, er, be created.
So the likelihood of that is p raised to the power of k,
and then the other coin flips have to result in not-edges so that would
mean it's 1 minus n minus 1 minus k, right?
So that's the probability of,
er, missing the rest of the edges.
And if you look at this formula,
this is exactly the formula for a binomial distribution.
If you look at binomial distribution,
it has this kind of bell-like shape.
So essentially, it's like a discrete analog of our Gaussian distribution.
It has, you know,
average- average or mean of the distribution is simply p times,
er, n minus 1.
So average degree in a G_np network will be,
er, p times n minus 1.
And if n is massive,
then this is essentially, er,
n times p. So this is about binomi- er,
about the degree distribution.
And again, notice that,
you know, in MSN network,
we didn't see like this bell-shaped like curve, we saw something very skewed.
So clearly, er, G_np does not
generate graphs with the degree distribution that is similar to the MSN network.
How about clustering coefficient, right?
Um, remember the definition of clustering coefficient;
it is twice the number of edges between the neighbors of
a given node divided by k times k minus 1,
where k is the degree of that node.
And remember also that edges in G_np appear iid with probability
p. So the expected number of edges between, um,
um, these neighbors, e sub i of a given node i is simply, er,
the total number of possible edges times the probability that, you know,
each one of them comes- comes up,
er, and actually materializes.
So the expected number of edges,
so the expected e sub i is simply k times k minus 1 divided by 2,
where k is the degree of node i times the probability
that actually this- this particular pair of nodes,
this particular edge appears,
and that is k choose 2 type number of such pairs, er, as written here.
So now if you put these two together, er,
what you get is you get the- the- the, um, ah,
clustering coefficient is er, er,
er relates to, er,
to the graph as average degree divided by the number of nodes.
So what does this mean,
is that the average- what,
we can conclude is that the average clustering coefficient
of a random graph will be small, right?
So basically, if you generate
bigger and bigger graphs and keep the average degree constant,
um, then, er, the clustering will decrease with the size of the graph, right?
And we could even take the values from MSN network and plug them here, right?
So average degree was 14, number of nodes
was 180 million, right?
So this means that, um, our,
er, clustering coefficient should be 10 to the minus 6, right?
It's 14 divided by 180 million.
Um, so, um, that's kind of what is the clustering coefficient of,
a Erds-Rnyi random graph.
So this means that clearly our MSN network is not a random graph.
And then the third property I wanna comment on is connected components, um,
and this is very interesting because in, er, um,
in G_np you can actually analyze how is the graph going to be
connected as a function of the edge probability p, right?
If the p is 0,
then the graph will be empty.
No, there would be no edges.
It could be just a set of isolated nodes.
If the p equals 1,
then every pair of nodes will be connected and that's a complete graph.
What the result is that the- the connected component is going to appear when p is,
um, is of the order 1 over n.
Which basically means as soon as the average degree gets above 1,
the giant component will start to appear.
And if the average degree is below 1,
there won't be this large giant component.
So we get these kind of phase transition behavior, uh, in G_np.
And then of course, you can also analyze what is happening as the,
er, edge probability is increasing.
And for example, you can find out if you know, er,
edge probabilities of the order 2 times log n divided by n minus 1.
Then at that point in time,
your graph will be- um, um,
er, won't have any isolated nodes.
It may still have a couple of disconnected components,
but every- every node in- will have some edges.
So the point being,
is that in G_np,
the largest connected component occurs exactly when the average degree is greater than 1.
And here's the result of a simulation where we change the average degree.
And here we are asking what is the fraction of nodes in the largest connected component?
And you see how largest connected component is kind of small, small, small.
But as soon as the average degree hits value 1,
which means that, er,
p is greater than 1 minus 1 over n minus 1,
this giant component starts to emerge, and you know what?
When the average degree is 2,
it's already 80% of the nodes in- is in the big, er, largest component.
So the point being, G_np is connected as soon as or has a large component,
as soon as, er, average degree is greater than 1.
So, er, what have we learned so far?
We learned that G_np has a binomial degree distribution.
We learned that clustering coefficient of G_np is average degree divided by
n. We learned that large connected component will
exist if average degree is greater than 1.
So now, let's talk about,
er, average, uh, path length, so the diameter.
So to be able to talk about diameter,
we need to define this notion of expansion.
And the way expansion is defined mathematically is to say for a given graph,
uh, it will have expansion Alpha.
If for every subset of, uh, vertices S,
the number of edges leaving S is,
um, greater than Alpha times,
uh, size of the S, right?
Uh, and here, uh, I have the- this minimum here,
just to account for the fact that S can be more than half of the size of the graph,
but if I assume that S is, uh,
less than, uh, half of the nodes of the graph,
so it's, er, the smaller part of the graph,
then the- the- the point is that if I pick any set of S nodes,
there would be at least Alpha times S edges sticking out of that set.
So, uh, equivalently, Alpha is the minimum over all possible subsets of nodes,
number of edges leaving that subset dividing by the size of the set,
and the point is that this is min- this is defined as the minimum overall, uh, subsets.
Um, so the way to think of this is to say here is a set S,
here's- here are the rest of the nodes that are not in S,
and we a- we ask how many edges cross between S,
uh, and the rest of the graph,
and this ratio between edges crossing and the size of S is called, uh, expansion.
So, uh, you know,
and the way you think of expansion,
you can think of it as a measure of robustness, right?
Uh, for example, Uh,
do I do mean- it would mean that if you want to disconnect l nodes, uh,
from the graph, well that means you need to- to cut alpha times l, uh, edges, right?
Because if you say I want to disconnect a set S from the graph,
then I need to- I need to cut alpha times, uh, uh,
size of that set number of edges so that
that- piece that set of nodes is disconnected from the rest of the graph.
Um, that's another way to think of a-
a- a- notion of expansion because it tells me how robust is the network.
So let me now give you some examples of networks with high expansion,
low expansion, and show you some- you know,
some- something in between.
So this would be a network of low expansion because I had this one bridge edge, right?
So basically, because Alpha is defined as a minimum over all, uh,
subsets of nodes, basically if I pick this as a set of S,
then, uh, this is the edge that sticks out of the set.
So this is an example of a graph with low expansion.
Uh, this is an example of a graph with high expansion, right?
Kind of everything connected to everything, uh,
out of every set S,
there is a lot of edges, uh, sticking out.
Um, and then, you know, something that's kind of in-between at
these types of networks with community structure where
you have the subsets with high expansion
and then perhaps the expansion between the clusters,
uh, is a bit, uh, lower.
So, um, you know,
so real networks will be kind of somewhere in-between these two extremes.
Um, that is a fact, um,
or one can mathematically show that for a graph on n nodes with expansion alpha for, um,
between all pairs of nodes,
then it means that, um,
for any pair of nodes in the graph,
there is a path length,
the shortest path length order,
uh, log n divided by Alpha.
So basically it means, um,
the bigger the graph,
the longer the the path, right?
We have log n, but also the bigger the expansion,
bigger Alpha, the shorter the path length will be, and, uh,
now given this fact what you can then, uh,
what is then also been shown is that for, uh,
graphs where, um, log n is greater than, uh,
n times p is,
er, greater than some constant,
the diameter of, uh,
G_np will be log n divided by log, uh, np, right?
So basically the way to think of this, if,
uh, the average degree,
right- sorry, if, uh,
average degree is greater than,
um, uh, log n,
uh, then the, um, then the, um,
diameter, uh, will- diameter of the graph will be, uh, order log n,
and this means that random graphs have good expansion,
so it takes a logarithmic number of steps of- from- of
a breadth-first search from
a given node to basically visit all nodes of the graph, right?
So basically the diameter is logarithmic in the number of nodes.
Um, and this is interesting because it means the diameter is
exponentially smaller than the number of nodes, right?
Because n- log n is kind of a,
er, is exponentially less.
Log n is- is exponentially less, um,
than m. So it means that shortest paths are in G_np, are very short.
They are only logarithmic,
uh, in the size of the graph.
So, um, this is the notion, uh,
of expansion, and it turns out that G_np,
random graphs have high expansion and that's why,
uh, the diameter is, uh,
order log n. Um, again,
there is, um, a super cool,
uh, theory and ways to prove this.
Uh, we can link to some papers, uh,
where you could kind of work out and- and learn how to make, uh,
these proofs, but doing them is kind of outside the scope, uh, of this class.
So to give you an example, here is the, uh,
number of nodes in a- in a,
uh, G_np graph here.
Um, uh, and this is the length of the average, uh,
shortest path, and here we keep the average degree constant.
So all these graphs have,
um, ah, different number of nodes,
but average degree in all of them is constant and you
see how basically the diameter grows very,
very slowly as the graph sizes, uh,
increases, and you see this- this basically is like a logarithmic, uh, shape.
So now that we have seen, uh,
what the G_np is and we've kind of worked out these, uh,
properties of it, let's compare it to the MSN network.
So, uh, let's compare.
For example, in the MSN network,
the degree distribution is heavily skewed, right?
If I plot a histogram,
it is just kind of axis aligned.
A G_np degree distribution would- has this nice bell shape.
So clearly the two don't match.
The average shortest path length in MSN network is 6.6.
In, uh, in G_np,
we said the average shortest path length will be,
uh, order log n, right?
Log, you know, let's say log base 10 of 180 million is,
you know, 6, 7, 8.
So it's close, right?
So we say, yes,
the MSN network has short paths and the G_np also has, uh, short paths.
So that sounds great. How about clustering coefficient?
Clustering coefficient of MSN network was 0.11,
and we've worked out that, um,
that the, um, clust- that clustering of a G_np graph would be average degree divided by,
uh, n, where n is the number of nodes.
If we- if we enter the average degree of MSN which is 14 and divide by 180 million,
we get, uh, we get average clustering coefficient to be,
um, 10, uh, 10 to the minus 8.
So clearly we are nowhere close, right?
We are, you know, uh, six,
er, orders of magnitude apart.
We are a million- you know,
we have missed the mark by a million times.
I mean, um, you coul- you cannot miss smaller, right?
It's like as- as different as possible,
so clearly totally different,
and then in terms of largest connected component,
we saw that in MSN,
largest connected component has 99.9% of the nodes, um,
in- in Erdos-Renyi, the largest connected component
starts existing when average degree is greater than 1,
MSN, average degree is around 14.
So yes, uh, you know that- that is something that is matched.
So basically what we can conclude is MSN,
um, and the G_np, uh,
can- G_np can match MSN in terms of shortest path lengths,
in terms of connectivity,
it totally misses clustering,
it totally misses degree distribution.
So why- why this was interesting is because is
now we know that MSN network is not a random network.
It's fundamentally different from the, uh, random graph.
So let's now, um,
see if we can extend or fix this G_np model to
match more of the properties of our MSN network.
Um, and one sentence I will add here it is,
even though we know we've worked with a single instance of a network, this MSN network,
it turns out that these properties are quite shared across huge class of
networks in a sense that you have skewed
degree distribution, that you have short- shortest path lengths,
that you have clustering coefficients that are
quite high and that the graphs are connected.
So these properties that the MSN network has are kind of universal across,
uh, real-world graphs.
Okay, so, uh, to summarize,
what have we learned so far?
You know, if you ask, are real networks like random graphs?
Uh, the answer is giant component,
yes, they- they- it behaves like in a random graph.
Every shortest path length also behaves like in a random graph.
Clustering coefficient, totally different,
degree distribution, totally different, right?
So the problem with the random graph model is
that the degree distribution differs significantly from real network,
that, um, and also that in real networks,
giant component, uh, does not emerge through some kind of phase transition, um,
and the other biggest problem is that,
um, random graphs have no local structure, right?
Like there is no friend of a friend is- is my friend type effect,
so there is no triangles.
The- the clustering coefficient is way too low.
So the question is,
are real- real world networks random?
Do they look like G_np?
The answer is no, no way.
However, this model is still very useful because it
serves as a first basic reference point,
whenever you wanna generate,
uh, a graph and,
uh, compare it to something,
you- you would use a G_np model.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 14.3 - The Small World Model.txt
Now that we have looked at,
uh, the G_np model.
Let's move on and talk about the small-world model,
and the small-world model,
um, what- the goal of it will be- it,
uh, it will try to- to give us two things.
It will try to give us the average shortest path length to be small,
but it will also give us high clustering coefficient.
And the- the point why this is interesting,
because these two forces kind of go one against the other, right?
You get shortest path length to be small.
If you have a random graph where we have a lot of
these random connections everywhere, right?
But then if you have a lot of random connections,
you have- you- you have low clustering coefficient as we see in, uh, G_np, right?
So if you want low diameter, uh,
be like G_np's, but the problem is you have no triangles,
you have no local structure.
So we have low clustering coefficient.
If you have, let's say
a very irregular network like this lattice where, you know,
uh, you have this triangle,
so you have quite high clustering coefficient.
Everything seems nice, but now the diameter of this thing will be long because it
takes a huge number of steps to get from one corner to the other corner.
So kind of the tangent is the following,
is that regular lattice type grounds they have
high clustering coefficient because nearby nodes are connected to each other.
The problem with the structure is that it has high diameter.
You know, on the other hand,
random graphs have no local structure,
have low clustering, which is bad,
but they have low diameter,
which we also find in real networks.
So the big question is,
can I have both?
Can I have a small diameter but still a lot of clustering coefficient,
a lot of this local clustering structure,
a lot of triangles,
a lot of friend of a friend is my friend type,
uh, structures in the network.
So, um, the point is that this clustering implies what we call edge locality, right?
It basically, um, it implies that friend of a friend is a friend, right?
So it means that if two people, um,
have a friend in common,
then that is more likely to be edge there.
But the problem is that this edge is now just
locally connecting two people that are already connected.
And you cannot use this edge to connect different parts of the network.
So your diameter becomes large.
And we saw that the MSN network has seven orders of magnitude,
larger clustering coefficient, then the corresponding random graph, right?
So it's a huge difference, right?
So 0.11 is massive.
It's a massive clustering coefficient, right?
And to give you an example in- in- in other- for other cases,
for example, this is a movie actor collaboration network.
Here the- I'm telling you the average shortest path length, uh,
on- in the actual graph versus
the random graph with the same number of nodes and the same number of edges.
Um, and you see that, you know, average, uh,
shortest path lengths correspond quite nicely
between the actual graph and the random graph.
While here I have clustering coefficient between the actual graph, and the random graph.
And you see that that is orders of magnitude difference,
at least one order of magnitude.
So at least factor 10,
if not, you know, a few hundred, um,
between the actual clustering coefficient,
and the clustering coefficient in the corresponding random graph.
And this is between movie collaboration networks,
uh, electronic power grid, uh,
you know, stuff that the ways wires are connected to provide power,
like power to Texas.
Uh, and the C. elegans network, uh, which, uh,
is now a network of how neurons in the C. elegans worm are connected with each other.
And again, in all cases you see we have relatively short
shortest paths as nicely captured by an Erdos-Renyi random graph,
but the clustering coefficient is far higher than what a random graph gives us.
So the controversy, or the contradiction,
or something that is unclear is- is the following, right?
The consequence of expansion,
is that shortest paths exist, right?
This means basically that networks that have high expansion, um,
have small diameter because we can,
uh, get it, uh,
if we, uh, by simply traversing, uh, the graph.
But the problem with- with expansion and problem when
you- when you have short paths is that you have no clustering.
You have no local structure.
Um, triadic closure, meaning friend of a friend is
my friend type behavior leads to this local clustering of edges, right?
You have this edge that is close to the friend of a friend.
But the problem is that with high clustering.
You run out of the edges that would serve us kind of shortcuts across,
uh, different parts of the network.
And the problem then becomes that your clustering,
yes would be very high because you close a lot of triangles.
But the problem will be that now your diameter is high as well,
because it takes a long time to navigate from one part of the network to the other.
So the big question is, can we have both?
Um, and here's the idea.
The idea is that we wanna interpolate between
regular lattice graphs and G_np, uh, random graphs.
And, uh, the model that allows us to do this is called the small-world model.
And it will actually lead to high- to high clustering coefficient and low diameter.
So basically we will borrow the best from both sides.
We are going to have local structure,
so we have high clustering coefficient and we are going also to
have shortcuts so that we have, uh, low diameter.
So how do we achieve this?
So G_np, um, has two, uh,
sorry, uh, the small- the small-world model has two components.
First is- we wanna start with the low-dimensional regular lattice like a set of nodes,
uh, on a circle.
And here a node connects with its immediate neighbors and neighbors of neighbors.
So that's why this will have high clustering coefficient,
but also notice it has high diameter because to get from one- one node to the other,
you have to kind of navigate around this, uh, circle.
So I have high clustering,
I have high diameter.
What I will do now is,
I will do in the second phase,
I'm going to introduce some randomness.
I'm going to introduce shortcuts.
And what this means is that I'm going to add and remove
edges to create shortcuts between the remote pairs,
uh, on the- on the lattice- on the circle.
So basically for each edge with probability p,
I'm going to move the endpoint to a random node.
So the idea is that, you know,
I pick a- a- a random edge and I take one of its, uh,
endpoints and select it at random and rewire that edge to the other end.
So this would mean that for example,
this edge from black to, uh,
from blue to black, I would rewire.
It still touches the blue,
but it now connects to some other, uh, random node.
And then this is p, right?
Then the way you can think of it is,
um, on the left,
I have my starting lattice where I have high clustering and high diameter.
On the- when I have p equals 1,
when all the edges are rewired,
then I have a G_np random graph model.
I have low clustering and low diameter.
And what turns out is that for some intermediate values of p,
I- I retain both.
I have enough shortcuts so that enough edges rewire.
So the diameter is small.
And I still have a lot of, uh,
these original edges present so that the clustering, uh, is high.
So basically this rewiring mechanism allows us to interpolate between, uh, the, um,
uh, high clustering, the regular network,
and the, uh, random graph.
And if we create a plot of the following where on the x-axis,
we create the rewiring probability,
and then on one side of the axis we- we
plot the clustering coefficient here in dashed line,
and on the other axis we plot average shortest path length.
And notice how clustering coefficient remains high even as
the- as the probability of rewiring increases and only then starts to decrease.
But the shortest path length drops very quickly as I keep increasing, uh,
the rewiring probability p. So it means that there is this range of
p where I have high clustering but low shortest path length.
And this is what we call the small-world, right?
It's- it's a network that is heavily clustered but still has short, uh, shortest paths.
So to summarize the small-world model,
um, you know, could we have a network with high clustering?
Uh, and the same time,
uh, be a small-world,
meaning have small diameter?
And, uh, the answer is yes.
And you know why?
Because you don't need more than just a few random links to bring the diameter down.
So basically the diameter,
the shortest path lengths are
much less robust property than the clustering coefficient, right?
For- to have a low clustering coefficient,
you need a lot of edges in triangles,
but to have low diameter,
you just need a couple of random shortcuts along distant parts
of the network and that will collapse the diameter very- very quickly.
So, um, what's the significance of the small-world model?
It provides insights into the interplay between the clustering and small diameter.
It captures the structure of, uh,
many, uh, real-world networks.
Uh, and it accounts for high clustering in- in those networks,
um, but what is still missing is,
uh, the degree distribution, right?
The- the- in- in- how- the way we defined the small-world model,
uh, all the nodes have the same degree.
You know, in our case,
I think all the nodes had degree 4 which each- which
is kind of unrealistic as we saw from the messenger network.
But just, um, basically through these two models that they have presented,
I wanted to show you how really you can- you can think of this
as a- as a way to explain formation of networks.
And you can think of it as a way to,
um, capture different properties of the network.
And think about what kind of processes might be happening in the real world,
in, you know, in our everyday social networks that give rise to the networks.
Uh, with properties that we actually observe, uh, in real life.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 14.4 - Kronecker Graph Model.txt
In the last part of the lecture,
I wanna now talk about the third generative model,
which we call Kronecker graphs, uh, generative model.
And this is a very different, uh,
model because it will be kind of much more, uh, mathematical,
in some sense, much less interpretable,
but it allows us to generate very rich, uh, networks.
So, uh, let me explain what the Kronecker graph model is.
So the way we will think about this is we'll think
about it from the viewpoint of recursive graph generation, right?
Can we think of network structure recursively?
And in particular, can we generate networks that have this type of self-similar,
uh, structure where, you know,
the object is self-similar if- uh, um,
if the object it's- is- is similar to a part of itself,
like, you know, the hole has the same shape as one of its parts.
Um, and the idea is that we would be able to then generate big graphs from small, uh,
building blocks, right, from- and in this way,
perhaps imitate different communities and growth of, uh, different communities.
And the Kronecker graphs will allow us to do this through this notion of
a Kronecker product that will allow us to
generate self-similar adjacency matrices, right?
So the idea will be that if- that if I start with some starting graph,
I somehow would recursively expand the starting graph,
uh, into bigger and bigger graphs.
So I wanna ge- have a small kind of generator,
small building block, and then I wanna
generate- use it to generate bigger and bigger graphs.
And the way this will work is, I will, uh,
take a small generator matrix like the one I have here,
let's call it K_1.
And then I'm going to, uh,
apply Kronecker product to- uh, to itself.
So I am going to do K_1,
Kronecker product K_1 to get now,
uh, a matrix, uh, K_2.
And then, you know, I can keep multiplying,
and I will get this very nice self-similar structure.
Uh, now, let's say, you know,
that has size 81 by 81.
And just notice that K_1 has this,
uh, pattern of, you know, um,
uh, 1s kind of along the diagonal band and 0s,
uh, off the diagonal.
And notice how now this adjacency matrix also has same structure, right?
I have this block of 0s,
and then I have this block of 1s,
but each of these blocks is similar, has 0s,
uh, of the diagonal and kind of has non-0s on the diagonal.
And then each of these blocks is again similar to the original block.
So that's kind of why these Kronecker graphs,
uh, uh, model will allow us to do.
So now, how do we define it?
How do I get what I just showed you on the previous slide?
We are going to define the Kronecker product of two matrices A and B the following way.
So the idea is, if I have two matrices, A and
B and I Kronecker multiply them, then basically,
the way to think of this is that I take the matrix B,
I put it into every cell of A and multiply with the entry in that cell.
So this means I can take two matrices,
uh, A and B, that have different sizes,
and the Kronecker product of them,
the size of the Kronecker product will be the product of the sizes, right?
So if, uh, uh,
matrix A has n rows and, uh,
matrix B has k rows,
then the number of rows of the product will be n times k,
and the same for columns, right?
Then the- you- the way you see now,
the product is simply you take B and you put that entire matrix into
the every cell of matrix A and multiply with an entry,
uh, at the- that- uh, cell.
So now that we know what is a Kronecker product of, uh, general matrices,
we can define a Kronecker product of two graphs by simply as
a Kronecker product of the corresponding adjacency matrices, right?
So, uh, now that we have defined this,
um, this basically goes- goes here, right?
I have a Kronecker- I have,
uh, first, adjacency matrix,
now I do Kronecker product of this adjacency matrix with itself,
and I get this type of structure, right?
I have zero here because if I take K_1, put it here,
and multiply it by 0,
I get a three-by-three block of 0s.
While in these other parts,
I get simply a copy of K_1 because,
um, because I have a coefficient 1 here.
So basically, now I get a matrix of size 9-by-9,
uh, where I have two blocks of size 3-by-3 of 0s,
and the rest are simply copies of this, uh,
matrix K. And now that I have a 9-by-9 matrix,
I could Kronecker multiply 9-by-9 with another 9-by-9 matrix,
and I would get, uh,
Kronecker, uh, matrix now on,
um, 81 nodes, so I have now a graph of- on- in 81 nodes.
So how are we going to define Kronecker graphs?
Kronecker graph is obtained by growing- uh,
by a growing seq- sequence of graphs by
iterating Kronecker product over this initiator matrix K_1.
So if I wanna get the Kth Kronecker graph K,
I'll simply take my initiar- initiator matrix K- K_1,
and I'm going to Kronecker multiply it with itself, uh, m times.
So basically, I'm taking the nth Kronecker power of the initiator matrix,
uh, to get the- to get the final graph.
Um, and of course,
and nobody says you have to take
one initiator matrix and just power it with itself m times.
You would have different-ish initiator matrices and multiply them to get a bigger graph.
So there's a lot of flexibility here.
To give you an example, uh,
here is an initiator matrix on four nodes,
here is the corresponding graph,
here is the adjacency matrix.
Uh, notice basically, uh,
we have self-loops, uh,
on the diagonal and then kind of,
uh, first row, first column,
uh, is basically the- the star node linking to other satellite nodes.
If now I generate, uh,
a Kronecker graph with this initiator here as the adjacency matrix,
and again, notice the self-similarity pattern.
Uh, here's a different, um, uh, uh,
initiator matrix with the corresponding adjacency matrix, right?
Again, I have this node,
uh, that links to all other nodes,
but then there is a triangle, uh,
here as well, um,
denoted in this part.
And now, notice as I create a Kronecker graph,
I get the same self-similar structure, right?
I get basically band of non-0s.
I have these two elements that are 0, corresponds to this area here.
I have this- this part corresponds to this part here, right?
So you see how I get
this recursive self-similar structure where basically this initiator matrix is- uh,
its structure is retained at the different, uh,
levels, and this is how we define the Kronecker graph.
Um, so far, we have defined Kronecker graphs as deterministic, right?
We started with a 0,
1 adjacency matrix and we generated a bigger adjacency matrix.
So what we wanna do next,
is we wanna make, uh,
these gra- these Kronecker graphs stochastic.
We wanna make it so that it generates random graphs.
And the way we generate random graphs with Kronecker graphs is that
rather than thinking of the initiator matrix as a- as a little,
uh, graph, as a little, uh, 0,
1 type matrix, we are going to think of it as a probability matrix.
So basically, rather than having K_1,
we are now going to have Theta 1.
And rather than ha- K had either an edge or no edge,
now, we will say that every edge has a probability between 0 and 1.
So theta is now a matrix of probabilities.
Then we are going to take this Theta 1,
and we are going to apply Kronecker power to it.
We are going to Kronecker power it with itself to get the kth power.
And then now that we have this probabilistic adjacency matrix k,
we are going to sample edges from it at random
according to the probabilities defined by this,
uh, in this- uh, in this matrix.
So let me basically give you, uh, um,
um, a picture that will explain what I mean, right?
So the idea is I started with some initiator matrix,
the entries have to be between 0 and 1,
and they don't have to sum to 1,
I Kronecker multiply this initiator matrix to get a bigger,
uh, probabilistic adjacency matrix.
Now, every cell here can be interpreted as a probability of an edge.
So in order to generate the graph,
I now simply traverse this matrix and flip a coin where, uh,
every entry tells me what is the bias of that coin,
and if the coin says, uh, yes,
if it lands on tail- uh, sorry,
on head, then we actually create a graph.
We- we create an edge in the graph,
and we then call this- this is now an instance sampled from this,
uh, probabilistic, uh, adjacency matrix.
Um, right. So the idea is once you have this probabilistic, uh, matrix,
you simply go and flip the coins according to the biases, um,
that are encoded in the values of the cells and this will give you a, uh, a graph.
It will give you, uh,
now an instance of a Kronecker graph sampled from this,
uh, stochastic Kronecker graph.
Uh, this is great because now we can generate graphs, uh, stochastically,
but all having kind of similar structure,
all being generated from the initia- the same initial, uh, probability matrix.
Uh, what is the problem?
The problem is that we have to flip all these coins.
And if the graph has size n,
which means this matrix has size n by n,
then we need to flip order n square, uh, coins.
Uh, and that's far too many coins to flip if I
wanna generate a million node graph because then I need to flip,
a million squared, uh, nodes.
So it will be- I have to flip 10 to the 12, uh,
coins, or if I want to generate a billion node graph, which is, you know,
nothing too large, then I would have to flip,
uh, 10 to the 18 different coins,
um, and that's unfeasible in practice.
So let me now tell you how to do this faster, uh,
using what is called a ball-dropping, uh,
or an edge-dropping mechanism, uh, and it's quite cool.
So here is how we are going to generate Kronecker graphs, uh, faster.
So think that you had this 2-by-2, um,
initiator- probabilistic initiator matrix that has, you know,
entries a, b, c, and d. Now,
if you Kronecker multiply this with itself,
the Kronecker product matrix will look like this, right?
The first entry will be a times a,
then it would be a times b,
b times a, b times b,
a times c, and so on. All right?
So that's one way to look at it,
is basically let- first,
you would generate all the cells and then you can flip the coins.
But what you can also notice is that we have this kind of hierarchical structure, right?
I have a, b, c, and d,
and then each of these four- four, uh,
quadrants has further a,
b, c, d in there, right?
So the way you can now think of this is is a two-level structure.
And the way you can say what is the probability of a- of a given edge,
it simply, um, you can fli- you can basically, uh,
separate it out as a set,
almost like as a diving,
dropping into the underlying adjacency matrix,
where at every stage you have to decide which of
the four quadrants do you wanna dive into.
So let me give you an example.
If this is my final, uh, you know,
this is my final adjacency matrix size I'd like to create,
so this would be number of nodes is,
uh, whatever is the number of rows and columns here,
then the way I can think of it is I can split this into four quadrants and I can say,
a-ha, you know a fraction of times I'm going to go into this part of the network,
b fraction of times I'm going to go here, c,
and d. And then basically you can pick again, uh, um,
uh, according to these values, a,
b, c, and d, one of these four quadrants.
And now that you have picked it,
you now then say, a-ha,
I- I- I have now picked this, uh,
this quadrant, let me again split it into four parts.
And again, let me pick which of
these four different subparts do I want to dive into and you would pick one of them.
And now you are still not all the way down at- at the bottom of the matrix,
you still have, um, um,
uh, to choose which cell you go to.
So again, you split this into four,
and again decide which of these four,
uh, to pick, right?
So, uh, what this means is that basically, rather than, uh,
flipping these, uh, coins kind of row by row,
you start at the- at the top,
and then you- you descend into one of
the four quadrants with probability proportional to a,
b, c, and d and you pick one of them and move into that part.
And now you say, a-ha, I will again,
I have four ways to go to and pick one of the four,
uh, ways at random and you'll move to the one part.
And then again, you say, Aha,
now I'm here, I wanna go deeper.
And you keep descending until you hit, uh, the, uh,
individual cell, and this is when you,
uh, when you stop and you put an edge.
Um, this means that you are going to land in a given, um,
in a given cell exactly with the probability,
um, uh, according to that cell.
The only difference is that you may get a couple of edges colliding, right?
You may land to the same cell multiple times.
If that happens, just ignore it,
uh, and try again.
And this gives you now a very fast way to generate a- a Kronecker graph, right?
So, uh, basically, this, uh,
edge-dropping or ball-dropping mechanism basically says,
um, take the initia- initial, uh, matrix,
whatever are the entries, uh,
normalize them and then keep dropping in, um,
in- keep descending in until you hit an individual cell and put an edge there, right?
Put a value 1 there,
and this would mean that you have connected nodes,
uh, i and j that are in a column,
uh, in the row i and column j.
And if you do this, what is interesting is that,
for example, with a very simple, uh,
adjacent parameter matrix only four values,
you can actually generate the graphs,
uh, that correspond to real graphs quite well.
Here is for example, the- this is now a directed graph in-degree, out-degree,
clustering coefficient, uh, diameter which is number of nodes,
uh, at a given distance.
Uh, these are some other properties that people care about.
And you can notice how Kronecker graphs in green and the real graph match really well,
and all you have to do is decide on these four different parameters, right?
Four different parameter values.
So basically with just four parameters,
we are able to generate, uh,
a realistic graphs using, um, Kronecker graphs.
Here the network we are trying to mimic has 75,000 nodes and,
uh, uh, more than half a million, uh, edges.
Uh, so the cool thing is now we are also able to model
degree distribution class- clustering as well as, uh,
shortest path, uh, di- distribution,
as well as some other properties with just four free, uh, parameters.
Here are the, you know,
the numbers of those parameters.
If you start with this initiator matrix,
you can basically generate this social network on 76,000 nodes and half a million edges.
So, uh, to conclude the lecture for today, uh,
the summary, so today we looked at these traditional generative models for graphs.
First, we discussed about, um,
what metrics do we use to characterize the graph.
And we talked about degree distribution,
clustering coefficient, um, the giant connected component,
as well as shortest path-length.
We started with the simplest possible generative model
called Erdos-Renyi random graph model,
um, and we- we saw that it generates shortest path-lengths.
Realistically, that it gives us connected graphs,
but it doesn't generate clustering coefficient.
So we then, uh,
talked about the small-world, uh, graph generator,
and we talked about how you only need a couple of
random shortcut edges that bring down the diameter but- but keeps the clustering high.
So this is what was in the- the significance of the small-world model.
And then we looked at,
um, at the different model called, uh, Kronecker graphs.
That's kind of more mathematical and, uh,
is defined as based on the Kronecker product of graph adjacency matrices.
We then defined the stochastic Kronecker graph, uh,
where the adjacency matrix is stochastic and we can then generate multiple instances,
uh, from the stochastic, uh, matrix.
Um, we talked and then- then last about this ball-dropping, uh,
mechanism that allows us to generate Kronecker graphs, uh, quick, um,
in a fast and efficient way so we can generate graphs with billions of nodes,
uh, without any problem.
So, um, with this, uh,
this finishes the lecture for today.
What we are going to talk about, uh,
next week is about deep generative models for networks, right?
So while today the models were kind of mechanistic with a lot of,
uh, insight from the network generative processes that happen in real world,
uh, the- in- on Tuesday,
we are going to talk about deep generative models where basically we will
say let's just formulate this as an- as a machine learning problem,
as a kind of complex, uh, prediction problem,
and all we care about is to generate a
realistic structure and we don't care so much about, uh,
whether, uh, you know,
what is the true underlying,
let's say real-world, uh, generative process.
So we learn how to generate graphs,
uh, from, uh, raw data.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 15.1 - Deep Generative Models for Graphs.txt
Welcome to the class.
Today we are going to
discuss deep generative
models for graphs.
So let me explain
that in more detail.
So, so far we talked about how
to classify nodes and edges
and perhaps entire graphs.
But now we are going to
talk about a new task, which
is the task of
generating the graph.
The idea is that we want to
have a generative model that
is going to generate
a synthetic graph that
will be similar to
the real-world graph.
An application of this
type of graph generation
happens in many
different places.
You can imagine that you can
represent molecules as graphs
of bonds between the atoms.
And then in this case,
you want to generate
novel molecular structures
based on this generative model.
Or for example in
material design,
you may want to generate
optimal material structures
and this is what you can do.
In social network
modeling, you may
want to generate
synthetic social networks
so that you can then use
them for various kinds
of downstream tasks.
And even in some other
applications for example,
if you think about generating
realistic road layouts.
If you want to think
about generating
realistic layouts of cities,
all these types of things
you can model as a graph
generation process.
And even for example, some
combinatorial problems
like the satisfiability problem
or the Boolean satisfiability
problem, you can generate
artificial instances
of that problem by representing
the satisfiability instance
as a graph and then learning
how to generate those graphs.
So in all of these
cases basically,
the goal is that
we want to learn
how to generate a
graph that is somehow
similar to the underlying
real-world graph.
And the field of
graph generation
has a rich tradition.
And the way this
started, it started
with the study of properties
of complex networks
like real-world networks.
And identifying what are
the fundamental properties
that these real-world
networks have
like power or scale-free
degree distributions
and also like the
small world property
and so on and so forth.
Based on these fundamental
properties of complex networks,
then there have been
a lot of development
of generative models
for graphs, which
generally fell into two camps.
One camp was very
mechanistic generative models
like the preferential attachment
model, that basically allowed
us to explain how could
certain properties
like the scale-free
property of networks
arise from this microscopic
preferential attachment type
model.
Another set of models
for generating graphs,
came mostly from the statistics
and the social networking
literature where basically, the
idea was that there is maybe
some, that there might be
some latent social groups
and based on those
latent social groups,
edges of the social
network get created.
And then the question is,
how can you take that model,
fit it to the data and
perhaps discover the groups?
However, today and
in this lecture,
we are going to
use deep learning
and representation
learning to learn
how to generate the graphs.
So in contrast to prior work in
some sense that either assumed
some mechanistic
generative process
or assumed some statistical
model that was motivated
by the-- let's say
social science, here
we want to be kind of
agnostic in this respect.
And the goal will be that,
can we basically given a graph
or given a couple of graphs,
can we learn how to gene--
what the properties
of those graphs are,
and how can we
generate more instances
of those types of graphs?
So we'll be kind of
completely general,
we'll just learn from the data
in this kind of representation
learning framework.
So this is one way how
we can look into this.
Another way how we
can look into this
is that, so far in
this class we've
been talking about the deep
graph encoders, where basically
the idea was that, we have a
complex network, complex graph,
complex relational
structure on the input
and you want to pass it
through several layers
of this representation and
a deep learning network that
at the end produces-- let's
say node embeddings, edge
embeddings, entire
graph embeddings, right?
So this is what we call
a deep graph encoder
because it takes the
graph as the input
and encodes it into some
kind of representation.
The task of graph
generation actually
goes in the other direction.
It wants to start on
the right-hand side
and then through a series of
complex nonlinear transforms
wants to output the
entire graph, right?
So our input perhaps will be
a little small noise parameter
or something like
that and we'll want
to kind of expand
that until we have
the entire graph on the output.
So we will be kind of decoding
rather than encoding, right?
We'll take a small
piece of information
and expand it into
the entire graph
rather than taking
a complex structure
and compress it into or
into its representation.
So we are talking about
deep graph decoders,
because on the output we want
to generate an entire network.
So in order for us to do
that, I want to first tell you
about kind of how are we going
to set up the problem in terms
of its set up, in
terms of its kind
of mathematical
statistical foundation.
And then I'm going to talk
about what methods allow
us to achieve the goal
of graph generation
using representation learning.
So let's talk about
graph generation.
Generally, we have two
tasks we will talk about
in this lecture.
First, is what we will call
realistic graph generation,
where we want to
generate the graphs that
are similar to a
given set of graphs.
And I'm going to define
this much more rigorously
in a second.
And then the second
task, I'm also
going to talk about is what
we call goal-directed graph
generation.
What basically you want
to generate a graph
that optimizes a
given constraint
or a given objective.
So if you are generating
a molecule you could say,
I want to generate molecules
that have a given property,
maybe the property
is solubility,
maybe the property is that
these molecules are non-toxic.
And you say I want to generate
molecules that are non-toxic.
So how do I generate the
most non-toxic molecule?
Or how do I generate the
molecule that is most soluble
and still looks like drugs?
Imagine another case
I was giving example
before if I want to generate a
road network-- a realistic road
network of a city, that is
a graph generation problem.
I could say, I want to
generate the optimal road
network of a city,
right, whatever
the optimality constraint is
that is kind of, we assume
it is given to us, right?
So that's what we mean by
goal-directed graph generation
where you want to generate
a graph with a given--
with a given goal that optimizes
a given black box objective
function.
So that's the two parts
of the lecture today.
But first, let's
talk about how do we
set up this graph generation
task as a machine learning
task?
So we are going to proceed
in the following way.
We are going to assume that the
graphs are sampled from this p
data distribution.
So basically nature is
sampling from p data
and is giving us graphs.
Our goal will be to learn
a distribution p model
and then be able to learn how
to sample from this p model.
So basically, given
the input data,
we are going to learn a
probability distribution p
model over the
graphs, and then we
are going to sample new
graphs from that probability
distribution.
And right, and our
goal somehow will
be, that we want this p model
distribution to be as close as
possible to this unknown
p data distribution
that we don't have access to,
that only nature has access to,
right, the data set creator
has access to this p data.
We want to approximate
p data with p model.
And then as we have approximated
p data with p model,
we want to draw
additional instances,
we want to generate
additional graphs,
we want to generate additional
samples from the p model
and those would be the graphs
we will want to generate.
So if we want to do
this, then there are--
this is an instance of what we
call generative models, right?
We assume we want to learn a
generative model for graphs
from a set of input graphs,
let's call them x i.
Here, as I said before p data
is the data distribution which
is not known to us and we
don't have access to it,
all we have access
to it are samples
x that are sampled from
this unknown p data.
We also will have another family
of probability distributions,
let's call them p model
that are defined by theta,
theta are parameters
of our model.
And we will want to use
this p model distribution
to approximate p data.
And then, what
would be our goal?
We have two-step goal.
First goal is to find parameters
theta so that p model closely
approximates p data, and this
is called a density estimation
task.
And then we also want to be
able to sample from p model.
Basically means, we want to
be able to generate new graphs
from this now p model
distribution to which we
have access to, right?
We want to generate new
samples, new graphs from it.
So let me give you
more details, right?
Our goal is to make p model be
as close to p data as possible.
And the key principle we
are going to use here,
is the principle of maximum
likelihood estimation,
which is a fundamental approach
to modeling distributions.
Basically the way you can
think of it is the following,
we want to find parameters
theta star of our p model
distribution such that the
log likelihood of the data
points x of the graphs x that
are sampled from this p data
distribution, their log
likelihood under our model--
under p model that is basically
defined or parameterized
by theta, is as large
as possible, right?
So our goal is to
find parameters theta
star, such that the
observed data points x.
So basically, the
observed graphs
have the highest log likelihood
among all possible choices
of theta, right?
And of course here,
the important thing
will be that p model needs
to be flexible enough
that it's able to model p data.
And then the question
is, how do we search over
all the instances of probability
distributions captured
by this p model?
So we actually kind of capture
by these parameters theta, so
that the likelihood of
the data that we observe
is as high as possible, right?
And in other words,
the goal is to find
the model that is most likely
to have generated the observed
data x, right?
Find the p model that
is most likely to have
generated the observed data x.
Now, this is the
first part, which
is the density estimation.
The second part
is also important,
because once we have the
density that's not enough.
We need to be able to draw
samples from it, right?
We want to create samples from
this complex distribution.
And a common approach how
you can generate samples
from a complex distribution
would be the following.
Is that first, you start with
the simple noise distribution
just like a simple,
let's say a scalar value
that's a normally distributed
0 mean unit need to variance.
And then you want to have
this complex function that
will take this
little noise kernel
and is going to expand it
until you have the sample x.
So in our case, we'll start
with a little random seed,
and we are going to expand
it into a full graph x.
Where of course, now the
hope is that x will follow
the distribution of p model.
So in our case, how do we
design this function f?
We are going to use
deep neural networks,
and train them so that they can
start with the little kernel
and generate the graph.
So that's how we are
going to do this.
So now in terms of
deep generative models,
our model will be an instance
of an auto-regressive model.
Where this p model will be used
both for density estimation
and sampling, right, because
we have these two goals, right?
And in general you don't have
to use the same neural network
to both do the
density estimation
and to do the sampling and
there are other approaches
that you could choose to do here
like variational autoencoders
or generative adversarial
networks and so on.
But in our case, we are going
to use an auto-regressive model.
And the idea is
that, we are going
to model this complex
distribution as a product
of simpler distributions.
And the reason why
we can do this,
is of the chain rule in
probability and Bayesian
networks.
So we're basically--
which basically tells us,
that any joint distribution
on a set of variables
can be exactly
modeled or expressed
as a product over the
conditional distribution,
right?
So basically I'm
saying this p model
that is a complex
distribution over my x, which
may be a set of
random variables,
I can write it out as a product
over all these random variables
from the first one
to the last one,
where all I have
to now express is
this probability of a
random variability t given
the values instances of all
the previous random variables,
right?
So in our case for example,
if x would be a vector,
then x sub t is the t-th
coordinate of that vector,
right?
If x is a sentence, then x sub
t would be the word, right?
So I'm basically saying,
rather than generating
an entire sentence or to write
out the probability of a given,
let's say sentence,
I'm going to--
I can model that
as a product where
I say, given the words
so far, how likely
or what is the probability
of the next word?
And if I multiply this
out, I have the probability
of the entire sentence.
And we can do this without
any loss of generality
or any approximation,
if we really
condition on all the
previous words, all
the previous elements, right?
In our case, what this
means is that the way
we apply these two
graphs is that, we
are going to represent
a graph as a sequence
as a set of actions.
And we are going to say, a-ha,
the probability of next action
is conditioned on all
the previous actions.
And now what will the actions
be, it will be add a node,
add an edge, right?
That's the way we are
going to think of this.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 15.2 - Graph RNN Generating Realistic Graphs.txt
So.
Now, we are ready to look
about our generative model
for graphs, and this generative
model is called graphRNN.
And, it will allow us to
generate realistic graphs
without making any kind of
inductive bias assumptions.
And the model will
be as we will see--
extremely general and
scalable, and being
able to generate
a vast diversity
of different types of graphs,
of different sizes and shapes,
and structures, and so on.
So the key idea that
this is-- that we
are going to harness
here, is that we
want to generate
graphs via sequential
adding of nodes and edges.
And why do we want
to do it this way?
We want to do it this
way-- exactly what I
explained at the end of
the previous segment.
Which is, that
basically, we want
to model a complex distribution
that captures distribution
over graphs.
Modeling that complex
distribution--
we don't know how to do.
So we are going to break
that complex distribution
into many small parts.
And the way we break
creation of the graph,
we are going to break it
into many small parts,
into many small actions.
And we are simply--
all we'll have to do is model a
model transition probabilities.
Basically saying,
given the graph,
given the actions we have made
so far, conditioning on that,
what is the next
action we want to take?
And this way, by
multiplying these together,
we are able to model the
entire distribution--
all the dependencies in it.
So in our case, the
idea is that, imagine I
have this graph, and
I want to generate it.
The way I'm going to describe
the generation process,
is that, I'll start
with the first node,
and then, I'll add
the second node.
The second node will
link to the first one.
Now I have already built
a bit of the graph,
a third node is added and
links to the first node,
then node number 4 is added
and links to two and three,
and then node number 5 is added
and it links to three and four.
So that's basically,
the generative
process I want to model
and I want to capture.
And in this part
of the lecture, I'm
going to explain to you
the model called graphRNN.
And, here is the
link to the paper,
so if you want more details
you can read the paper
that I'm citing here.
So let's think about this
modeling graphs or generating
graphs as sequences.
A graph with node
ordering pi can
be uniquely mapped into a
sequence of nodes and edge
additions S. What
do I mean by this?
Graph is a set of nodes
and a set of objects,
and apriori graph has
no ordering to it.
So we need to this node
ordering, this permutation pi,
that determines the order
in which the nodes appear.
And given that ordering
in which the nodes appear,
then the sequence to generate
the graph is uniquely defined.
Because, first, I
add the second node.
Sorry.
First, I add the first node,
then I add the second node,
I add the third node.
And then, in terms of adding
edges I can always say I go--
I ask, should the
link the node 1?
Yes, no.
Should I link to node 1?
Yes, no.
Until all the nodes after
already present in the graph.
So that is basically the
key-- the key idea is
that we need some node
ordering, and given
that no node ordering,
then generating the graph
is simply a sequence problem.
Another observation that is
important for graph generation,
is that, this sequence is
actually a two-level sequence.
There is a sequence
of adding nodes,
and there is a sequence
of adding edges.
Right at node level sequence,
at each step we add a new node.
And then, we also
have the second level
after a node is added the edges
need to be added for that given
node.
So this means we s a two-level
sequence where we will first
have the-- we have the high
level node level sequence.
And then, the low level
edge level sequence.
Node level, a step--
we do one node
level step and then
we do a lot of edge level steps.
And then, we do a
node level step,
and we do a lot of
edge level steps.
And each edge level
step is simply
an addition of a new edge.
So you can think of node
level steps being, add node 1,
add node 2, and node 3.
While the sequence elements
in the edge level are,
should I add this node--
this new node that I have added,
should be connect it to node 1?
Should I connect it to node 2?
Should I connect it to node 3?
And you can think of
this as generating
a binary sequence that says,
don't connect, connect,
connect, So that's the idea.
So a summary of what
we have learned so far,
is that, a graph
plus a node ordering
gives us a unique
sequence of sequences
to generate a given graph.
A node ordering for now, let's
assume is randomly selected
or somehow it's given to us,
will come to this question
later.
And the way you can think
of this is right if--
basically is to say, Oh, if I
have a partially built graph
and I want to add a new
node, then adding this node
to the graph simply
means I have to print out
this particular column
of the adjacency matrix.
So I'd basically
saying, node 4 doesn't
link to node 1, node
4 links to node 2,
and node 4 links to node 3.
This is first node,
second node, third node,
and we just generated the
column of the adjacency matrix
for the node number 4.
So node level sequence is
going to add one new column
to the adjacency matrix.
And edge level sequence is
going to kind of print out
the rows-- the entries
of this column, where
0 means the edge
does not exist and 1
means that the edge exists.
So.
So far we have transformed
the graph generation problem
into a sequenced
generation problem.
Now, we need to model the
two processes, the process
of adding new nodes, where
basically, we want to generate
a state for a new node.
And then, we want to generate
edges for the new node
based on it's state.
And will be the
edge level sequence.
So the point will be that, an
approach we are going to use
is-- we are going
to use what are
called recurrent neural
networks to model
this two-level sequence.
And we are going to do use a
nested recurrent neural network
as I'm going to explain.
So the idea is the
following, what
are recurrent neural networks?
Recurrent neural networks are
designed for sequential data.
And a recurrent neural
network sequentially
takes an input sequence to
update its hidden state.
And based on the hidden state--
the idea is that
this hidden state
summarizes all the information
input to the RNN so far.
And then, the
update is conducted
via what is called RNN
cells, for every time
step I have a new RNN cell.
So the way you can
think of it is,
I initialize the RNN
with some hidden state,
I give it the input.
The RNN is going to update its
hidden state and put it here,
and it's going also
to create an output.
And now, the second time
step this hidden state--
next cell is going
to take as the input.
It's going to take as the input
the input from the environment,
it's going to update its hidden
state, here denoted as S2,
and it's going also to
produce me an output.
So basically, the idea
is that this hidden state
s keeps memory of what the
RNN-- what kind of inputs
the RNN has seen so far.
So to tell you more and
give you more mathematics,
S sub t is the state of
the RNN at the step t.
X sub t is the input to
the RNN at that time,
and Y sub t is the output
of the RNN under that time.
And the RNN is-- has
several parameters,
and these parameters
are called W, U, and V.
These are trainable
parameters, this
could be trainable vectors,
trainable matrices.
And the way the
update equations go
is the following, hidden state
gets updated by basically
saying, what is the hidden
state in the previous time step?
Let's transform it.
What is the new input
of the current time?
Let's transform it.
Pass It through
the nonlinearity,
and that is my
updated hidden state.
And then, the output Y is
simply a transformation
of the hidden state
and this is what the--
what we are going to output.
In our case, S, X,
o t will be scalars.
So basically, we
are going to output
a scalar that will tell us
whether there is an edge
or not.
Of course you can have
more expressive cells
like GRU, LSTM, and so on.
But here I am talking
about RNN, which
basically, is the most basic
of this sequence based models.
So.
So far we learned what is RNN.
Now, we want to talk
about how to use
the RNN to model the node level
and the edge level sequence.
And the relationship between
two RNNs is the following,
is that, node level RNN
generates the initial state
for the edge level RNN,
and then, edge level RNN
is going to produce the sequence
of edges for that new node.
And then, it's going to pass
its state back to the node level
RNN, who's going to generate
a new node, update the state,
and push it down to
the edge level RNN.
So this is how this
is going to work.
So to give you an idea, we will
initialize the node level RNN
with a start of a
sequence, and we will--
the node level RNN is
going to create a node.
Now, the edge level RNN--
given this node is
created a node level
RNN will add a new node, and
then, ask the edge level RNN
to generate edges for it.
So node 1 is already there.
Now, node level
RNN decides to add
one more node, node number 2.
Now, the edge level RNN has
to say, does 2 link to 1,
and this number 1 here,
would say that 1 links to 2.
And, here is now the new graph.
Now the node RNN says, OK,
let's add a new node, number 3.
Now, we ask the edge
level RNN to generate
the edges for the node 3.
And node 3 will say, it
links to the first node,
but does not link
to the second node.
So now our new
graph is like this.
And again, the node
level RNN decides,
let's add one more
node, let's add node 4.
And the edge level RNN
prints out-- basically,
a sequence of zeros
and ones that will mean
does falling to node 1?
Does it link to node two?
Or does it link to node 3?
And 4 links to 2 and 3.
So, here is now
the current graph.
And then, node level RNN says,
OK, let's add one more node,
node 5.
Edge level RNN
outputs the-- four
every previous node it tells
us, whether 5 should link to it
or not.
And that's how we get the graph.
And the generation
will stop when
the node level RNN
we'll say, I'm done,
I won't create any new nodes.
So this is the idea, where
edge level RNN sequentially
predicts if a new
node will connect
to each of the previous nodes.
Now, how do we do this with RNN?
So let me give you more ideas.
So in terms of how we use the
RNN to generate the sequence,
we are basically going
to take the output
from-- of the previous
cell, and put it
as an input of the next cell.
So whatever the
previous decision output
was, for example, whatever
was the previous edge--
that would be the input to
the next cell or the next time
step.
How do we initialize
the input sequence?
We initialize it to
have a special token.
We basically train
the neural network
that when it gets
this special token--
we'll call this token
SOS, so start of sequence.
As the initial
input, the network
started generating the output.
Usually, SOS-- when I
say a special token,
it could be a
vector of full zeros
or it could be a
vector full ones.
For example, just
something you reserve.
And then, when do we
stop the generation--
we stop the generation when
the end of sequence token
is produced.
And if end of sequence is
0, then RNN will continue
generation , and if end of
sequence is one the RNN will
stop the generation.
So to give you an idea now.
How do this-- how does
this fit together?
Will somehow initialize the
hidden state, will input
the start of sequence token.
The RNN is going to
generate some output
and then we are going
to connect this output
as the input to the next step.
And of course, the hidden
state will get updated.
And now, given the
hidden state, and given
the output from
the previous state,
this is again going
to be combined here,
another output is
going to be produced,
and another hidden state
is going to be updated.
This is all good.
But the problem with the model
as I've wrote it right now,
is that, it's all deterministic.
It's all a set of
deterministic equations
that are in a deterministic
way generating these edges
and nodes.
So what we want is, we
want the stochastic model.
We want a model that will
have some randomness to it.
So our goal is to use RNN to
do-- to model this p model
and RNN really is--
we are using it
to model this product of
conditional distributions.
So this marginal distribution of
p of Xt given everything else.
And let's write that
Y sub t is the p
model of X sub t
given all the X's that
were previously generated.
Then this means we need to
be able to sample Xt plus 1
from Yt.
So essentially, we
want to be able to get
to sample from this p model,
from this single dimensional
distribution.
This means that
each step of RNN--
Now, it's going to output a
probability of a single edge.
So each step of
the edge level RNN
is going to output the
probability of a single edge.
And then, based on
that probability,
we are going to
flip a coin, if we
are going to sample from
the Bernoulli defined
by that probability.
And then, whether
we get 0 or 1, that
is actually going to be
input to the next step.
So rather than saying the
RNN generates an edge,
RNN generates a
probability, and then, we
have a stochastic event,
a coin flip without bias,
that then lands heads
or tails, zero or one,
and we use that as an
input to the next step.
So that is essentially the idea.
So at the generation
time, at the test time,
let's assume we have
already trained the model.
Then, how this is
going to work, is
that, our Y sub t will be a
probability, will be a scalar,
will be basically a
Bernoulli distribution.
And let's use this square to
mean that this is a probability
and this square takes
value 1 with probability p,
and it takes value 0 with
probability 1 minus p.
So the idea will be
that, we will start--
we'll start our RNN, it
will output a probability.
And now, we are going to
flip a coin without bias
to create-- to decide whether
there is an edge or not.
And whatever is the
output of this coin,
we are going to use this as
an input to the next state--
to the next cell.
And again, we are going to
get another probability,
flip the coin, get
the realization,
and include that as an input
to the RNN cell who's going
to give me another probability.
So this is how we are
going to generate,
or how we are going
to unroll this RNN.
Now, the question is, how
do we use the training data?
How do we use the training
graphs that we are given?
Write this Xs.
And, in order to
train the model,
we are assuming that
we are observing
the graph that was given to us.
So this means we observe
a sequence of edges,
we basically assume--
we observe how the given
graph was generated.
So we observe zeros
and ones whether--
that corresponds to where
the edges exist or edges
don't exist.
And we are going to use this
notion of teacher forcing,
this technique of feature
forcing that place--
that replaces the input and the
output by the real sequence.
So the point will
be the following.
We are going to start
the model, and the model
is going to output some
probability of an edge.
But at the training time, we
are not going now to flip a coin
and use that as an
input to the next cell.
We're actually going to say, OK,
what was the true value there?
Oh, the true value
was, there was no edge.
So we are going to force this
as an input to the next step.
So teacher is kind of,
forcing the student, whatever
the student does
in the next step--
teacher correct the
student and the student
starts from the right--
with the right input
for the next step.
So the point, is
that, the inputs will
be the correct sequence,
not the sequence generated
by the model, and this is
called teacher forcing.
Now, of course we need
to define the laws that
measures the discrepancy
between the output of the model,
the output of the student,
and the ground truth
sequence that we try to
teach the model to generate.
And we are going to use binary
cross entropy, loss function,
which you can write
out the following.
You can-- the star is the--
why is a binary variable 0, 1.
1 means edge exist, 0 doesn't--
means that does not exist.
And Y without the star is
the probability of the edge.
And basically, the idea is
that because Y star is either
0 or 1--
if only one of
these two terms is
going to survive when we
actually implement it,
because when the
Y star comes in.
And basically, the idea is, if Y
star is 1, then we are really--
the loss really boils
down to minus log Y1.
So basically, in order for
us to minimize the loss,
we want to make log of Y1 to
be as close to 0 as possible.
Which means we want to make
Y as close to 1 as possible.
And then, if we want to--
now, on the other hand
if the edge is not
present, then we
want to minimize minus
log of 1 minus Y1.
And here we want
to make Y1 lower so
that the entire expression
again is log of something
close to 1, which
will be close to 0.
So this way, Y1 is fitting
to the data samples Y star.
And again, just to remind you
Y1 or this Y's are computed
by RNN, and the loss is going
to adjust the RNN parameters,
those matrices W, U, and
V, using back propagation
to try to minimize the loss.
Minimize the discrepancy
between the true sequence
and the generated
probabilities .
And basically, the point
is that the loss will say,
wherever there is a 0,
generate a probability
that's close to 0, and
wherever there is a 1,
generate a probability
that is close to 1.
OK.
So we do this, let's
put things together.
So do-- our plan
is the following,
we want to have two RNNs.
First, we want to
have an RNN that
will add one node
at each step, and we
will use the output of it to
initialize the edge level RNN.
And then, the edge
level RNN is going
to predict what other nodes--
what existing nodes does
the new connect to?
And then, we are going
to add another node,
and we will use the last hidden
state of the edge level RNN
to initialize the node
level RNN for one more step.
And then, how is this--
how are we going to
stop the generation--
if the edge level RNN is going
to output the end of sequence
at step 1, we know that no edges
are connected to the new node,
and we are going to
stop the generation.
So it's actually the edge level
RNN, and that, we have decided
will determine whether we stop
generating the graph or not.
So let me give you
now an example.
So that you see how
all this fits together.
So this is what is going to
happen under the training time.
For, let's say, a
given training--
observed training graph.
We are starting with the start
of sequence and a hidden state.
The node level RNN
will add the node,
and then, the edge
level RNN will be asked,
shall this node that
has just been added,
shall it link to
the previous nodes?
Yes or no.
It will update the probability.
And we are then going to flip
a coin that will determine--
with this given bias that will
determine whether the edge is
added or not.
And then, and then, we'll take
this and use it as an input--
as an initialization
back to the node level
RNN who's now going to
add the second node,
and this would be node number 3.
And then, the edge
level RNN is going
to tell us, will node
3 link to node 1,
will node 3 link to node 2.
And again, it's
outputting probabilities,
we are flipping
the coins, whatever
is the output of that coin is
the input to the next level
RNN.
So here are the
probabilities 0.6.
Perhaps you were lucky,
the output was 1,
so this is the input
for the next state.
And then, after
we have traversed
with all the previous
edges, we are going over
all the previous
nodes, we are again
going to ask node RNN
to generate a new node.
And so on and so forth.
And this is going to continue
right until the last node has
been added, then the node
RNN will add one more node,
but the edge level
RNN will say, I'm
not willing to connect
it to anyone else.
So this is the end of
sequence and we stop.
So basically, it's like,
when we add an isolated node,
we know that's the signal
that we want to stop.
So that's the idea.
And then, for each
prediction, here we
are going to get
supervision from the ground,
from the ground truth.
So the point is that, we will
be doing teacher forcing.
So, even for example,
here, when this was 0.6
and we did a coin flip
and maybe we were unlucky
and we got a zero, we are going
to output the true edge as we
said, the teacher--
the teacher forcing.
And then, the structure
of our neural network
when we do back propagation,
will be basically doing back
propagation through time.
We are going to basically back
prop from all these events all
the way to the beginning--
to the beginning of time,
to the firr-- to the first
node in the graph to update
the parameters of the RNN.
And then, how about at the test
time, at the generation time?
We are basically going to
sample the connectivity based
on the predicted distributions,
predicted probabilities,
and we are going to replace
the input at each step
by the RNNs own
prediction or own output.
So here we are going
to flip the coin,
the coin will say what it will
say, and it will go as an input
to the next one.
And I think, here, we are
going to do the coin, whatever
is the output here, should be
the input to the next step,
so it should be a 0
here, it's a mistake.
OK.
So that's the idea.
So the summary is we have costed
the problem of graph generation
as a problem of
sequence generation.
Actually, a problem of a
two-level sequence generation,
node level sequence, and
an edge level sequence,
and we use recurrent
neural networks
to generate the sequences.
And what I want to
discuss next is, how do we
make the RNN tractable?
And how do we evaluate?

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 15.3 - Scaling Up & Evaluating Graph Gen.txt
What I want to discuss
next is how do we
make the RNN tractable?
And how do we evaluate it?
So we are first going to
discuss about tractability.
How do we make our
model more scalable?
And the issue is that
in a graph in principle
any node can connect
to any prior node.
So this means there
might be a lot of steps
for edge generation, right?
When node 1,000 joins, we
may need to generate now
1,000 possible edges, right?
Does it link to node 999, and
then 998, all the way down
to node let's say
number 1, right?
So this would mean
that in principle we
need to generate a
full adjacency matrix.
And this is complex,
and it's like--
it leads to very kind of
long-range edge dependencies.
Because you have to memorize--
the model needs
to memorize a lot
in terms of what
previous nodes were added
and which previous nodes does
a new node want to connect to,
right?
So if you have a node
ordering like I show you here,
then for example,
the way you generate
this graph would be add node
1, add node 2, add node 3,
and only now start
creating edges, right?
Then this doesn't
feel like natural, it
seems that like this
when I added node 3,
I needed now to remember that
1 and 2 were already added
and all that, right?
And the point is that a
new node needs to link to--
can in principle link to
any previous node, right?
Even node 5 could
link to node number 1?
And the question is, how would
we limit this complexity?
And how would we limit these
long range dependencies, right?
Because this means that
the hidden state of node 5
somehow has to remember that
node 1 was added at all the--
at the beginning and
that 5 should link to 1.
And that's a lot to kind
of ask our model to do.
So if we can help it in any
way that would definitely help.
So the way the insight
is that we can take a--
because the insight
is that we can come up
with an node ordering that makes
our model much more tractable.
So the insight is that,
rather than having
a random node ordering and
having the model to worry
about long-range
dependencies, we
are going to use a node
ordering that helps
the model learn better.
And the node ordering
we propose is
called the breadth-first
search node ordering.
Basically, we are going to
start at some random node
in the graph.
We are going to label
it as node number 1.
We are going to label its
nodes as, let's say 2 and 3.
Then their neighbors
are 4 and 5, right?
And this leads now to a
much more natural recipe
to generate the graph, right?
We are saying add node 1,
add node 2, connect 2 to 1,
add 3 connect it with 1, add
4 connect it with 2 and 3.
It's kind of much more nicely
interwoven, these things are.
And if you say, how would
you draw this graph?
You'd probably draw it this
way, not some other way
that you first put all the nodes
down and then connect them,
right?
So the BFS ordering,
what does it buy us?
It buys us the following,
because node 4 does not
connect to node 1, we
know that no other node
is going to connect to
1 from now on, right,
because it's a breadth-first
search ordering.
So if for example, node 5
were to connect to node 1,
then its ID wouldn't be 5, it
would be less than 5, right?
So we know that all
of node 1's neighbors
have already been traversed
when a given node does not
link to it.
Therefore, node 5, and
all the following nodes,
right, as I said, will
never connect to node 1.
And why is this important?
Because this means
that when node 5 comes
I don't need to worry about
node 1 anymore, right?
Node 1 I can kind of forget, I
need to have much less memory,
right?
I don't need memory, I only
need memory of two steps
rather than memory of
remembering what I did
all the way at the beginning.
So the BFS ordering,
the key insight
is that node 5 will
never connect to node 1.
And this means that we only
need memory of two steps,
rather than the memory
of n minus 1 steps where
n is the number of
nodes in the network.
And this also means
that it reduces
the number of possible
orderings, right,
rather than considering all
the possible orderings which
is n factorial of them.
We only have to kind of consider
the number of distinct BFS
orderings.
And it also reduces
the number of steps
for edge generation
because of this insight
that we know that
5 won't link to 1.
And this is important.
Because so far, I
explained to you,
I said, the edge level
RNN generates the column
of the adjacency matrix.
But if you take this
BFS based ordering,
then the edge level
RNN does not really
need to generate
the entire column.
It can-- it only
needs to generate
a small part of the
column because we
know that the rest is 0, right?
So, rather than
generating connectivity
to all previous
nodes, and having
to remember all this with
the proper node ordering,
we are guaranteed
that all we need to do
is generate just a small band
of this adjacency matrix.
And again, this
doesn't prevent us
from generating any
kind-- like graphs,
this is still fully
general, it is just
exploiting the ability that
we can re-number the nodes.
We can order the nodes in
whatever order we want.
And because real
graphs are sparse,
a favorable ordering
gives us a lot of benefit
because it's much
easier to learn
to generate just this
blue part, than to learn
to generate this
entire upper triangle
of the adjacency matrix.
So this was the
discussion of how
do we scale up our model and
its insight, that we can come up
or we can decide on the ordering
and if you are smart about it,
it can really help us.
It could help the model learn.
The second thing I
want to talk about
is, how do we evaluate
graph generation, right?
And there are two ways
how you can do it.
One is that we
visually look at them
and see whether
they are similar.
And that's good to
get some intuition,
but we also want to define a
statistical notion of graph
similarity.
And of course, you
could try to say,
I'll take two graphs and I'll
somehow align them one on top
of the other, but that is very
expensive and for big graphs
you cannot do that.
So we have to define some kind
of statistical measure of graph
similarity between two graphs.
So first, let me show
you some visual examples
of what GraphRNN is able to do.
So what I'm showing here is
three input training graphs.
These are the output
graphs from GraphRNN,
and here are some output from
three traditional generating
models.
This is the Kronecker
graphs generating model,
this is the mixed membership
stochastic block model,
and this is the
preferential attachment
to the Barabasi-Albert model.
And what you notice
is that GraphRNN,
if you give it grids it's
going to generate you grids.
You see little mistakes because
the model is stochastic,
so it may make some little
mistakes, that's OK.
But you see that other
models completely fail,
they are not able to
generate the grid.
And this is not surprising
because none of these models
was developed to generate grids.
They were developed
to generate networks
for different types
of properties.
So that's OK, right?
GraphRNN can generate the
grid the others cannot.
What is interesting though,
is, even if you for example
give GraphRNN examples
of graphs with two
clusters with this kind
of community structure,
GraphRNN is able to learn
that these graphs have
that structure and is
able to generate you
graphs with such a structure.
Why?
For example, Kronecker graphs or
Barabasi-Albert, they cannot--
they were not done to
generate graphs with community
structures.
So they cannot do that, but
mixed membership's stochastic
block model was developed
for community structure.
So it does a good job, right?
And what I want to say
here is the following,
right, is that GraphRNN
is super general, right?
It's basically able to
take these input graphs
and learn about the
structure and then
generate new graphs with
similar structure, right?
You don't have to tell it,
Hey, I have communities.
Hey, this is a grid.
You just give it the graph, and
it will figure it out by itself
that the given graph
is a grid and it
needs to generate a grid, or
that it's a community structure
graph or anything else.
So it's quite remarkable that
such a diversity of graphs,
the same model can
cover and you don't even
have to tell it what
the input graphs are.
So now, how about doing
more rigorous comparison
about statistical
similarity, right?
How do we do that, right?
we-- as I said we cannot do
direct comparison between two
graphs, trying to
do graph alignment,
because graph isomorphism
testing as we have seen is
an NP, is a hard
problem, let's say.
So the solution is to
compare graph statistics,
and the typical graph statistics
we have already discussed
would be like degree
distribution, clustering
coefficient, or orbit count
statistics from the graphlets
that I discussed, I think
in lecture number two.
So the point would
be that given a graph
we are going to describe it
with a set of statistics,
and then we are going to
say that the two graphs are
more similar if their
corresponding statistics are
more similar.
And in our case, each
of these statistics
we are going to think of it
as a probability distribution.
And I'm going to explain
why is this important.
OK.
So first, is that,
given two statistics,
maybe two graph
properties, maybe degree--
two degrees sequences,
two degree distributions,
two orbit count distributions.
We want to compare this
on sets of training graphs
as well as on the
syntactic graphs.
You want to see how similar
is a set of training
graphs to the set of
synthetically generated graphs.
And we'd like to measure
the level of similarity
between the two.
And we are going to
do a 2-step approach.
In the first step, we are
going to do the following.
We are going to take
each of these graphs
and we are going to describe
it with a set of statistics.
We'll say here is the
degree distribution,
here is the clustering
coefficient distribution.
And now, we'll take
and we are going
to this for all the input
graphs, training graphs,
as well as, for all
the generated graphs.
And now, we are going to take,
let's say, degree distribution
of a synthetic graph and degree
distribution of a real graph,
and we want to see how much
of these distribution differ.
And to measure--
to quantify that we
are going to use something
that we call the earth mover
distance.
And now, that we have compared
the statistic individual
statistics.
Now, we need to aggregate
and measure how--
once we have a measure of
degree distribution similarity,
we have a measure of clustering
coefficient similarities.
Now, we need to take
these similarities
and further aggregate them to
get the overall similarity.
And for this second
level aggregation,
we are going to
use what is called
the maximum mean discrepancy
that will be based on the earth
mover distance.
So let me tell--
let me first define the
earth mover distance and then
the MMD.
So the earth mover
distance, kind of
tries to measure similarity
between two distributions,
or similarity between
two histograms.
And the way you
can-- the intuition
is that, what is the
minimum amount of effort,
minimum amount of Earth, minimum
amount of probability mass
to move from one
pile to the other,
so that one pile gets
transformed to the other,
right?
So I say, I have a distribution,
I have two distributions,
how much more?
How much mass?
How much of this yellow--
dirt, yellow earth,
do I have to move
between these
different pillars, so
that I will make
it and transform it
into this type of distribution?
So if I have distributions
that are very different,
then the earth mover
distance would be high.
And if they are kind of similar,
the earth mover distance
will be low.
And earth mover distance can
be solved as an optimal flow,
and is found by using a linear
program optimization problem.
We're basically saying, the
amount of work I have to do
is, how do I take
F and transform it.
How much earth do
I have to move,
so that it minimizes
the overall cost
didj between the probability
distributions x and y.
So that's the intuition behind
the earth mover distance
metric.
And then, the
second part will be
that, we are going to use
the maximum mean discrepancy,
or MMD.
And the idea of
maximum discrepancy
is to represent distances
between distributions,
as distances between mean
embeddings of their features,
right?
And here I give you
the formula for it.
But basically, the MMD between
two distributions p and q,
you can think of it--
is-- that this is the--
if I write it in terms
of some kernel k,
it's kind of the expectation
over these elements x and y
that are drawn from
distribution p and q,
and taking the expectation over
the distribution of p and q.
And of course, we need
to have this kernel k.
In our case, the kernel
will be the L2 distance.
So now, let me just summarize.
How do we put all this together?
We are given two sets
of graphs and we want
to see how similar they are.
The way we are going to do
this is, for every graph
we are compute--
we are going to
compute its statistics.
We are then going to
use earth mover distance
to measure to the discrepancy
between the two statistics,
between the two distributions.
And then, we are going
to apply the mean--
the maximum mean
discrepancy to measure
the similarity between
these sets of statistics.
Where the similarity
between sets elements--
which means the
individual distributions
in your statistics is computed
with the earth mover distance.
And this means, for
example, that, this way
we can rigorously evaluate
the correspondence
between the
particular statistic--
set of statistics on
the training graph,
as well as, on the
testing graphs.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 15.4 - Applications of Deep Graph Generation.txt
In the last part
of this lecture,
I'm actually going to talk about
an application of deep graph
generative models to
molecule generation, right?
So basically, if you want to
generate drug-like molecules,
you can use graph
generative models.
So let me tell you about that.

So what we are going
to do is the following.
The question is,
can we learn a model
that can generate valid and
realistic molecules that
optimize some property?
Right.
So the way you
can think of it is
that we want to have a model.
The model is going to
output a given molecule.
This molecule has to be valid--
basically, it has to obey
the rules of chemistry.
It also has to be
realistic, right?
You cannot generate some
Frankenstein type molecule.
It has to be realistic.
It has to look like a drug.
And we want it to
optimize a given property.
For example, we want it
to optimize drug likeness.
We want to optimize
its solubility.
And the paper I'm going to
talk about, or the method
is called Graph
Convolutional Policy
Network for Goal-directed
Molecular Graph Generation.
And it's linked here.
And you can read it if
you want more details.
So, here is a high level
overview of this paper,
and of this novel problem of
optimal molecule generation,
right?
The goal here is to generate
graphs that optimize a given
objective, like drug-likeness,
that obey underlying rules--
meaning that the
graphs are valid,
like chemical validity
rules, like the bonds
and things like that--
and are learned from
examples, meaning
they look realistic, right?
They imitate molecular graphs
which we use for training,
right?
And we just talked
a bit about how
do we imitate a given
distribution of graphs.
But here the difference is,
we don't want to only imitate,
we want to generate
graphs that are valid,
and we want to
generate graphs that
actually optimize a
given criteria, a given
black box, right?
And here the important point
is that the criteria is really
a black box, right?
It's this black box where
the graph generation
will get some feedback, right?
The objectives
like drug-likeness
are governed by physical
laws, which to us will
be assumed they are unknown.
What I mean by that is we don't
need them to be written down.
All we have to do is
to have a black box.
If we give it a
molecule, the black box
tells us how good
that molecule is.
But we don't have to
look into the box.
That's the important point.
So, how are we going to do this?
We are going to do this and cast
it as a reinforcement learning
problem.
And the way with
reinforcement learning,
the way that we formalize it is
that we have a machine learning
agent that observes
the environment,
takes an action to interact
with the environment,
and then receives a
positive or negative reward.
And then the agent wants
to learn from this loop.
And the key idea is
that agent can directly
learn from the
environment, which
is a black box to
the agent, right?
So we think that there
is the environment.
The agent is taking
actions, and it's
interacting with
the environment,
and the environment is
giving back some feedback,
some rewards to the agent.
And there are two
types of the rewards,
there is the
instantaneous reward
and then there is the
long term reward, right?
In our case,
instantaneous reward
will did I just add an
atom to the molecule,
and I did it according to
the rules of chemistry.
And then the long
term reward will
be after we are done with
generating the molecule, how
good was that molecule?
That's the long term reward.
OK.
So the solution to this
goal-directed molecule
generation, we call it Graph
Convolutional Policy Network
that combines graph
representation
and reinforcement learning.
And the key component
of GCPN is that we
are going to use a graph neural
network to capture the graph
structure information,
we are going
to use reinforcement
learning to guide
the generation towards
the desired objective,
and we are going to
use supervised learning
to imitate examples on a given
training data set, right?
We want our molecules
to look realistic.
How does GCPN differ
from GraphRNN?
First, what is the commonality?
The commonality is that these
are both generative models
for graphs, and they
try to kind of imitate
or they can be learned
given a data set.
What are the main
differences is that GCPN
is going to use a graph
neural network to predict
the generation of
the next action,
and while GraphRNN is using
the hidden state of an RNN
to decide on the next action.
A graph neural network
is more expressive
than a recurrent neural
network, so that's
the benefit of
the GCPN approach,
but on the negative
side, the GNN
takes longer time to
compute than an RNN.
So molecules
generally are small,
so we can afford this more
complex algorithm that
has bigger expressive
power, and it
will-- is able to learn more.
So GCPN will then also
use reinforcement learning
to direct graph generation
towards our goal,
towards our black box.
And reinforcement learning will
enable us this goal directed
graph generation.
So to give you an idea,
both of these two,
both GCPN and GraphRNN are
sequential graph generation
approaches.
But in the GraphRNN we
predict the action based
on the RNN hidden state, right?
So the node gives the hidden
state to the edge level RNN,
and then the hidden
state is passed on,
and the edges have
generated, and then
the hidden state goes back
to the node level RNN, right?
So basically all the
information, all the history
is captured in
this hidden state.
And if you have
generated 10,000 nodes,
this means this hidden state has
been transformed 10,000 times,
and then for every edge
it's also been transformed.
So it's a lot that this
hidden state needs to capture.
So in a GCPN, we won't have
this notion of a hidden state,
but we are going to use the
GNN to basically give me
the embeddings of the nodes.
So I'm going to say, here is
a partially generated graph,
and here is a new node.
What I'm going to
do is I'm going
to embed each of the nodes in
the partially generated graph,
and then I'm also going to have
some embedding for the new node
number 4.
And then, based on
these embeddings,
I'm now going to
predict which node
number 4 should link to, right?
So this means that
basically now I'm
not using the RNN to do this,
but I'm using a graph neural
network to generate the
state, and then I'm simply
doing link prediction.
So I'm kind of using--
predicting potential links
using node embeddings
rather than to directly generate
them based on the hidden state.
That's the difference.
And this would be much more
scalable, sorry, much more
expressive, much more
robust, but less scalable
because we have to now
compute these embeddings
and then evaluate
these link predictions
for every single action.
So the overview of GCPN
is that it has these
following four steps, right?
First, we are going
to insert the nodes.
Then we are going to
use the GNN to predict
which nodes are going to
connect with each other.
Then we are going
to take an action,
and we are going to
check chemical validity.
And then if the
action is correct,
we say yes, you
created a good edge,
you didn't create a good edge.
And then, after the model is
done generating the graph,
we are going to compute
the final reward.
We are going to ask
the black box, what
do you think of the
molecule we generated?
So a few questions
about the reward--
we will have two rewards.
One will be the
reward per step, which
will be basically to--
whether the model has
learned to take a valid action.
Basically, at each step
a small positive reward
will be awarded for taking a
valid action-- so basically,
by respecting the
rules of chemistry.
And the final reward will be
proportionate to the-- the goal
is for it to optimize
the desired property
of the molecule, right?
At the end, we are going
to get huge positive reward
if the molecule is
good, and a low reward,
or no reward if the
molecule is bad, right?
And the total reward is
going to be final reward
plus these stepwise rewards.

And then, in terms of training
the model, there are two parts.
First is the
supervised training,
where we are going to train
the policy by imitating
the actions given real observed
graphs using the gradient.
So basically, here
we are just kind
of going to try to
learn our model how
to generate realistic molecules
and not worry about optimizing
the structure yet.
So it's just about learning
to generate proper molecules
and obey chemistry.
And then in the second
part of the training
we are going to actually
train a policy that
optimizes the reward.
And here we are going to use
a standard policy gradient
algorithm that is kind of
classical reinforcement
learning.
But the point is, we are going
to have two steps of training,
one to learn the chemistry
and then the second one
to learn how to
optimize the reward.
And now if I want
to show you this,
we will have the
partially generated graph,
the GCPN is going to decide how
to grow it one node at a time
and how to create connections.
We are going to get
small positive reward
and the gradient
based on the fact
that we have generated
the graph correctly.
And this is going to
loop until we decide
the molecule is generated.
Now that the molecule
is generated,
we are going to ask our
black box to tell us
how good is this molecule.
And then this final
reward is going to be
also back-propagated, right?
So this generation is
going to be trained
using the supervised
learning, and then
this overall end-to-end
with the delayed reward
will be trained in this
kind of reinforcement
learning framework.
And what is the benefit
of this approach
is that we can generate
molecules that try
to optimize a given property.
So here I'm showing
you different molecules
that optimize log P, which is
a particular chemical property,
or here we are
optimizing QED, which
is the quantum energy--
again, something
that medicinal
chemists worry about.
And you can see how these
graphs that we generate
look like real molecules.
Another thing that this
allows you to do-- it
allows you to take a
partially-built molecule
and complete it.
So for example, you can start
with some starting structure,
where here, log B--
I think it's solubility.
So basically you start with some
very bad values of solubility,
and then you say, how do
I complete this structure
to improve solubility?
And here you see how it went
from minus 8 to minus 0.7,
and from minus 5 to minus
2, by basically completing
the molecule, right?
So this is the point, is
we can take, basically,
a partially built structure or
finish it, or create a brand
new structure.

So let me summarize the
lecture of graph generation.
So complex graphs can be
successfully generated
via sequential generation
using deep learning.
Each step is a decision that is
made based on the hidden state,
and this hidden state can
either be implicit or explicit.
In the RNN, this
vector representation
about keeping the
state was implicit
because it was all in this
hidden state, while in the GCPN
the state was explicit because
it was computed directly
on the intermediate graphs and
encoded by a neural network.
I also showed you possible
tasks for GraphRNN.
We talked about imitating
a given set of graphs.
For the second
part, for GCPN, we
talked about optimizing
graphs to a given goal.
I talked about the application
to molecule generation
to try to generate molecules
with optimal properties,
but you could apply this to any
kind of graph generation task,
to any kind of
property-- for example,
including generating
realistic maps,
generating realistic cities,
road networks, materials,
and things like that.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 16.1 - Limitations of Graph Neural Networks.txt
Welcome everyone, uh, to the class.
What we are going to talk today about is,
um, some advanced topics,
and in particular we are going first to talk about limitations of,
uh, graph neural networks,
um, and then we are also going about how do we improve
their expressive power and how do we, um,
then, uh, study, uh,
how robust are graph neural networks against, um, adversarial attacks.
So the idea for today's lecture is- lecture is the following.
Um, what would the perfect GNN model do, right,
if we wanna say about what are some limitations of graph neural networks and
especially when we looked at their expressive power in terms of the,
um, in terms of the, um,
uh, WL kernel, right?
If we go through a thought experiment then we could say,
what would a perfect graph neural network do?
And the k-layer graph neural network embeds a node
based on the K-hop neighborhood structure around that node, right?
And this picture tries to illustrate that.
That is that basically if I wanna embed this particular, um, node here,
I can take the graph structure around this node and then,
um, through message passing,
I wanna compute the embedding of that node;
and a perfect GNN would be such that it would build
an injective function between the neighborhood structure around the target node,
um, and the embedding that it produces.
So essentially, what we'd like to do is- with
a perfect GNN would take every different node neighborhood structure,
uh, and embed it into a different position,
uh, in the embedding space.
Um, there are two important, uh,
observations, uh, building on this intuition.
First is that a perfect GNN will do the following, right?
If two nodes have the same neighborhood structure around the- around them,
then they will have the same embedding.
Again, here we are assuming there is no discriminative,
uh, feature information given to us.
So v1 and v2 in this- in this,
uh, graph, with, let's say,
two connected components will be embedded into
the- exactly the same point because their neighborhood structure,
uh, around them is identical,
and of course, right?
If we have two nodes that have different neighborhood structures, then, um,
we'd like them to be embedded into different points in the space
because the net- the- the neighborhood structures of these two nodes are different.
One is in a triangle,
the other one is in a square,
so they should be embedded into different points.
So that's kind of what we'd like to do.
That's what we'd like our perfect,
uh, GNN to do.
However, these observations, 1 and 2 may not be always, uh, true.
For example, the- the observation 1,
um, can have kind of the following, uh, issues.
Even though two nodes may have the same neighborhood structure around them,
we may wanna assign,
um, different embeddings to them.
Um, and this is because, uh, you know,
nodes may appear in different positions or in different locations in the graph.
Um, and we, uh,
we are going to call, uh, uh,
this notion of a position in the graph and
these tasks that require us understanding the position,
we'll call those position-aware tasks.
And I'm going to define this more, uh,
precisely throughout, uh, the lecture, right?
So basically even a- a perfect GNN,
that has that, um, injective,
uh, function between the neighborhood structure and
the embedding will fail at these, uh, tasks.
Uh, for example, here if I have a simple grid graph and I have nodes v1 and v2,
and I'd like them to be embedded into different points in space because they
are kind of at the opposite ends of the- of the underlying uh,
graph, actually a graph neural network is going to embed them, uh,
into the same position because the neighborhood structure around them is identical.
They are both in the corner,
uh, of the grid.
Um, so this is kind of one issue that, uh,
graph neural networks, as we have defined them so far,
uh, are not able to do.
Um, the second important, uh,
implication of the observation 2 is that,
um, GNNs that we have introduced so far are kind of not perfect, right?
Their expressive power, um,
is not- is not enough, right?
Uh, and in- particularly in lecture 9,
we discussed that the expressive power of our graph neural network,
this message passing graph neural network with indiscriminative features, uh,
it's expressive power is upper binded- bounded by the Weisfeiler-Lehman,
um, graph isomorphism test.
So, uh, for example, um,
if I have nodes v1 on a cycle of length 3 and a node v2 on cycle of length 4,
if I look at the structure of their computation graphs,
um, the structure of the two computation graphs will be the same.
So without any discriminative node features
or if assuming all node features are the same,
graph neural network is not going to be able to distinguish, um,
or it won't be able to assign different embeddings to nodes 1 and, uh, 2.
So basically nodes v1 and v2 will always be embedded into the same space, uh,
under the assumption that there is no useful node features,
um, because their computation graphs,
uh, are identical even though one
resides in a triangle and the other one resides in a square.
So, uh, the plan for the lecture today is that we wanna resolve both of
these issues by building or designing more expressive graph neural networks.
Uh, and the way we are going to fix these issues is the following.
Uh, to fix the issue, um, uh, one,
we are going to create node embeddings based on their positions in the graph.
Um, and the idea will be that we wanna create reference points in
the graph and then quantify the position of a node against those,
uh, reference points, and the class of models that
allow us to do this are called position-aware graph neural networks.
And then to fix the issue number- number 2, um,
we- we- we are going to build, uh,
message-passing GNNs that are more expressive than the WL, uh, test.
Um, and the method, an example of such a mes- message- method,
is called Identity-aware graph neural network.
So this is what is going to be the,
uh, plan for the, uh,
for the first part of the lecture,
and then in the last part I'm going to talk about adversarial attacks.
So, uh, here is our approach,
and this is how we wanna think about it.
So, um, we will use the following thinking.
Um, given two different, uh, inputs,
for example, nodes, uh, graphs, uh,
uh, edges, um, er,
let's assume they are labeled differently,
and we are going to say that, you know, kind of,
the model fails, um,
if it- i- if it is always going to
assign the same embedding to these different inputs,
or these different objects,
and a successful model is going to assign different embeddings to these,
uh, different, uh, types of objects.
Um, so if we focus,
let's say on node-level embeddings, then, you know,
embeddings in a GNN are determined,
uh, by the underlying computation graph.
Right? And in my case,
imagine again I have a graph with two connected components.
I have two vertices, v1 and v2.
Imagine v1 and v2 are labeled with different labels.
V1 is labeled with A,
v2 is labeled with B.
The goal will be to build a graph neural network that is
going to assign different embeddings to node v1,
then to the node v2.
Again, under the assumption that node features are the same,
uh, or, non-discriminative between uh, v1 and v2;
and perhaps what is- you may seem- or you may say
striking or interesting is that the models we have,
uh, um, developed so far,
uh, actually fail to distinguish v1 and v2.
Like, even though we have built
so much super cool machinery that works amazingly well in practice, uh,
and empirically, um, we still cannot
distinguish v1 and v2 in this kind of corner case, uh, example.
So what we are going to do is to understand how can we resolve this?
How can we build a network that- a graph neural network that will be able to distinguish,
uh, basically, uh, v1 and v2.
So meaning assign them different embeddings,
so that then we can assign v1, one label,
and we can assign v2, the other label,
because we cannot assign them different label if they both map into the same point.
So a naive solution to this,
that kind of doesn't work, would be,
uh, to use one hold and- one-hot encoding.
So we would like to say,
Okay, we don't have any features,
but let us- let's assign each node a
different ID and then we can always differentiate different nodes,
um, in- in a graph or different edges or even different graphs.
So if I have, you know,
two- two graphs here,
uh, as we had before,
I could simply assign a one-hot encoding to every node,
um, and now of course,
because nodes now have, uh, features,
it will be- the computational graphs will be distinguishable because,
you know, v1 will have, uh,
two children, one with 0100,
and the other one with, you know, 001;
and v2 is going to have,
um, different, um, types of, um, uh,
neighbors because their- their,
um, one-hot encodings, uh, will be different.
Then, even though with the two,
if they will be the same at the first level,
they won't be the same on the second level.
So basically, computational graphs will be different,
so our GNN will be able to, uh, distinguish them.
Um, what are the issues with this?
The- there are two very important issues.
First is that this approach is not scalable;
meaning we need an order N feature dimensions
where N- N is the number of nodes to be able to encode, right?
Basically, we need a separate feature for every individual node,
and if, you know,
if we have a 10,000 or a 100,000 or a million node network,
then every node has now a million features,
basically a one-hot encoding of its ID.
Uh, and then the second problem is that this is in- this is not inductive;
meaning it won't generalize to new- new nodes or
new graphs because these one-hot encodings are kind of arbitrary,
node ordering is arbitrary,
so the map- the network,
it could basically learn according to that node ordering,
and then if we, um,
try to transfer this to a new graph,
or if a new node appears in the network,
this won't- this won't work, right.
If a new node appears we'll have to expand- extend the feature dimensionality because
we wanna encode- use one-hot encoding for that node as well and we'd have to retrain.
Uh, or if we wanna transfer to a new graph,
we have no guarantees because, uh,
the one-hot encoding and node IDs are kind of arbitrary,
so it won't, uh, it won't generalize.
So this is why, you know,
this is a bad- bad idea,
but, ah, this idea kind of to enrich,
uh, the nodes so that we can, um,
differentiate different computational graphs is a good idea.
Just one-hot encoding, uh,
doesn't work in this case.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 16.2 - Position-Aware Graph Neural Networks.txt
So let me now tell you about position-aware
graph neural networks that are going to solve,
uh, part of the- part of the problem.
So imagine there are two types of tasks on the graphs, right?
There is what we are going to call
structure-aware tasks and there is position-aware tasks, and, again,
this is just to illustrate this concept, um,
about how graph structure may affect the underlying labels.
And in reality, in the real world, you know,
every task is a bit of a structure-aware and a bit of a
position-aware but some tasks will be more position-aware,
some will be more structure-aware,
and we'd like to have models that can operate in both regimes, right?
When structure-aware- in structure-aware tasks, uh, for example,
labeling of the nodes, uh,
for this simple, uh,
graph with two connected triangles,
if nodes are labeled according to these labels A and B here, then, um,
this is one way because the- the r- the structure
of the node and the neighborhood basically defines its label.
A different type of a task is what we call position-aware task,
where, for example, if you think about community detection,
community detection is a position-aware task right here, uh, you know,
nodes on one side have one label and nodes on
the other side have the other label even though their,
uh, uh, uh, local structures are,
uh, are comparable or iso- isomorphic, right?
Like, uh, Node 1 and Node, uh, 2,
um, they basically have the same neighborhood structure surrounding them.
So instructure-aware tasks, they should be labeled.
With the same label in position-aware,
they might be labeled with different labels because they
are in different parts, uh, of the network.
And, uh, the point is that the GNNs,
the graph neural networks,
the GCN, GraphSAGE, uh,
graph attention network, they work well for, uh, structure-aware tasks.
Right here, basically, we can differentiate v_1 and- and v_2, um,
because we are using- because they will have different,
uh, computational graphs as illustrated here, right?
v_1 has the following computation graph,
v_2 has a different computation graph just because v_2 has
three neighbors and v_1 has two neighbors even at the first hop so we are
able to distinguish them and meaning we are able to assign them different labels
because they'll have different embeddings because they have different computation graphs.
How about position-aware tasks where now we change the labeling of the nodes,
you know, let's say, according to the communities?
In this case, a plain GNN is going to fail because nodes v_1 and v_2, these, uh,
labeled here in the graph,
have the same computation graphs
because they're kind of symmetric with each other, right?
So the point is that now, um,
because they have the same computation graphs and, again,
because we are assuming there is no discriminative node feature information given to us,
the two nodes have the same, uh,
local neighborhood structure, they have the same computation graph,
which means they will be- they will have the same embedding,
which means they will- the classifier will have to assign them the same label.
Um, and in this case, we want them to,
uh, label them differently.
So the question is,
how can we extend graph neural networks,
deep learning methods that are- that would be able to, uh,
solve or work well in this, uh, you know,
toy example that kind of tries to
illustrate this notion of position-aware prediction tasks.
Um, and the key idea, uh,
for this part is the notion of an anchor
because the way you know your location is to- to know,
um, what is your position or,
uh, uh, against some reference point, right?
And we are going to call these anchors to be a reference points.
And if I know how far away from, uh,
reference points I am and you know kind of
how far away you are from different reference points,
then we can, uh, distinguish our locations.
It's almost like you wanna triangulate the position of the node inside
the graph by- by characterizing some kind of a distance to the anchor node.
So that's- that's the idea.
The idea is we wanna have a reference point, um,
and to quantify the location so we are going to use this notion of anchor,
uh, anchor nodes to give us these, uh, locations, right?
So we are going to basically pick these r- anchors at random and we are going to say,
let's pick a node,
let's say S_1 in this case, uh,
and let's call it an anchor node and then we are going to represent
the position of v_1 and v_2 by their relative distance to the, uh,
anchor node and because these two- this- the distance, uh,
of v_1 and v_2 in this case to the anchor node s_1 will be different, uh,
this basically means that we'll allow- this will allow us to
differentiate or distinguish v_1 from v_2, right?
So intuitively, the anchor node serves almost like as a reference point,
as a coordinate axis that tells us, um,
how far away from, uh,
each different nodes are and this basically allows us to, kind of,
triangulate or locate the position of the node, uh, in the graph.
Um, of course, we are not only- only going to use one anchor node,
we are actually going to use multiple anchor nodes
because if we can- if we use multiple anchor nodes,
we can better characterize the position of
a node in different region- regions on- of the graph.
So kind of if you have multiple anchor nodes,
we are able to better,
um, distinguish or, uh,
set, uh, our position.
Of course, we don't wanna have too many becomes- beco-
because then it becomes computationally hard, but, you know,
having some number of them and there is actually,
uh, a theory how many we wanna have, um,
then, uh, we can, uh,
characterize node's position in the network, uh, quite well.
Here, in this case, I- you know,
s_1 and s_2 are anchor nodes.
Um, v_1 and v_2 are nodes of interest and I'm simply saying, you know,
v_1 is one hop away from s_1 and two hops away from s_2,
while v_2 is two hops away from s_1 and one hop away from, uh, s_2.
And now, this kind of allows us to distinguish v_1 from, uh,
v_2 because they are at different distances from these, uh, anchor nodes.
Um, there is another, um,
generalization that turns out to be important is
that we don't wanna really only talk about anchor nodes,
we wanna talk about anchor sets, right?
So, uh, we are going to generalize this notion of
an anchor node from a single node to a set of nodes and then we
are going to define the distance between the node of interest and
the anchor set as the minimum distance to any of the nodes,
uh, in the anchor set, right?
Um, and the idea here is that, uh,
this will allow us to even triangulate the position of the node at
a much more fine-grained level because anchor sets will allow us,
uh, to provide more precise,
uh, position information, right?
Um, and it will allow us to keep the total number of anchors to be still small.
So what I mean by this is that, for example,
I could say, let's- let's have,
uh, now anchor sets.
I have, uh, anchor node s_1,
I have anchor node s_2,
but then also have an anchor set,
I'll denote it as s_3 that includes node v_3 and, uh, s_1.
And now, I'm going to characterize the distance of- uh,
of a given node,
uh, towards, uh, against that anchor set.
Um, and in this case,
for example, if I'm interested in position of v_3, uh,
v_3 will have a distance of 0 to the anchor set
s_3 because it is part of the anchor set while
v_2 is going to have a distance of 1 because
the closest node in the anchor set to v_1 is- is,
uh, one hop away.
Um, so, uh, what does this mean that,
um, for example, if we would, as we had before,
if I only use s_1 and s_2 as my anchor, uh,
nodes or anchor sets,
then v_3 and v- uh,
uh, v_1 cannot be differentiated with each other.
They have the same distances.
But now if I use this anchor set of Size 2,
I can actually just differentiate,
uh, v_1 and v_3.
And, again, there is a nice theory that says that it- it
is beneficial to use anchor sets of, uh,
different sizes because then the number of,
uh, anchor sets, the number of coordinates,
the number of reference points,
uh, you need, uh,
to locate a node in the graph is, uh, relatively small.
It's smaller than if you would just use, uh,
anchor nodes like s_1 and s_2 and add,
uh, multiple, uh, anchor nodes.
So what is the summary so far?
Uh, we are going to de- we have just developed
this positional encoding of a node in the graph,
where we are going to represent a node's position by
its distance to randomly selected anchor sets and each dimension in this,
um, in this, uh,
encoding will, uh, tell me the, uh,
the- will be tied to a given anchor set and will be
the minimum distance from a node of interest to any of the nodes in the anchor set.
Uh, that is the- that is the idea of how we are going to,
uh, uh, create this,
uh, positional, uh, encoding.
Now, uh, before I move on and use- how this position information is used,
the way- of course,
you can ask how many of these sets do you need and how big they need to be?
And what we are going to do is we are going to do the following.
We are going to have an e- expone- er,
anchor sets of exponentially increasing size,
but we are going to use exponentially fewer of them, right?
So we will have a lot of, uh,
anchor sets of Size 1,
we'll have half that number of anchor sets of Size 2,
we'll have, you know, uh,
half of that number of anchor sets of Size 4,
Size 8, Size 16, and so on.
Um, so this- this means we'll have, you know,
some relatively small number of anchor sets where
each next anchor set size is going to be doubled,
but the number of them will be half of what we had before.
And that's usually a good way how to generate these, uh,
anchor sets and the- the nodes that belong to anchor sets,
we simply select them, uh,
uniformly, uh, at random.
And then we charac- as I said, we characterize, uh,
this positional encoding of a node by simply the minimum distance from the node to the,
uh, any of the nodes,
uh, in the given, uh, anchor set.
So now, how do we use this positional information?
A simple way of using
the positional information is to use it as an augmented node feature.
And this works really well- well in practice.
So basically, we just enrich the feature descript- descriptor of a node with this,
uh, positional information, uh,
characterized by the shortest path distance,
uh, to the anchor sets.
Uh, the issue here is that since
each posi- dimension of position encoding is tied to a random anchor,
dimensions of positional encoding,
um, can be randomly permuted and the encoding, uh, could be,
uh, basically is semantically the same meaning,
um, er, er, without changing it- its meaning.
So, uh, and- and what this means, uh,
imagine you permute the input dimensions of a normal, uh, er, uh,
er, neural network, the output will, uh, change.
So what is, um,
what is a more rigorous solution than just using these positional encodings as they are
is to design a special set of neural network operators that can maintain this,
uh, permutational invariant property of positional encoding.
So basically, uh, that- the position encoding is order invariant,
which you can achieve through,
let's say, some kind of, uh,
some aggregator or, um,
uh, aggregators that are uh, order invariant.
Uh, because, uh, permuting the input feature dimension
will only result in the permutation of the ou- output dimension,
uh, but the value of each dimension shouldn't change.
And, uh, you know, there is a paper, er,
that introduces position-aware graph neural networks, uh,
to say how you can do this in a more rigorous way but the key here is this notion of
an anchor and the notion that you can
quantify the position of a node in the graph by the distance,
uh, to the anchor and that allows us to now improve the expressiveness of
graph neural networks because nodes won't
only know what is their local neighborhood structure around them,
but they will also know what is their location,
uh, or position, uh,
in the neural network.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 16.3 - Identity-Aware Graph Neural Networks.txt
So now we are going to move forward and we are going to move to the next topic,
which is called identity-aware graph neural networks, right?
So in the previous part of the lecture, we talked about,
how does the node encode its position in the network?
How does the node know where in the network, the node is?
Now in the second part,
we are going to develop a more expressive graph neural network that is
going to take care of
all these different symmetries that can account- that can appear in the network and uh,
in the underlying graph,
and it - it will make the graph neural network more expressive.
So what we have learned so far is that
classical GNNs would fail for position-aware tasks.
And we said, let's use, um,
let's use anchors to improve graph neural network performance on position-aware tasks.
Now, we are going to switch back and to- and focus more on structure-aware tasks.
And say, can GNNs perform perfectly on structure-aware tasks?
And as we have seen before,
the answer here is unfortunately no.
Uh, and the issue is that
GNNs exhibit kind of three levels of failure cases in, uh structure-aware tasks.
And I'm going to show you some,
you know, failure cases.
And of course, all these failure cases are kind of worst-case scenarios uh,
that are very intricate in a sense that uh,
due to the symmetries,
the GNN is going to fail.
So perhaps they don't necessarily appear in practice uh, too often,
but they may appear in some parts of the data,
and they are still very useful uh, to study.
So here is the first uh, failure case.
Uh, this is for the Node-level tasks.
Imagine you wanna do a Node-level classification,
you want to do Node-level prediction.
Here, different inputs from the same, uh,
basically different inputs, but at
the same computational graph will result in the same embedding.
So if I have these two nodes,
v_1 and v_2, uh,
you know, residing in these types of connected components, as we said before,
their computational graphs, um,
if you work it out are exactly the same because they have
two neighbors and each of their neighbors has two neighbors and so on and so forth.
So this means that these nodes v_1 and v_2 will be embedded into
exactly the same point in the embedding space
and we won't be able to assign them different labels.
Now, the same type of things can happen also, for example,
for link prediction, where for example,
you can have this type of input graph.
And you want to decide whether you know,
v_0 should link to v_1 or should it link to v_2?
And again, if you look at the computation graphs,
um, the -the computation graphs are the same.
So nodes v_1 and v_2 are going to have the same embedding.
And because they have the same embedding,
the neural network will give the same probability to- to edge A as well as to edge B.
And perhaps that is not uh, the most realistic.
So that's our failure case again for a different type of uh, input graph.
And then, you know,
for graph level tasks,
there are also uh,
well-known failure cases because
different input graphs will still
result in the same graph neural network based embedding.
Um, and why- why is that the case?
It's because if you, for example,
uh, in these types of- in these types of networks,
all the nodes have the same degree, um,
but you notice that these two graphs are different because
here the nodes link to exactly the immediate nodes.
Here the- the nodes link kind of a bit farther out.
But if you look at the computation graphs,
the two computation graphs uh, will be the same.
So again, uh, these two- these two entire graphs will get the same embedding.
So again, this is, um,
a very kind of highly symmetric uh, input graph.
But still these two graphs are different.
They are non-isomorphic.
But you know, this is kind of a corner case for
WL test and it is also a corner case for graph neural networks.
So here again, uh, graph,
A and graph neural network without
any useful node features will always classify nodes A and B,
uh, or graphs A and B into the same position, into the same class.
So now, how are we going to resolve this?
What is the big idea here?
And the big idea in this second part of the lecture,
is that we can assign a color to the node we want to embed.
And that's why we call this identity-aware,
because the neural network,
as we unroll it,
will know what is the starting node,
what is the node where we started?
So the idea is, if I want to embed nodes v_1- v_1,
I'm going to color it.
And if I go, um,
and because I'm going to give it a color,
um, now, the graph, the computational, uh,
graph will be different because I will remember whenever I unroll uh,
the computational graph, I will remember the color of this colored node.
Right? So this means that, uh,
now our computational graph,
will- will- will remember whenever it hits the node of interest v_1.
So it we'll have these colors um, and you know,
why- why is this,
um, uh, why is this useful?
This is useful because it is inductive.
Right? It is invariant to the node ordering, um,
or identities of the nodes because the only node we color is the node where we started.
And then we just look,
how often does this node,
where we start appear in the computation graph, right?
So eventually, right, like if- if
our graph is- is connected as- as we go deeper into more layers of a graph neural network,
there will be some cycle that will lead us back to the starting node,
and we will remember that and have that
node colored in the computation graph, uh, as well.
And the important point here is because- because the node coloring is inductive,
even though I- I have these two,
let's say, different input graphs,
but I have labeled, uh,
or numbered the nodes differently, right?
I have 1, 2,
3 versus 1, 2,
3, the underlying computational graphs will be the same,
which is good because they don't change under, uh,
permuting the IDs or identities, uh, of the node.
So this is a great feature to have because it
means our models are able to generalize better.
So let's now talk more about this, uh,
inductive capability of node coloring.
And let's look at the node level task.
Um, and the point is that
this inductive node coloring helps us with node classification tasks.
For example, I have here,
um, the case, we have already,
uh, looked at before.
I have the node on a triangle,
I have a node on a square,
um, I colored the root.
And now I say, let's create the computation graphs.
Here I create the computation graphs and you, um,
very quickly see that the computation graphs,
um, are quite- are quite different.
And in particular they- they become different at, uh,
at the bottom level,
where in the- in the part B here,
when I go to two hops- when I go two hops out,
I only hit these nodes while in the- in the first case,
um, I actually get- go and hit again the starting node.
So now these two computation graphs are different because we also consider colors.
So we will be able to successfully differentiate between nodes v_1 and nodes v_2.
So, uh, this is a very elegant solution to the- to- to this, uh,
to this, uh, uh problem that where
a classical graph neural network, uh, would fail.
Um, and similarly, we can do the same thing,
uh, for- um, for graph classification, right?
If I take my two input graphs, uh,
the way I created the- aga- the embedding of the graph is to create an embedding,
uh, of nodes and then aggregate those.
So if I look at node-specific, um, uh,
computation graphs, uh, structurally,
they might be the same,
but- but- but because I have labeled the starting node and
now I- I know whenever my computation graph returns back to the starting node,
you'll notice that now the coloring pattern between these two graphs,
uh, is different- these two nodes is different,
which means their embeddings will be
different which means that when we aggregate the embeddings,
the embeddings for the graphs will be different,
which means we'll be able to take these two input graphs,
A and B, um,
and embed them into,
um, different points and assign them different classes.
So this is exactly what we want.
And then, you know,
for the edge level tasks,
again, ah, if, you know,
I start with V_0 and I say,
you know I want to assign a different, uh, uh,
probability to, um, uh, to nodes,
V_1- to the edges A and B,
I can say what will be the embedding of V_1, uh, here?
What will be embedding of V_2?
And I see that their corresponding computation graphs, uh,
will be different because, uh,
V_1 is going to hit V_0 sooner than, uh, V_2.
So, um, here, the point is that when I'm embedding nodes for link prediction,
I'm given a pair of nodes and here,
I'm going to color,
um, both- both nodes,
the- the- the left node and the right node and this way,
I'll be able to distinguish, um,
uh, the two computational, ah, graphs.
So this means it will allow us to, uh,
assign a different probability to the node- to the edge A versus,
uh, the edge B, which is, uh, what we want.
So what you- what I have demonstrated so far is that
this node coloring where we color the identity of the- uh,
of the starting node or in link prediction of the- of this- of these both, uh,
nodes in involving the link prediction task
allows us to differentiate and distinguish, uh,
these types of symmetric, uh,
corner cases that make classical neural network- graph neural networks, uh, fail.
So now, the question is,
how do you build a- a GNN that uses this node coloring and that you- it will allow us,
uh, to distinguish these different colored, uh, computation graphs.
The idea is the following and the model is called identity aware, uh,
graph neural network and what we wanna do
is you've wanna utilize inductive node coloring in
the embedding computation and the key idea
is that you want to use heterogeneous message passing, right?
Normally in a GNN,
we apply the same message aggregation computation
to all the children in the computation graph, right?
So whenever we- we are, uh, aggregating, uh,
masters and transporting messages,
we apply the same aggregation in the same neural network operator, right?
So, um, this is what we classically do.
In our graph neural- identity aware graph neural network,
we are going to do heterogeneous message passing.
So we are going to use different types of aggregation, um,
different types of message passing applied to different nodes based on their color.
So it means that in an IDGNN,
we are going to use, um,
different message and aggregation, uh,
functions, uh, for nodes with different, uh, colors.
So it means that for example,
whenever we are aggregating into a color node,
we are going to use one type of, uh,
transformations and message passing operator
and whenever we are aggregating into a non-colored node,
we are going to use a second type of,
uh, aggregation and transformation.
So this means that in a given layer,
different message and aggregation, uh,
functions would be used to nodes based on the color,
uh, of the node and this is the key, right?
Because if the node- nodes is color and it
can use a different aggregator, this means that,
uh- that the- the- the message will get transformed differently,
which means the final results will be different depending on whether
the nodes with colors were involved in aggregation versus nodes without colors,
uh, uh, being involved in aggregation.
So um, you know why does this heterogeneous message-passing work?
Right? Suppose that two nodes,
V_1 and V_2 have the same computational graph structure,
but they have different node coloring, right?
Um, and since we apply different neural network embedding computation,
right- different, um, uh,
message passing and different aggregation, right,
we have different parameters for nodes with one,
uh, color versus the nodes with the other color,
this means that the final result, uh,
will be different and this means that the final output-
the final embedding between V_1 and V_2 is going to be, uh, different.
So, you know, what is the key, uh,
difference between GNN and identity aware GNN?
If we look at this,
uh, you know- uh, use, uh,
this case- uh, this example we have looked so far,
if I have nodes V_1 and V_2 I wanna distinguish them,
in the classical GNN computation,
the two computation graphs are the same,
uh, all the nodes are the same,
there is no differentiating node features,
so the aggregations across these two, uh,
trees will be the same,
so we won't be able to distinguish A and B.
In the case when we actually color
the starting node and now the two computation graphs are different,
so all we have to account for now that- is
that when we aggregate the information from the- um, uh,
from the leaves to the root of
the graph neural network that this information about the color is preserved
or somehow accounted for so that the final message will have
a different value depending on one, uh, versus the other.
And this is exactly,
um, the case what is happening,
and this is why IDGNN allows us,
uh, to make this distinction.
Um, another thing to think about it,
what is G- IDGNN really doing?
IDGNN is really counting cycles of different lengths,
uh, uh starting at a given,
uh, given the root node, right?
So if I start here,
IDGNN will now able to count or realize that there is a cycle of length 3, right?
So here- this is now basically a cycle.
You get off- you go from yourself to the- uh,
to the neigh- to the neighbor, uh,
and then to another neighbor,
and you'll come back to the node, right?
So this is a cycle of three hops.
While- while in this case you- we- we'll
reali- graph neural network is going to realize that this is a cycle of length 4,
because you have to go to the node,
to the neighbor, um,
to the first neighbor, to the second neighbor,
to the third neighbor, and from here,
you only arrive, uh,
to the starting node itself, right?
So here we'll real- the- the computational graph will be able to capture
that- or be able to compute that- that node is part of a cycle of length,
uh, 4, but no cycles of length 3.
While here, these will be able to capture that
the node is a part of cycle of length o- uh,
3 but not, uh, let's say 4.
So this is what IG- IDGNN is able to do.
It's able to count cycles and it's able to learn and count- count them through the,
uh, uh, message passing of the graph neural network.
So, um, how do you now,
uh, uh, how do you do this now?
So as I said, one is to use,
uh, heterogeneous message passing.
Um, and, uh, and the second way how you can do this is that,
um, we can- based on the intuition we have, uh, just proposed,
you can also use a simplified version of the IDGNN,
where basically the idea is to include identity information
as an augmented node feature, um, and, uh,
sidestep the- the heterogenous node, uh, uh,
message passing and the idea here is basically you can augment the node feature,
by using the cycle counts in each layer
as an- as an augmented node feature and then apply,
a simple GNN, right?
So basically, you want to use cycle counts in each layer as
an augmented feature for the root node and then simply apply het- uh,
homogeneous message-passing, basically drop the colors, right?
So the idea would be that every node gets now,
um, ah, uh, gets a description that simply says,
how many cycles of length 0 are you part of,
how many cycles of length 2- like- cycles of length 3s,
and, uh, and so on.
And this way, you will be able to, um, uh, uh,
to distinguish the node- uh,
the two computational graphs,
and be able to distinguish the two nodes, uh,
into two different, uh, classes.
So let's summarize the identity aware graph neural networks.
Uh, the- this is a general and powerful extension to graph a neural network framework.
Um, it makes graph neural networks more expressive.
Uh, the IDGNN, this idea of inductive node coloring and
heterogeneous message passing can be applied to any graph neural network architecture,
meaning, um, gra- graph convolutional neural network,
GraphSAGE, um, GIN,,
so graph isomorphism network and- and any other, uh, architecture.
Um, and IDGNN will provide a consistent performance gain in many node-level, um, er,
as well as edge and graph-level tasks,
because it allows us to break the symmetries and allows
us to the- basically identify how the node,
uh, belongs to different, uh, cycles.
This means that IDGNNs are more expressive than their, uh,
graph- kind of classical graph neural network, uh, counterparts.
Um, and this means that IDGNN is kind of this- the, uh,
uh, the simplest model that is more expressive than 1-WL test.
Um, and it can be easily implemented because it's basically just you color the root node,
and that's all the information,
uh, you need to, uh, worry about.
So, uh, this is quite cool because it allows us now to
distinguish this node on a triangle versus node on a square.
Um, this was the first and the key idea is here to have this inductive node coloring,
and then we also talked about position aware graph neural networks,
where the idea is that you want to distinguish the position of the node in the graph,
and the key idea there was to use the notion of
anchors and characterize the location- the position of the node,
by the location, uh,
by the distance of the node to the anchors.
And we talked about we want to have anchors- anchors of different sizes.
And we wanna have a lot of anchors of size 1,
we wanna have a lot- uh,
fewer anchors of size 2,
even fewer of size 4 and so on and so forth.
Um, and the- the distance of a node to the anchor
is the distance of the node to the- any node that is part of this,
uh, anchor or, uh, anchor set.
[NOISE]

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 16.4 - Robustness of Graph Neural Networks.txt
The second topic, uh,
I wanna discuss is,
I wanna talk about robustness of graph neural networks.
So that's, the question is how robust are these models against,
let's say, adversarial or,
uh, any other type of attack?
So, uh, this is motivated by kind
of more broader field of deep learning while in recent years,
we have- we have seen impressive performance of
deep-learning models in a variety of applications,
um, especially, let's say in computer vision,
natural language, and so on.
But what has been, uh, shown is that,
if these models are applied to the- to the real world, um,
and we have different types of actors in the real world,
then the question becomes,
how robust are these models to-, um,
uh, to various kinds of attacks?
And one example is a notion of an adversarial attack, where for example,
you can take the original input image and ask the neural network,
what do you think this is and the neural network is going to say,
"Oh, it's a- it's a- it's a panda."
But what do you can do then is very slightly perturb,
uh, values of the pixels.
Um, you know, just you- you- you- adds a bit of
noise and now the neural network will get totally confused and it will say,
"Oh, it's a- it's- it's a gibbon, right?
It's a monkey. So the point being is that to a human,
when you look at these, they both look like,
uh, images of a panda to us.
But by being very careful, uh, and systematic,
very adversarial, you can change the prediction of the neural network.
Um, and these types of
adversarial examples are also reported for natural language processing model,
audio processing and- and the point is that the adversary can very quickly,
um, change the prediction of our model by only slightly manipulating the input.
So, um, the existence of the- of
adversarial examples prevents reliable deployment
of deep-learning models in the real world.
Uh, because adversaries may try to actively
hack the deep-learning model and kind of, uh, sidestep it.
Um, and the model performance this way may- may be much,
much worse than what we'd expect, right?
Um, and like has been found is the deep-learning models are often not robust.
Um, and this is a very active area of research to mas- to make
these models robust against adversarial attacks or adversarial examples.
So, uh, let's look in this part of the lecture about how would you formalize
the notion of adversarial example to
graph neural networks and are graph neural networks
robust to adversarial examples?
Um, and the premise is that, um,
if common applications of graph neural networks,
uh, you know, uh, involve, let's say,
platforms that are accessible to the public or that there
is some financial monetary interest behind them,
then these types of attacks, uh, may happen.
So for example, in recommender systems,
social networks and social-, uh,
and search engines, there's always some types of actors, attackers,
who may wanna manipulate the system.
So the adversaries have incentive to manipulate
the input graphs and basically try to change or flip or hack,
the predictions of the graph neural network.
So how would we going to start- to set up,
um, the framework to study robustness of graph neural networks?
Um, to study robustness of graph neural networks,
we are going to, uh,
specifically consider the following setting,
we are going to look at semi-supervised node classification and we are going to
use the graph convolutional neural network model just as an example.
And the idea is, right,
we have a partially labeled graph.
So here's the example.
We have a graph, uh,
if you are thinking about node classification,
nodes have features, some nodes already have,
uh, labels, and we would like to predict, uh, labels, uh,
for the other nodes as I-,
uh, as I show, uh, here.
So, uh, then, um,
the question is the following, right?
Um, what we are going to do is,
we are going to first describe several,
um, types of attacks that the adversary may do.
Then we are going to talk about,
uh, you know, how a graph neural network, uh,
or a GCN, uh, how- how would- how would that-,
uh, how we could try to hack it?
Um, and this means that, um,
a- adversary- adversary needs to know what underlying model we are using.
And then we're going to mathematically formalize
the attack problem as an optimization problem.
And then I'll show some empirical results to show how
vulnerable GCNs are to adversarial attacks.
So let's first talk about what would,
uh, an attack to a graph, uh, look like?
Uh, and we are going to talk about direct in- and indirect attacks.
Uh, and in particular,
the way we are going to think about this is the following.
Let's say there is a target node t,
whose label pre- prediction we wanna change.
And let's say that there are some other nodes in the network,
um, uh, let's call them S that the attacker can modify.
So these are the nodes that the attacker controls.
And now, what can the-, you know,
what can the attacker-,
uh, what can the attacker do?
So let's say that there is a direct attack,
and a direct attack means that the attacker is the target node, right?
So the attacker owns the target node.
So what the attacker can then do,
the attacker can modify the target node features.
For example, they can change- change their social network profile.
They can change the website content.
They can also add connections,
um, from the target to other nodes in the network.
Like, you know, they can buy likes,
they can buy followers,
they can make frie-, uh,
friendships or things like that.
Um, and they can also of course decide to remove some connections.
You know, they can unfollow users.
They can drop friendships if that would allow them to flip
the classification label for that target node that they, uh, own.
So this is what we call a direct attack because the attacker owns the node,
the- the node that the attacker wants to flip is- is- is the attacker.
So for example, if you think about some kind of spam detection,
the attacker may want to say "Aha, you know,
the social network is classifying me as a spammer.
What do I need to change so that
the social network won't classify me as a spammer anymore?"
Um, and they may change their attributes,
they may change who they follow or they may- they may drop,
uh, some of the links in the network.
Um, there is also a- a second type of attack which we call an indirect attack.
And in an indirect attack,
a target node is not the attacker node.
So it means that, the attacker does not,
uh, account- does not control the target,
but the attacker can, uh,
control some other nodes in the network and the attacker can change,
let's say, the features of those nodes.
So, uh, the attacker can change the connections of those nodes and
attacker can also basically can add them or they can remove those connections.
And you know, what is a realistic example for this case, it is that,
for example, the attacker wants to, uh,
harm some other, let's say,
innocent node in the network.
So the attacker is going to- to change, um, uh,
the- the properties of the nodes it controls to
perhaps change the label of the target node they don't control.
So perhaps the target node is not a spammer,
but the attacker wants somebody to be excluded from the network.
So they want them to be labeled as a spammer.
So they are going to now change,
uh, the network structure,
the connections to the target node,
so that the neural network is going to,
uh, classify the target node differently even though the target node has no clue that,
uh, they are being, uh, under attack.
So how are we formalizing the adversarial attack?
The goal is that we wanna maximize something subject to some constraints,
and the idea will be that,
we wanna maximize the change of the target node, uh, label.
So we wanna change the target node label subject to very small,
uh, graph manipulation, right?
We wanna manipulate the underlying graph as little as possible such
that the class probabilities of the target node are going to change, right?
Basically, the idea is that if the graph manipulation is too large,
it will be easily detected,
um, by the-, uh,
by the- by the- by the agent,
by the social network and it will say,
"Hey, there is an attack going on."
But if a successful at- if an attack is successful,
then we should be able to change just a small number of edges,
maybe a few features here and there, uh,
and this will be kind of unnoticeably-small manipulation that is going to flip,
uh, the prediction, the- let's say,
the class label for the target node, uh, significantly.
So the way- the way we wanna think of this is we'll say,
perhaps there is a class or a label to which the target node gets, uh, classified
right now, so what we wanna do is we want to decrease the probability of
the correct class and we wanna- wanna increase
the probability of some other chosen class,
let's say class three,
uh, in our case.
So how are we going to formalize this mathematically?
We are going to say that we have the adjacency matrix of
the original graph and we have the feature information of nodes of the original graph.
And then we'll also have, uh,
A prime and X prime to be manipulated graphs.
So these are the graphs after the attack,
after we have manipulated them.
And we'll have the adjacency matrix and we'll have the feature matrix.
And our goal or assumption is that the two- that the manipulation is small, right?
Our goal is- that this manipulation, the changing
the adjacency matrix or a change in the node features is,
um, uh, unnoticeably small.
So basically we wanna preserve basic graph statistics like degree distribution,
uh, node feature statistics and so on.
And as I mentioned that the graph manipulation can either be direct,
changing the node and feat- and, uh,
features and connections of the target node or it can be indirect,
which means you're- we may want to change, um,
the connections and features of some other nodes in
the network with the hope to change the,
uh, label of the target node.
So, um, now that we said what- what we wanna do, let's formalize it.
So we're- we'll be given a target node v, um,
and let's us- what did the- what, uh,
the- the system is going to do, uh, you know,
let's say the social network is going to learn the original,
um, the classifier over the original graph, right?
So it's going to say, "Oh, let's, you know,
define this likelihood over the training data and let's optimize
our neural network model parameters over the training data."
And then, um, you know,
the- now that the model has been trained,
how is the prediction going to look like?
We'll say, a-ha, for this target node of interest v. Uh,
let's classify in- into the class that our predictor f,
our neural network parameters,
theta star that we have trained applied to this graph,
um, this is our prediction, right?
We are going to predict the class of- for this node v,
that has the highest predicted,
uh, probability, just kind of standard stuff.
Now, uh, the attacker comes into play.
So what the attacker is going to do, the attacker, um,
can-, um, what they can do is,
they cannot change the parameters of the model, right?
The- the model parameters,
uh, cannot be changed because, you know,
the social network, whoever,
the operator has already trained the model and has deployed it.
So the model parameters, um,
won't necessarily, uh, be able to be-,
uh, to be changed.
So, um, what we can- what we can do in this case is that the GCN, um,
we can- like the way we will think about it,
is that we are going to think about it, that the GCN will now be
learned over the manipulated graph, right?
So basically, we are going to change A to
be A prime and we are going to change X to be X prime.
Um, and now, the- the- we are going to train over
these manipulated graph and we are going to get a different set of parameters.
And then the GCN's prediction of the target grow-,
uh, node may change,
both because the input is different as well as because
the parameters we learned will be different because
we learned them on a different graph and with different node features.
And we want the prediction to change,
uh, after the graph is manipulated, right?
So we want the class for node v, uh,
after manipulation to be different than what did the class was,
uh, before the manipulation?
So we want to flip the,
um, uh, the- the class.
So the way we are going, uh,
to do this is the following, right?
The way we are going to do this is to say, uh,
we wanna change the predicted probability, uh,
for a- of a- of a class of a given node,
um, between the true class and the manipulated class, right?
So here I'm saying f is outputting the probability,
um, for a given node to be of a given class.
So here I want the predicted log probability of a newly predicted class,
uh, c star prime to be- to increase.
While, um, I want to, um,
decrease the predicted log probability of the originally, uh, predicted class.
We want this term to decrease and we want that term to dec- to increase.
So we are going to formulate this into this delta- function delta,
that is basically cha- changing the prediction on the target node, uh,
v. It is a change, uh,
in like- in- in likelihood of the,
uh, correct class versus likelihood of the manipulated class.
And the way we make this term, this delta high,
is to make this probability here as small as possible and we make this probability here,
um, as large, uh, as possible, right?
We want these to be close to 0 and we want this to be, uh, close to 1.
So we wanna increase,
uh, the first term,
the left term, and we wanna decrease, uh, the second term.
So the way we can write this out as an optimization problem,
we wanna say we wanna do the argmax
over the underlying graph structure and the feature vector,
where we wanna maximize this delta, right?
This delta function of the change in the prediction, uh, on the target,
subject to the constraint that the manipulation, um,
of the graph structure and the feature vector,
uh, is as small as possible.
And ideally, what we'd like to do is,
solve this optimization problem.
Uh, the challenge in optimization- in optimizing this objective if we are an,
uh, adversary, is that the,
um adjacency matrix A is a discrete object.
You cannot take gradients, um,
uh, against A to decide what edges to manipulate.
Um, and that's the first challenge,
and then the second challenge is that for every modification to,
um, to the adjacency matrix and the future vector,
we need to retrain the underlying graph neural network,
to see whether the,
prediction, uh, is- will flip, uh, or not.
And I won't go into more details here and, you know,
here's a paper, uh,
you can read, uh,
to learn, uh, more about this.
But essentially, the idea,
the idea here is,
that we wanna manipulate,
the graph structure and the future vector as little as possible,
so that the- so that the,
predicted label of target node v,
is going to change,
um, as much as possible.
So imagine we are now able to solve this, uh, optimization problem.
And we can ask ourselves,
how easy it is to find
these strategically placed edges and the changes in the future vector,
that lead- that would, uh,
let the model to completely wrongly,
make predictions on a given node.
So here we are working on a,
semi-supervised node classification task with graph convolutional neural network, uh,
model, on a paper citation network with 2,800 nodes,
and about 8,000 edges.
Um, and you know, this is a six way classification task.
In what we are doing here is we are looking at a set of nodes,
that belongs to the class, uh, six.
And we ran the graph neural network,
we trained it five times,
uh, from random starting points.
And here we are saying, you know,
what is the pre- pre- predicted probability of these nodes,
that belong to Class 6?
And you can see that, they have probability.
So- so our graph neural network is learning well.
Now, imagine, we are an attacker and we say we want this, um, Class 6.
So basically, uh, research papers that belong to Topic 6.
We wanna, manipulate the underlying graph structure,
to make them belong to Class, uh, 7, right?
So this means, we are eh,
going to change the adjacency matrix,
um, and the, um,
uh, see if we can flip, uh, their labels.
Um, and what is interesting is if you use this framework and ask,
how many edges do we need to manipulate,
in a direct adversarial attack,
it turns out that GCN's prediction will flip, um,
aft- only, after modifying just five edges,
atta- attached, to the target node.
So here I show you for a given target node, in, you know,
five different, uh, retrainings,
just kind of five different instances.
We see that every single time,
we were able to flip the label of this target node,
who was of, uh,
Class 6, to be now of Class 7.
Um, and the model is super confident,
that this node belongs to Class 7,
even in reality it is,
uh, in Class, uh, 6.
And to summarize, uh, this, uh,
robustness of graph neural networks against attacks,
uh, this is summarized here.
Where, um, 0 would mean- in classification margin intuitively,
you can, you can think a bit about,
how confident, um, is the,
is the- or how correct,
is- is the, um, is the classification.
So, if it's positive it means, the,
the classification is correct,
with a bid gap,
so we are very confident in correct classifications.
And if the, uh, classification margin is negative, it would mean,
we are wrong, and we are,
and we are wrong by a lot, right?
We assigned a high confidence to the wrong prediction.
And we see that, you know, um,
with no attacks, the graph neural net- this is individual nodes here.
We see that, for the majority of the nodes,
the graph neural network is able to make, uh,
correct prediction and it is very certain about its prediction.
And you know, the class- the prediction,
the prediction performance is not perfect.
So there are a few mistakes,
but those mistakes are also kind of borderline.
The, the network is,
unsure about the classes of this,
uh, of these nodes.
Now, if we apply, uh, a,
a direct adversarial attack,
um, then you can see how we can very easily change, um, the,
uh, the- the, the neural network to basically
always predict- to always make a mistake and be super confident uh,
that it- in the, in the wrong, uh, class,
so assign, high confidence in the,
in the wrong class.
So this is a direct attack- direct adversarial attack.
Uh, here is what would happen,
if the- if it's still a direct attack,
but it's a random attack.
Meaning, we don't adversarially strategically, change the,
the edges, but we add edges,
uh, at uh, random.
Um, and you see that basically the neural network will get confused,
but not so badly, right?
It, uh, still majority,
it is making correct predictions.
So definitely decreases performance,
but not so drastically.
If you look at uh, indirect attacks,
were we basically now manipulate uh,
other nodes in the network and see if we can change the label of the,
of the, uh, target node.
Here you see again,
we are able to do this, uh, quite well.
Meaning for quite a few of the nodes,
we are able to flip,
uh, the, uh, uh, label.
So what this means is,
adversarial direct attacks are very strong and they
can- they significantly worsen GNN performance.
Uh, random attack is much, much weaker,
than adversarial attack and somewhat decreases, the performance.
While indirect attack is,
uh, less effective than direct,
but more effective, uh, than, uh,
than direct, uh, random attack.
So, um, what do we conclude from this?
Is that- that if the, uh, um,
uh, adversary has access to the full network,
has access to the- to the data and can, manipulate it,
then it is very easy to change the predictions of this deep-learning,
uh, graph neural network, uh, models.
So to summarize, we studied the adversarial robustness of
graph convolutional neural networks applied to semi-supervised node classification.
We considered different attack strategies,
different attack possibilities, uh, in, in graphs.
We talked about direct attacks as well as indirect attacks.
Uh, we talked, uh, we then mathematically
formulated the adversarial attack as an optimization problem,
so this is the optimization problem,
that the adversary has to solve.
And we, looked at some examples to empirically demonstrate that, um,
er, GCN's prediction performance can be significantly harmed, by adversarial attacks.
So what we conclude is that, uh,
GCN is not robust against adversarial attack,
but it is somewhat robust to indirect attacks,
as well as to random, uh, noise.
And this topic of robustness of these types of, uh, methods,
is a topic of, uh,
active research, and a very fruitful, uh, research area.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 17.1 - Scaling up Graph Neural Networks.txt
Welcome, uh, to the class.
Uh, today we are going to talk about scaling up,
uh, graph neural networks.
Uh, and in particular, we will be interested in how can we build, uh,
graph neural network models that can be applied, uh, to large, uh,
scale graphs, and what kind of, um,
techniques, uh, can we use,
uh, to be able to do that.
So, uh, let's first, uh,
kind of define and, uh, motivate the problem.
So, uh, there are large graphs in many modern applications.
Um, if you think about recommender systems, for example, uh,
systems that recommend you products at Amazon,
systems that recommend you mov- videos to watch on YouTube,
uh, pins to look at at Pinterest,
posts to examine at Instagram,
and so on, um,
you can think of these as a- as a- as a way where we wanna connect users, uh,
to the content, products, videos that,
uh, they might be interested in, right?
And, uh, you can think of this recommendation task,
um, as a way to, uh,
do link prediction, uh, in this, uh,
large bipartite graph where you have, you know,
hundreds of millions, billions of users on one end,
on the other side you have tens of millions,
hundreds of millions, tens of billions of items, uh,
on the other end, and you would like to predict what,
uh, what, uh, mo- what movie,
what video, or what product, uh,
is, uh, interesting, uh,
for what, uh, user,
and you can formulate this as a link prediction, uh, task.
Another example, uh, where,
uh, graph neural networks can be very, uh,
uh, well applied is in social networks,
for example, uh, Twitter, uh, Facebook, uh,
Instagram, where you have users and friend and follow relations,
and you want- wanna ask about how do I wanna make,
uh, recommend, uh, friends to each other at the link level?
Uh, do I wanna do some kind of user property prediction, for example,
uh, predict what ads the different users would be interested in or predict what, uh,
country they come from,
or perhaps I'm doing some kind of attribute imputation in a sense that maybe some-
some users tell me their gender but I don't know
the gender of other users and I wanna predict it.
Um, and again, in this case,
these networks have, uh,
billions of, uh, users and tens to hundreds of billions, uh, of edges.
And then, uh, another application I- you know that can be motivated
by what we are going to talk about today is- is a notion of heterogeneous graphs.
And, for example, you can- you can- one such, uh,
dataset is called Microsoft Academic Graph,
which is a set of 120 million papers,
120 million authors, as well as their affiliations,
institutions, um, and so on.
So, uh, this means now we have a giant heterogenous, uh,
knowledge graph on which you can try to do a lot of interesting machine learning tasks.
For example, a paper categorization which will be- predict what category,
what topic the paper is about, uh,
recommend collaborators to authors,
uh, predict, uh, what papers should cite each other.
You can imagine this to be very useful when you are writing,
uh, a new paper or doing a new piece of, uh, research.
And these are again, uh,
machine learning tasks over this giant heterogeneous graph.
And more broadly, right,
we can think about, uh, knowledge graphs,
the knowledge graphs coming out of Wikipedia,
coming out of, uh, Freebase,
where again we have, um,
100 million entities and we wanna do knowledge graph completion tasks or we wanna do,
uh, knowledge reasoning tasks, uh,
over these, uh, uh,
over these knowledge graphs.
And, you know, what all these, uh, um,
applications have in common is that they are large scale, right?
That number of nodes is in millions to billions,
and, you know, the number of soft edges is, I know,
tens- tens or hundreds of millions to,
uh, uh, or, uh, to tens- to,
uh, hundreds of billions.
And the question is, right,
like if we have these types of large-scale graph,
these large- this set of large-scale data,
and we would like to do, um,
tasks in- in all these cases that I have motivated,
we can do kind of node-level tasks in terms of user, item,
paper classification prediction as well as, uh,
pair-wise level tasks like, uh, link, uh,
recommendation, uh, uh, link prediction, and so on.
Um, and, er, the topic of today's class will be about how do
we scale up GNNs to- to graphs of this- of this size, right?
What kind of systems,
what kind of algorithms can be developed that will allow us to learn or over,
uh, these kind of, uh,
massive amounts, uh, of data?
So let me explain why is this hard or why is this non-trivial, um,
and it will become clearly- very quickly clear that we need,
uh, special methods and special approaches, right?
When we have a big, uh, dataset,
let just think in general- like in a general deep-learning framework,
when we have a large number of N data points,
uh, what do we wanna do, right?
In- in general, our objective is to minimize the- the- the loss,
the average loss, uh,
over the training data.
So if I have, uh,
N training data points, then, you know,
I go- I have the summation from 0 to n minus 1 where for every,
um, data point i, I'm asking what is the loss of it, right?
What is the discrepancy between the true label,
the true prediction, and the,
um, uh, uh, pre- uh, and the- and the,
um, and the prediction made by the model?
And- and I would like to compute this total loss, um,
under the current model parameters, uh,
Theta, so that then as I can compute the loss,
I can then compute the gradient with respect to the loss and update,
uh, model parameters in such a way that the total loss will be minimized,
that the- that the- so basically,
the model parameters will change,
which means model predictions should change such- such that the loss,
the discrepancy between the true labels and the predicted labels,
um, gets, uh, smaller.
And notice what is the point here,
the point here is that we have to sum up
these loss contributions over all the training data points.
Right? Um, and, uh, because, uh,
the number of da- training data points is too big,
what we then- what we then do is, um,
we would actually select small groups of M data points called the mini-batches and then
approximate this big summation with
smaller summations where i only goes over the points in the mini-batch,
right, uh, over, I know,
100, 1,000, uh, uh,
data points we have sampled and then
compute the loss over it and then make a gradient step.
This means that our, uh,
loss, our gradient will be- will be stochastic,
it will be imprecise, it will be a bit random because it will depend on what exact, uh,
subset, uh, of random points N have we compute- have we selected,
but it will be much faster to compute because M,
the size of the mini-batch,
will be much smaller than the total amount, uh, of data,
so our model will,
um, converge, uh, faster.
Now, what could you do in graph neural networks?
You would simply say, if let's say I'm doing a node-level prediction task,
let me subsample a set of nodes N,
uh, and put them in a mini-batch and let me learn, uh, over them.
Um, and, uh, here is the reason why this,
for example, won't work in graph neural networks.
Right, imagine that in a mini-batch I simply sample some set M of nodes, uh, independently.
And what this picture is trying to show,
is these are the nodes I'm going to sample.
And because, eh, the- the mini-batch is
much smaller than the total size- size of the graph,
what is going to happen, is that,
yes, I'm going to sample these nodes,
but the way graph neural networks operate is that
they aggregate information from the neighbors,
but most likely none or majority of this name neighbors won't be sampled in our,
uh, in our mini-batch.
So, um, what this means,
is that the nodes in a mini-batch will tend to be isolated nodes from each other.
And because the GNN generates node embeddings by aggregating, um,
information from the neighbors, uh,
in the graph, uh,
those neighbors won't be part of the- um,
of the mini-batch, so there will be nothing to, uh,
aggregate and whatever gradient we compute over
isolated nodes won't be representative of the- of the full gradient.
So this means that the stochastic gradient descent can- can,
uh, won't be able to train effectively on graph neural networks.
If we do this naive mini-batch implementation, where we simply,
uh, select a- a sub-sample of nodes,
as I illustrated here,
because nodes in the mini-batch won't be connected with each other,
so there won't be any message passing to be able,
uh- uh to be able to be done between the neighbors,
and the - the entire thing,
uh, is going to fail.
So, um, another way how we could, uh,
do this is to say,
let's forget mini batching,
let's not select a subset of the training data, uh,
to compute the gradient on,
let's do what is called full-batch implementation, full-batch training.
And the problem with full batch training is the following,
is that you can think of it that we generate
embeddings for all the nodes at the same time.
And the important thing here is- to notice that we
have embeddings of the nodes at several layers, right?
We start at layer 0,
which means the embedding of the node is just the node features,
then we propagate through the first layer
and we get the embeddings of the node at layer 1.
Now, to compute the embeddings of the node from layer 2,
we have to aggregate the first layer embeddings
to compute the second layer, uh, embeddings.
So what this means, is that you would first,
uh, load the graph in memory and all the features,
and then at each layer of the GNN,
you would compute the embeddings of all the nodes using
the embeddings of all the nodes from the previous layer, right?
So that now if I'm computing the layer K,
I need to have all the em- embeddings of all the nodes, um,
stored for layer K minus 1,
so that whenever I'm creating the next layer embedding of a given node,
I check who its neighbors are.
I take the previous layer embeddings, uh,
for those neighbors and aggregate them to create the next layer embedding.
And because of this recursive structure, I basically create,
um- I need to be able to keep in memory the
entire graph plus the embeddings of all the nodes from the previous layer,
as well as enough memory to create the embedding so for the nodes,
uh, for the next layer.
Uh, and then once I have that,
I can then compute the loss and I can compute the, perform the gradient descent.
So the way you can think of this is,
I start with the initial embeddings of the nodes,
I perform message-passing, create the embeddings of the nodes at layer K plus 1.
Now I put this, uh,
here and call this layer K, again,
do the message-passing to get to the next, uh, layer.
And this- this would be what is called a full-batch implementation of
a graph neural network.
Um, and why is full-batch implementation problematic?
Uh, it is problematic for two reasons.
One reason is that it takes, uh,
super long and it's kind of, uh, time-wise,
data-wise, inefficient to do full-batch training.
But imagine, you know,
you have all the time in the world,
so you say, I don't care if it takes a bit longer.
Um, even with that,
it is still impossible to implement a full batch training,
uh, for large, uh, for large graphs.
And the reason for this is because if we want the training to be,
uh- uh not- to be kind of, uh,
in the order of magnitude of what we are used today,
then we wanna use, um, GPUs.
So we wanna use um- uh,
graphic cards to do this,
right? For fast training.
But the GPU memory- GPUs have a very special kind of,
uh, super-fast, uh, memory,
but they have very little of it.
They have about 10-20 gigabytes, uh, of memory.
And I think the most you can buy today is around 30 gigabytes,
uh, bigger GPU cards.
And that's, you know, considered super state of the art.
Nobody has those graphics cards, right?
But the point is that the entire graphs and entire features,
if you have a billion nodes,
will be far, far bigger, right?
If you have a billion nodes,
um, and- and, uh, you just spend,
let's say one byte,
uh, um, uh, uh,
or do you have, I don't know, 100 bytes of, uh, features,
uh, per node, it will easily- you will need a terabyte of memory,
10 terabytes of, uh,
main memory, which is the amount of memory we
can get today for large servers using CPUs.
So the point is, CPUs have slow computation,
large amount of memory,
GPUs have very tiny amount of memory, but super-fast.
And the point is, if you wanna do full-batch,
you need to feed all the features, all the embeddings,
and the entire graph structure into the GPU memory,
and 10 gigabytes is- is- is- is- is nothing.
It's less than what your iPhone has memory.
So it is so small that we cannot scale,
uh, full-batch implementations beyond, uh,
a few thousand node, uh,
networks and definitely not to millions and not, uh, to billions.
So, um, this motivates the lecture for today, right?
So the lecture for today will be,
how can we change the way we think of graph neural networks?
How do we implement the training?
How do we, uh,
change the architecture so that we can scale them up,
uh, to, uh, large graphs of billions of nodes,
so that we can limit this issue with small, um,
GPU memory and that we can limit the issue that naive implementation of mini-batching,
uh, just doesn't work.
So I'm going to talk about two methods.
Uh, basically in this lecture,
I'm going to talk about three different methods,
two of them will be based on performing
message-passing over small subgraphs in each mini-batch.
So we are going to change how we design mini-batches.
Um, and this will lead to fast training time.
One- one is technique called neighborhood
sampling and the other one is technique called, uh, Cluster-GCN.
And then, uh, I'm also going to talk about the third method
which is called- which would be about simplifying the GCN, uh,
in such a way that we simplified the architecture so that, uh,
operations, uh, computation can be efficiently performed on a CPU.
And because CPU has a lot of memory,
we can scale, uh, to a large graph.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 17.2 - GraphSAGE Neighbor Sampling.txt
Let's talk about neighborhood sampling.
And this is really,
the key idea of the GraphSAGE,
uh, paper or the GraphSAGE architecture.
So what the GraphSAGE, uh,
what brought to the field of graph neural networks,
is a way to think about mini-batch implementations.
Before all the implementations were full batch and
people would then run- run graph neural networks on you know, 3,
4, 5,000 node graphs because that's what they could fit into GPU memory.
And what GraphSAGE , uh,
changed is they, it changed the way we think of graph neural networks,
the way we can think of creating minibatches,
and this also means that we can scale them up to graphs of,
uh, tens of billions of nodes and edges.
So, uh, let me now tell you about what do I mean by,
um, a- a neighborhood sampling.
So, uh, let's recall,
let- let's remember how, uh,
graph neural networks work and they operate through this notion of a computational graph.
Where GNN generate node embeddings via neighborhood aggregation,
meaning they aggregate features information from the neighbors,
and then they create a new message and pass it on.
So the way you can think of it,
if you say I want to create an embedding for this node, uh,
0, I will take neighbors and neighbors of neighbors.
Here is now my computation graph, um,
where nodes start with their individual feature vectors at the level 0.
Uh, these feature vectors get aggregated,
transformed, passed to the next level, um.
And then again, the same thing gets aggregated
transformed all the way to node- node of interest,
node 0, that makes the prediction.
Notice here that these nodes, you know, er,
the node 0 here,
means this is the feature vector of the node, uh,
0, um, er, for example,
here this means that this will be now a layer 1,
er, representation of nodes,
er, 1, 2, 3.
And here I have now a layer 2 representation of node ze- of node 0.
So what- what do I mean by this is that,
actually nodes have multiple representations,
uh, one for each, uh, layer.
And that's the- that's why when you do full batch,
you want to take level 0 embeddings to create level
1 embeddings of everyone, to create level 2 embeddings from everyone.
But here, because we have this local view,
we will only need to create level 1 embeddings for nodes 1, 2,
and 3, because we need those to create a level 2 embedding,
uh, for node uh, 0.
So that's, uh, the idea, right?
So the important observation is that a two layer GNN generates embedding for this node 0,
using got two-hop neighborhood structure and features around, uh, this graph.
So if I wanted to compute the layer, uh,
the embedding of node 0,
what I need is the graph structure,
plus features of this two-hop neighborhood around the- the node,
and I can ignore the rest of the graph.
I don't need to know the rest of the graph, right?
The embedding of node 0 will be independent of how
the graph structure is beyond the two-hop, uh, neighborhood.
And that's an important insight.
Because this means if I want to have a K layer GNN to emb- to generate
an embedding of a node using these K-hop neighborhood structure and features,
this means I only need to, um,
know at the time and I'm generating that the embedding that K-hop neighborhood.
And I can ignore the rest of the network.
And now, if the level k is, uh,
a relatively small or the neighborhood size is not too big,
which means that it is let say constant size,
but the entire graph can be much, much bigger,
then it means I need relatively little information,
or relatively little memory to use to generate or create the embedding,
uh, of node, uh, 0.
And, uh, this is the main insight.
The main insight is that we wanna- that to compute the embedding of a single node,
all we need is the K-hop neighborhood structure, uh,
around that node, and we can ignore the rest of the network.
The rest of the network does not affect the embedding of this node of interest,
um, all that affects it is the K-hop neighborhood around that node.
So what this means is that now we can generate minibatches in such a way that we say,
let's sample M different nodes in a mini-batch,
but we won't only put,
we won't put nodes into the mini-batch,
but we are going to put entire computation graphs into the mini-batch.
So it means that I would pick a first node and I'll
take its K-hop neighborhood computation graph,
and this is one element in the batch.
And then I'm going to sample the second node,
create its computation graph and put this into my mini-batch.
And so on and so forth.
So now this means perhaps my batches will be smaller
because batches are not now composed of individual nodes,
but batches are composed of network neighborhoods,
batches are composed from computation graphs.
So we are going to sample M computation graphs, um,
and then put these M computation graphs into the GPU memory,
so that we can compute the loss over this mini batch of M computation graphs.
So to emphasize again,
you know, what is the key idea?
The key idea is the following.
It starts with the insight that in order to compute the embedding of a given node,
all that we need to know is the K-hop neighborhood of that node.
So this means if I create a mini
batching not based on the nodes but based on the K-hop neighborhoods,
then I will be able to compute- um,
I will be able to compute the gradient in a reliable way.
So basically, rather than putting nodes into mini-batches,
we are putting, uh,
computational graphs, or in other words,
K-hop neighborhoods into the-,
uh, into the mini-batches.
Now, um, because we have put, um,
uh, we have put, uh,
uh, entire computation graphs,
entire network neighborhoods of K-hops into the, into the mini-batch,
we can, uh, consider the following, uh,
stochastic gradient descent strategy to train the model parameters, right?
We are going to sample,
um, let say M nodes.
For each node, we are going now to sample
the entire K-hop neighborhood to construct the computation graph.
And the we are, uh,
assuming that we have enough memory, um,
that we can fit into the mini-batch
both the nodes as well as their entire computation graphs.
Now, we have the complete set of information we need
to compute an embedding of every- every node in the mini-batch,
so we can then compute the loss over the- this mini-batch,
and we can then perform stochastic gradient descent to
basically update the model parameter with respect to the gradient,
um, er, to that,
uh, mini-batch, uh, loss.
And this is stochastic gradient because,
um, the batches are randomly created, so uh,
the- the gradient will have a bit of,
uh, randomness in it, but that's all fine.
It just means we can do, uh, great,
um, updates very, very fast.
So that's the, that's the idea.
So, um, if we, uh,
do it the way I explained it,
then we have, uh, still an issue with this notion of, uh,
mini-batches and stochastic training because for each node,
we need to get the entire K-hop neighborhood and pass it through
the computation graph and load it into the GPU memory.
So this means we need to aggregate a lot of information just to compute one- one node,
uh, e- e- embedding for a single node.
So computation will be expensive.
So let me tell you why it will be expensive.
First, it will be expensive because,
um, um, uh, the deeper I go,
the bigger these computation graphs and
these computation graphs are going to increase- um,
their size is going to increase exponentially, uh,
with the- with the depth of
the computation graph because even if every node has just three children,
it's going to increase, uh,
exponentially with the number of layers.
So that's one issue.
So the computation graphs are going to get very big if they get
very deep and then the second thing is that in natural graphs,
think of the lecture when we talked about
Microsoft Instant Messenger network when we talked about degree distribution,
we have these celebrity nodes,
these high degree nodes, uh,
that a lot of other people connect to or a lot of other nodes collect- connect to.
We have such nodes even in knowledge graphs.
If you think about, you know,
a node corresponding to a large country,
let's say like, uh, USA, um,
it will have a huge degree because there is
so many other entities related to it and of course,
a small country will have a much smaller degree
because there will be a smaller number of entities related to it.
So the point is, we'll have these hub nodes.
And now if a hub node has degree 1 million, uh,
which is nothing out of ordinary,
then you'll have to aggregate here information from 1 million nodes.
So this computation graph will get huge very quickly and you are
most likely going to hit these hub nodes, uh, very often.
So the point is,
you cannot take the entire K-hop neighborhood in most of the cases. So what do you do?
What do you do, is to do- is to
ap- apply on a project that is called neighborhood sampling.
Where the key idea is to cra- construct
a computational graph by sampling at most H neighbors,
uh, of every node.
So it means that every- every node- every node in the tree- in
the computation graph is going to have at most- is going to aggregate from at most,
uh, H other nodes.
So in our case, just to give you an example,
if I say, let's say H equals 3,
then my original computation graph is now going to be pruned in such a way that
every- every aggregation is going to aggregate from at most two other nodes.
So here, this entire branch is going to be cut out and you know,
some other nodes are going to be cut out as well.
And what this means is that now our, um,
computation graph will be much more manageable because we have just,
um, even if we hit the high degree hub node,
we are only to take,
uh, a fixed number of its neighbors,
uh, in the aggregation.
So the point is that you can use these print
computational graphs to more efficiently compute, uh, node embedding.
Um, so now, uh,
how do you do this computational graph sampling, right?
Basically the idea is for every node, for every layer, uh,
for every internal, um,
node of the- of the computation graph,
uh, we are going to, uh,
first basically compute the K-hop neighborhood, uh,
from the starting node and then for every, uh,
node in the computation graph,
we are going to pick at most, uh,
K, uh, H random, uh, neighbors.
Um, and this means that the K-layer GNN will, uh, involve, uh,
at most, uh, um, uh,
the product of the- of, uh,
H leaf nodes in the computation graph.
So our computation graphs are still gro- gro- going to grow, uh,
exponentially but the, uh,
but the point will be that,
uh, their fan out,
will be- will be, uh,
upper bounded by H. So the- the growth won't be uh,
that bad or that fast and we'll still be able to go,
uh, quite, uh, deep.
Now, let me make a few,
uh, remarks about these.
Uh, first remark is that there is, uh, the trade-off,
uh, in- in how many neighbors do we sample, right?
The smaller, uh, H leads to more efficient neighborhood aggregation
because computation graphs would be smaller but results in more unstable,
uh, training because we are ignoring
entire subparts of the network when we are doing message aggregation.
So our, uh, um, uh,
gradient estimates will be more, uh,
noisy, they will have higher, uh, variance.
Uh, another thing is that in terms of computation time,
even the neighborhood sampling,
the size of the computational graph,
as I said is still exponential with respect to the number of layers but
if H is not that large and we don't go too deep in terms of K,
uh, then it is still man-, uh,
uh, uh, manageable, right?
Um, so, you know, adding one more layer to the, uh,
to the GNN makes the computation H times,
uh, more expensive and now,
you know, if, uh,
H is maybe an order of 5-10,
and K is also, I don't know, on order of, you know,
5 plus minus, then you can still,
uh, keep, uh, doing this.
And then, uh, the last, uh,
the last important thing I want to mention is remark number 3 which is
this- this approach gives you a lot of freedom how you select the nodes,
uh, to sample and so far I don't- I call it a random sampling, right?
Just uniformly at random pick e- at H neighbors,
uh, of a given node.
Uh, but the- the issue is with this approach is that,
uh, real-world networks have these highly skewed degree distributions.
So there is a lot of kind of single- single nodes or low degree nodes in the network.
And if you are taking an, uh,
a given node of interest,
and sample H of its neighbors,
you are most likely going to sample this like degree 1, uh,
nodes that are not perhaps the most important nodes in
the network and perhaps very noisy, not the most informative.
This could be users that are not very engaged,
this could be pieces of content that are
not very important and you don't have a good signal on.
So what you can do,
and this works much better in practice,
is that you'll do a random walk with restart from the node of interest,
here, the green node.
And then your sampling strategy is to take, uh, uh,
H neighbors but not at- let's say not at random,
but based on, uh,
their random walk with restart scores.
So it means that at every layer you are going to take H most important neighbors,
uh, of a given node.
And, uh this means that the graph
you are going to select will be kind of much more representative,
much better connected, um,
and it will have- it will be based on
these more important nodes which have better and more reliable feature information,
these are more active users,
so they kind of provide you more information for predictions.
So in practice, this strategy of sampling the computation graph,
um, works much better,
um, and, you know, there is- I think here to say,
there is room to do a proper investigation about
how would you define what are different strategies to sample,
to define the computation graphs,
to sample the computation graphs and ha-
what are their strategies in which kind of cases?
I think this still hasn't been, uh,
systematically investigated but such a study would be very important,
uh, for the field of,
uh, graph machine learning.
So, uh, to summarize the pro- the neighborhood sampling approach,
the idea is that the computational graph is constructed for each node, um,
and the computational graphs are put into the mini- mini-batch because
computational graphs can become very big very
quickly by hitting a high degree hub node, um,
neighborhoods- we then proposed neighborhood sampling
which is where the computational graph is created
stochastically or is pruned sub-sampled to increase computational efficiency.
It also increases the model robustness because now the GNN architecture is,
uh, stochastic by it's self,
so it's almost like a form of dropout if you want to think of it that way, uh,
and the computa- pruned computational graph is used to generate node embeddings.
Um, here, uh, caveat is that if- if your network,
uh, GNN- number of GNN layers is very deep,
these computation graphs may still become large which means
your batch sizes will have to be smaller, um,
which means, uh, your gradient will be, um,
uh, uh, kind of more, uh, less reliable.
So if the batch size is small,
the, uh, the gradient is less reliable,
and if the pruning is too much,
then again the, uh,
gradient, uh, gradients are not too reliable.
So it's important to find a good balance between the batch size and, uh,
the pruning factor or sampling factor for the computation graphs.
But that's essentially the idea and this is really, I would say,
what most of the large-scale industrial implementations of graph neural networks use,
uh, to achieve, uh,
scaling gap to industrial size graphs.
For example, this is what is used at Pinterest,
at Alibaba and so on.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 17.3 - Cluster GCN Scaling up GNNs.txt
Now we are going to talk about the second method in this idea of neighborhood, uh,
sampling and trying to limit, uh, the- the,
uh, batch size, uh,
issues, uh, in graph neural networks.
Um, and the way we are going to do this is,
uh, to start with the following observation, right?
That the size of computational graphs, um,
can still become very big, uh,
or increases exponentially with the number of, uh, GNN layers.
And what we can also observe and notice is that, um,
computation in these, uh- computational graphs
can be very much redundant because of shared neighbors.
Like if you think for example, two nodes,
A and B, and you think about the computation graphs,
then they have, uh- because they share- they have a lot of friends in common,
these computation graphs are heavily redundant, right?
Like the- the, uh,
both to compute A and compute B,
the subgraphs the comp- the parts of the computation graphs
between- below node C and D are identical.
So this would be the duplicate- this will be duplicated, uh, um, computation.
And, uh, there are two approaches to do this.
One is to, uh, realize that this, uh,
computation is duplicated then computed only once.
Uh, there is a very nice paper about this at last year KDD called,
uh, uh, HAGs, hierarchical, um,
aggregation, uh, graphs that basically prevent,
um, uh, multiple computations, so redundant computations.
Um, and the- the second case will be about Cluster-GCN,
which is what I'm going to talk about today.
And the way we are, uh, motivating Cluster-GCN is to remember what is a full ba- batch,
uh, graph neural network, right?
A full batch graph neural network is that all the embeddings
for all the nodes are embedded together in a- in a single pass.
So basically, um, embeddings of all the nodes are computed for layer 1,
embeddings for all the nodes are then computed for layer 2,
uh, and so on, right?
The point is you need to have the embeddings of
all the nodes at lab- at layer L minus 1 to be
able to compute the embeddings for all the other nodes- for
all the nodes at layer L. So it means that,
uh, in each layer,
2 times number of edges,
uh, messages need to be, uh,
exchanged because every node needs to connect-
collect the- the message is from its neighbors,
so there are two messages along each undirected edge, you know,
for both, uh, endpoints,
and, uh, what is the observation?
The observation is that, uh,
the amount of computation we need to do in a single layer of, uh, let's say,
uh, full batch, uh,
implementation is linear in the size of the graph.
It's linear in the number of edges.
So it means it is very fast.
But the problem is that the graphs are too big.
Uh, so we cannot do this kind of at once,
uh, in the GPU.
So the insight from full-batch GNN is that
layer-wise node embedding updates allow us to reuse embeddings from the previous layer.
Um, and this significantly reduces the computational redundancy of,
uh, neighborhood sampling, right?
Uh, this means that this layer-wise update to generate
embeddings from one layer to the next is very cheap, right?
All I need to do is previous layer embeddings.
I need to aggregate them,
combine them with the previous layer embedding of a given node
and I have a new, uh- a new embedding.
So what this means is that,
uh, this can be done very fast.
It's only linear time operation in the size of the graph because it's aggregation,
uh, uh, basically takes time linear because
it's linear number of edges over which we aggregate.
So this sig- means that the computations are very fast.
But of course the problem is that layer-wise update is not feasible
for the entire graph at once because of the GPU memory.
So now, what the idea is in the Cluster-GCN is that can we sample
the entire graph into small sub-parts and then
perform full batch implementation on those subgraphs.
All right. So the idea is the following.
I'll take the large graph.
I'm going to sample a subgraph of it.
I don't know maybe I'll just say I'll sample the subgraph,
and then I'm going, uh, in this subgraph I'm able to fit it on a GPU.
So now I can apply a full batch implementation,
uh, in the GPU on that sampled subgraph.
All right? So that's essentially the idea.
So the difference between neighborhood sampling is that
there we compute the computation graph for each individual node.
Um, here, we are going to sample an entire subgraph of the original graph and
then pretend that this is the graph of interest and
just perform GNN on that, uh, subgraph.
Right? So then the question is,
how should I sample these subgraphs?
What are good subgraphs for training GNNs?
Um, and here's the reasoning.
The important point to remember is that GNN performs
embedding by passing messages via the edges of the network, right?
Every node computes its embedding by collecting
information from the neighbors, uh, right?
So the idea is now if I wanna have a good subgraph,
the good subgraph have- have- has to retain as many of the edges of the original graph as
possible so that my computations mimic
the computations on the big graph as well as possible.
So our subgraphs should retain
edge connectivity structure of the ne- original graph as much as possible.
Which means that the GNN over the subgraph generates embeddings that are close,
uh, to the- to the GNN over the original graph.
So to give you- to give you an idea,
if I have an original graph and let's say I sample, uh,
I- I sample a subgraph on four nodes like I show here on the left
or I sample another subgraph on four nodes like I show on the right,
then it's clear that the left subgraph is kind of much better for training.
Because if I now say,
let's compute the embedding of this corner node up here,
here, all the three neighbors of it are in the- in the subgraph.
So I'll be able to get a quite good estimate of the embedding of this,
uh, node, um, corner node up here.
So right, it's- all of its, um, uh,
uh, neighbors are- are part of my subgraph.
While if I take this other subgraph here,
we- denoted by green nodes.
And I wanna predict or generate an embedding for this green node as well,
then- then I- all I'm able to do is aggregate message from one of its neighbors,
because only one of its neighbors is part of my, uh, subgraph, right?
So the right subgraph drops many- many edges,
um, and leading to isolated nodes,
which means my computation graphs over this subgraph won't
be representative of the computation graphs in the original graph.
So left choice is far better,
uh, than the right choice.
And this now brings us back to
the network community structure because real world graphs exhibit community structure.
They exhibit clustering structure.
So large graphs can be decomposed into many small, uh, communities.
And the key insight would be that we can sample these subgraphs based on
the community structure so that each subgraph
retains most of the edges and only few edges, uh, are dropped.
So a Cluster-GCN, the way it works,
it has, uh, uh, a two-step process.
In the preprocessing step,
we take the original graph and split it,
cut it into many small,
uh- small, uh, subgraphs.
Uh, and then, uh, mini-batch training means that we sample one node,
group 1 subgraph, uh, and, uh,
perform the- the full batch message passing over that entire subgraph,
compute the node embe- uh- node embeddings of all the nodes in the subgraph,
evaluate the loss, compute the gradient,
and update the parameters, right?
So the idea is input graph,
split into many subgraphs.
We take one subgraph,
fit it into the GPU memory,
do the full computation on the subgraph, compute the loss,
update model parameters, load in another subgraph,
do the, uh, a full- full batch,
uh, layer embeddings of all the nodes in the subgraph.
Compute the loss, update the gradient,
load, uh, the next subgraphs.
So that's how vanilla Cluster-GCN, uh, would work.
So to give you more- more details,
the idea is that given a large, uh, graph,
we're going to partition it into,
let's say, capital C groups.
Um, we can use some scalable community detection method like Louvain, uh,
like METIS method, um,
or we can use, uh,
even BIGCLAM is fine,
um, and then basically this,
um, set of, uh,
node groups V. Uh,
we are going to take an induced subgraph on the subset of nodes,
um, and basically these induced subgraphs will
include all the edges between the members, ah,
of the- of the group, and of course,
these edges that go across the subgraphs,
those will be dropped.
[NOISE] So between group edges are going to be dropped in these,
uh, uh, graphs, G1 to GC which are these subgraphs we sampled.
So now, for each, uh, mini-batch,
we are going to sample one such node group, one such subgraph,
create an induced subgraph over that set of nodes,
and this is now our graph that we are going to use,
um, as a computation in the GPU.
So the point is that this induced subgraph has to fit into the GPU memory.
So now, we do layer-wise node update
basically the same thing as what we do in the full batch, uh,
GNN to compute now the embeddings of all the nodes in the subgraph, recompute, uh,
the loss, uh, and then do the, uh,
compute the gradient with respect to the loss and then do the gradient update.
So the point here is that when we were doing GraphSAGE neighborhood sampling, our nodes,
we were able to kind of sample them all- all- all across the graphs.
But in this case, um,
we are going to sample a subgraph of
the original graph and now only compute on those nodes,
uh, we have sampled, and that's the big difference between,
ah, the neighborhood sampling that can be kind of,
those neighborhoods can be distributed across the entire network while here,
we are going to create those neighborhoods basically just being limited to
the subgraph we sample and we feed that subgraph into the GPU memory.
So what are some issues and how do we fix them?
The issue with Cluster-GCN is that these induced subgraphs are removed in- in,
uh, between the group links, between subgraph links.
And as a result, messages from other groups
will be lost during message-passing which will hur- which can hurt,
uh, GNN performance, right?
So for example, if I sample this red graph here and I just zoom in into the node- uh,
into it, what I see is that,
for example, whenever, ah,
this particular node aggregates information,
it will never aggregate information from these two edges because these two- uh,
the node endpoints are part of the se- the different subgraphs.
So all the message aggregation will happen only between these four nodes,
uh, and the five,
uh, edges between them,
so there will be in some sense, uh, lost messages.
So, uh, the issue will be because the-
the graph community detection put similar nodes together into the same group,
uh, sample node groups tend to cover
only a small concentrated portion of the entire graph, right?
So we are going to learn only on a small concentrated portions of the graph while
neighborhood sampling allows us to kind of learn on small but very diverse,
uh, set of, uh, neighborhoods.
Um, and this means that because this sample node,
sample subgraph vor- will be- will be our concentrated part of the entire graph,
it won't be kind of diverse or a representative to
represent the entire, uh, training graphs.
So what this means is that when we compute the gradient and the loss, uh,
this is going to fluctuate a lot from one node group to
another because- because of the social communities,
uh, if you think of it that way.
Each subgroup will be very concentrated in one given
aspect and the- the model will have hard time to learn, right?
You will- I know- you know,
you have a cluster of computer scientists,
so you will learn how to make- you will compute
the gradient with respect to computer scientists,
then you have a cluster of, let's say, uh,
music students and now your gradient that
you have just- your models that were just computed over
the computer scientists in your social network are now going to
be computed over music people which are very, very different.
And then I don't know, you have mathematicians who are
different- community of mathematicians who are different from the two.
So the gradients are going to fluctuate quite a lot,
and training is quite unstable which in practice leads to slow convergence of,
uh, gradient descent or stochastic gradient descent.
So, uh, a way to improve this is to- is what is called Advanced Cluster-GCN.
And the idea is that here,
we wanna aggregate multiple node groups per mini-batch, right?
So the idea is to- to make the graphs we sampled even
smaller but then we are going to sample multiple of these,
uh, subgraphs, uh, into the mini-batch.
Um, and we are going to create, uh,
induced subgraph on the aggregated node group that is now composed of many small, uh,
node groups, and the rest will be the same is- I- as in the Cluster-GCN,
which is take now the induced subgraph,
put it into the GPU memory,
and create the update.
But the point is now that the subgroups can be more
diverse so the induced subgraph will be more diverse,
so your gradients will be more stable and more, uh, representative.
So here is the picture,
the picture is that now my node groups are smaller than what I had before,
but I can sample more of them.
So I sample here two node groups and now
the sample subgraph is the induced subgraph on the nodes belonging to the two groups.
So I will also include, uh, this,
uh, uh, blue, uh, edge that kind of connects the two groups.
So it means that, um, uh,
this makes the sample nodes more representative, more diverse.
They are more representative of the entire node population,
which will lead to better gradient estimate,
less variance in gradients,
and faster training, uh, and convergence.
So, uh, just to say more, right?
This is again a two-step approach.
I take the nodes, um,
I- I separate them into very small, uh,
subgroups, much smaller than the original version, but, uh,
when I'm doing mini-batch training,
I will sample multiple groups of nodes, um,
and then I will aggregate these groups of nodes into one super group,
and then I'm going to create an induced subgraph on the entire, uh, super group.
And then, um, I'm going to include the edges between all the members of the super group,
which means it will be edges between the- the small, uh,
subgroups as well as the edges between the small, uh, subgroups.
Um, and then I will perfo- perform, uh,
Cluster-GCN, uh, the same way as we did before.
So now, um, if I compare the Cluster-GCN approach,
with the neighborhood sampling approach,
here is how the two compare.
The neighborhood sampling says,
uh, sample H, uh,
nodes per layer, um,
and let's do this for,
uh, M nodes in the network.
So the total size of the mini-batch will be
M times H raised to the power of K. M is the number of computation graph,
uh, H is the fan out of computation graphs,
and K is the number of layers of the GNN.
So for M, uh,
nodes in the computation graph,
our cost in terms of memory as well as computation will be M times, uh, H_K.
In the Cluster-GCN, um, uh, the- uh,
the- we are performing message passing over a subgraph induced by M nodes,
and the subgraph over M nodes is going to
include M times average degree number of edges, right?
Each node in the subgraph has an average degree of, uh, D average,
so in total, we- we'll have M times,
uh, average degree, uh,
number of edges in the graph.
And because we are doing K hop, uh, message-passing,
the computational cost of this approach will be
K times M size of the subgraph times the average degree.
So if you compare the two, in summary,
the cost to generate embeddings from M nodes in a K layer GNN is M to the- M times H_K.
In Cluster-GCN, it is, um,
K times M times average degree,
um, and, um, if you assume, let's say, uh,
H to be half of the average degree,
uh, then it would mean that, ah,
Cluster-GCN is much more computationally efficient,
um, than neighborhood sampling.
So, um, it is linear ins- instead of
exponential with respect to the- to the- to the depth.
So what do we do in practice?
It depends a bit on the- on the dataset.
Usually, we set H to be bigger than the half average degree,
uh, perhaps, you know, two times,
three times, ah, average degree, um, and, uh,
and because number of layers K is not- uh,
is not that deep, um,
the- the neighborhood sampling approach tends to be kind of more,
uh, used, uh, in practice.
So to summarize, Cluster-GCN first partitions the entire,
um, set of nodes in the graph into small node groups.
In each mini-batch, multiple node groups are sampled, um, and their,
uh, and their nodes are kind of,
um, uh, aggregated together.
Then an induced subgraph on this, uh, uh, um,
node- nodes, uh, from the union of
the- of the groups that we have sampled, uh, is created.
And then the GNN performs
layer-wise node embeddings update over this induced, uh, subgraph.
Generally, Cluster-GCN is more computationally efficient,
than neighborhood sampling, especially when the number of layers is large, um,
but Cluster-GCN leads to systematically biased gradients
because of missing cross-community edges and also because,
uh, if, uh, number of layers is deep,
then in the original graph,
um, if you do neighborhood sampling,
you can really go deep.
But in the Cluster-GCN,
you are only going to go to the edge of the original of the sampled graph,
and then you'll kind of bounce back.
And even though you have a lot of depth,
this depth would be kind of oscillating over the subgraph
and won't even ec- really explore the real depth,
uh, of the underlying original graph.
So, uh, overall, um,
I would say neighborhood sampling is used more because of additional, uh, flexibility.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 17.4 - Scaling up by Simplifying GNNs.txt
So, um, the third topic I wanna discuss is
scaling up graph neural networks by simplifying their architecture.
Um, and this is kind of an orthogonal approach,
uh, to the first two approaches.
And here is how we are going to do this.
We will start from a graph convolutional,
uh, uh, network, uh,
just as an example architecture,
and we are going to simplify it by removing
the non-linear activation from the GCN, right?
And actually, there's been a paper, uh,
two years ago or a year and a half ago that demonstrates that
the performance on- on benchmarks is not too much lower because you have,
uh, removed the non-linearity.
Uh, and this means that now,
this simplified GCN architecture turns out to be an extremely scalable,
uh, model that we can train very fast.
So basically, the idea here is you are going to simplify
the expressive power of the graph neural network so that you can train it faster.
Of course, this kind of deb- defeats the purpose of deep-learning a bit
because the point of deep learning or representation learning is a lot of data over,
uh, over complex models that can model complex, um, uh, representations.
Here, we are saying, simplify the model so it's fast to run on big data,
and of course, there is a trade off there.
So let me, uh, introduce the, uh,
GCN, uh, and then we'll continue from there, right?
So GCN takes in, uh, a graph,
uh, with, uh, node features.
And let's assume that every node includes a self-loop.
Uh, this would be, uh, convenient for,
uh, mathematical notation later.
And let's think of this as a full batch implementation, right?
So we basically set that the node embeddings
at layer- at layer 0 to be simply node features,
and then we are going to iterate for K,
uh, K layers, uh, where at, uh,
every- every node at layer k plus 1 is going
to take the embeddings of its neighbors from the previous layers,
sum them up, um,
and divide by the number of layers- number of neighbors, uh,
um, transform by this learned matrix and pass through a non-linearity.
And then we're going to run this, uh,
recursion for k iterations and whatever we end up in the end with,
whatever embedding we end up at the end,
that is what we call, uh, final embedding.
Now, what is- benefit of GCN is that it is so simple.
We can very nicely write it into the- in the matrix form.
So the way we are going to write it in the matrix form is that we are going to take
these embeddings and we are going to stack them into an embedding matrix.
And then A is our adjacency matrix where every node also has a self-loop.
Then the way you can write the sum over the neighbors of a given node- sum
of the embeddings of a- of a given no- of the neighbors of a given node,
you can simply write this as a- as a product between the adjacency matrix A
and the- and the embedding matrix H. And, uh,
what this means is that now,
we can also define this notion of D to be
a diagonal matrix where we- it is all of 0 only on the diagonal,
we have the degree of every node.
And then the inverse of D,
D to the minus 1 is the inverse of
the diagonal matrix which is just you put 1 over the degree,
uh, on the- on the diagonal entry for every node.
So now, the way you can write a summation over the neighbors
divided by the degree of that node is simply, uh,
the inverse of the diagonal matrix,
so one over the degree times the adjacency matrix A times the hidden- uh, uh,
embeddings matrix H. So this means that now,
given H at level l,
if we multiply it by A and multiply it by D to the minus 1,
we get the matrix of node embeddings at level h plus 1.
So basically, what is elegant here is that you can rewrite
this iteration just as a product of three matrices,
and of course, in the GCN,
we also have a ReLU non-linearity,
um, uh, and, uh, transformation here.
So going back to the GCN,
here is the node-based, uh, uh,
formulation of the GCN,
if you write it in the matrix form,
you can write it that this is
a non-linearity times this A tilde that is simply degree times, uh,
A, um, times the previous layer embeddings times,
uh, the W matrix which is the transformation matrix.
So basically, the equation- this equation,
uh, that is based on the network and the following matrix equation, they are equivalent.
So if you've- if you've computed this product of these matrices,
you have just computed that the k plus 1 layer
embedding for all the nodes of the network, and now,
you can kind of iterate this capital K times to compute the k layers.
So, um, this is what a matrix formulation of our GCN looks like.
So let's now go and simplify the GCN.
Let's assume and go and remove this ReLU, uh, non-linearity.
So let's- lets say,
what would happen if the GCN would- would be governed by the following equation, right?
So going back, this is the equation with a non-linearity, now,
we decided- now we decided to drop the non-linearity,
so here is our, you know,
simplified the GCN equation.
So now, let's go and unroll this- uh,
this iteration, let's- let's unroll this recursion, right?
So we say, ah, here is the final layer embeddings for the nodes,
uh, they depend on the layer k minus 1 embeddings of the nodes.
So let's now take this H_k minus 1 and insert it, uh,
with the way we compute, uh,
H_k to the minus- H to the layer k minus 1.
And you know, I take this part and just I insert it here.
And now, I notice this depends on H_k minus 2.
So again, I can go take H_k minus 2 and again expand it.
And if I do this all the way down to the 0 layer,
then, um, I- I know what to do, right?
Like H0 is simply the vector of,
uh- of, uh, node features X.
These A tildes just kind of get multiplied together.
So this is A tilde raised to the power k. And this here at the end, these, uh,
parameter matrices, it is just a product of parameter matrices that is still a matrix.
So I can rewrite this, uh,
recursive equa- equation if I expand it in the following way,
and then I realized that a product of
parameter matrices is just a parameter matrix, right?
So basically, I have just rewritten the K layer
GCN into this very simple equation that is non-recursive.
It's A tilde raised to the power k
times the feature vector times the transformation matrix.
So what is important?
What do you need to, uh,
remember here about A tilde raised to the power k?
Remember in I think Lecture 1 or 2,
we talked about what does powering the adjacency matrix to the- to the kth power mean?
It means we are counting paths,
it means we are connecting nodes that are neighbors of-
that are neighbors- neighbors of neighbors and so on.
So basically, what this means is that this til- A tilde to the k really
connects the target node to its neighbors, neighbors of neighbors,
neighbors of neighbors of neighbors,
and so on, um,
one-hop farther out in the network as we increase,
uh, uh, K, um,
and that's very interesting.
So now, why did- what can we conclude?
We can conclude that removing the non-linearity significantly simplifies the GCN.
Uh, notice also that, um,
these A tilde to the k times x does not
include- contain any learnable parameters so we can
actually pre-compute this on the- on the CPU even before we start, uh, training, right?
And this can be very efficiently computed because all I have to do is,
um, multiply the mat- uh,
A tilde with, uh,
x, uh, kind of with itself, uh,
multiple times, and I will be able to get to the A tilde raised to the power K times x.
So computing this part is very easy,
it does not depend on any, um,
model parameters so I can just pre-compute it even before I start learning.
So now, um, how about,
uh- so what we have learned is that this, uh,
A_k times x could be pre-computed and let's call this X tilde.
So now, uh, the simplified GCN,
the final embedding layer is simply this X tilde times the parameter matrix,
and this is just a linear transformation of the pre-computed matrix, right?
So the way I can- I can think of this,
this is basically just a pre-computed feature vector for node,
uh, v, uh, times its- uh,
times the matrix- learned matrix W. So embedding of node,
uh, v only depends on its own pre-processed, uh, features,
right, where I had- I say this X tilde is really
X tilde computed by A- A tilde raised to the power K times X, right?
But this is a matrix that has one row for every node,
so if I say what is the final layer embedding of a given node?
Is simply the- the corresponding row in
the matrix times the learned parameter matrix, uh,
W. But what is important to notice here is that once this X tilde has been computed,
then learn- then, uh, um,
the embedding of a given node only depends on
a given row of the X tilde that is fixed and constant.
And, uh, the only thing that changes that is learnable is W. So what this means is
that embe- embeddings of M nodes can be generated in linear ti- in the time linear,
um, with M because for a given, uh, node,
its final embedding only depends on its own,
uh, row in the matrix, uh, X tilde.
So I can easily sample a mini batch of nodes,
I sample a set of rows from the matrix X,
and then I multiply that with W to get the final layer embeddings of those,
uh, nodes, uh, in the mini-batch.
So, um, and of course this would be super fast because there is no dependencies between
the nodes or all the dependencies are already captured in this matrix, uh, X tilde.
So in summary, uh, Simplified GCN consists of two steps,
the pre-processing step where- uh,
where a pre- where we pre-compute this X tilde to be simply the adjacency matrix A,
um, with, uh, one over the degree of the node on the diagonal.
We call this A tilde,
we raise this to the Kth power so we multiply it with itself,
K step, K times,
and then we multiply this with the original features of the nodes x.
And all of this can be done on a CPU even before we start training.
So now what this means is that we have this matrix, uh, uh, uh,
X, uh, X tilde that has one row for every node, um,
and that's all- all we need for
the training step where basically for each mini-batch we are going to sample uh,
M nodes at random,
and then we simply compute their embeddings by taking,
uh, W and, uh,
multiplying it with uh, with uh,
uh, with the corresponding, uh, uh,
entry in the- in the matrix, uh,
X tilde that corresponds to that,
uh, node, uh, um, uh, uh,
v. And we simply compute the final layer embeddings of all the nodes in the mini-batch,
um, and then, you know,
we can use these embeddings to make predictions,
compute the loss, uh,
and then update the matrix- the parameter matrix,
uh, W. So, um,
the- the good point here is that now the embedding of every node,
uh, computation of it is independent from the other nodes.
It is a simple, uh,
matrix times vector product where
the matrix W is what we are trying to learn and this can be done,
uh, super, super fast.
So, um, now let's,
uh, uh, summarize and kind of, uh,
compare, uh, uh, Cluster-GCN with,
uh, uh, other methods we have learned, uh, today.
The simplified GCN generates node embeddings much more, uh, efficiently.
There is no need to create the giant computation graph,
there is no need to do graph sampling,
it is all very, very simple, right?
You just do those matrix products and then the learning step is also very easy.
So, um, it seems great, but what do we lose?
Um, right, compared to Cluster-GCN,
mini-batch of nodes in a simplified GCN can be sampled completely at random from,
uh, the entire set of nodes.
As I said, there is no need to do group- uh,
to do node groups to do the,
uh, like in Cluster-GCN,
no need to do the induced subgraph, uh, nothing like this.
So this me- this means that, uh,
our- the training is very stable, um,
and the- the variance of the gradients,
uh, is much more under control.
But, right, what is the price?
The price is that this model is far,
far, uh, less expressive.
Meaning, compared to the original graph neural network models,
simplified GCN is far less
expressive because it lacked- it doesn't have the non-linearity,
uh, in generating, uh, node embedding.
So it means that the theory that we discussed
about computation graphs, Weisfeiler-Lehman isomorphism test,
keeping the structure of the underlying subgraphs, uh,
through that injective mapping,
all that is out of the window because we don't have the non-linearity anymore.
So that really makes a huge difference in the expressive power, uh, of the model.
Right? But, you know,
in many real-world cases,
a simplified GCN, uh,
tends to work well, um,
and tends to kind of work just slightly worse than the original graph neural networks,
even though being far less, uh,
expressive in, uh, theory.
So the question is, uh, why is that?
And the reason for that is something that is called graph homophily,
which basically means, uh,
this is a concept from social science, uh,
generally people like to call it, uh,
birds of feather, uh, sto- stick together.
So or birds of feather flock together.
So basically the idea is that similar people tends to connect with each other, right?
So that basically, you know,
computer scientists know each other,
uh, people who study music know each other.
So essentially, the idea is that you have
these social communities that are tightly compact where people share
properties and attributes just because it is easier
to connect to people who have something in common with you,
with whom you share some, uh, interest, right?
So basically what this means in,
let's say networks, both in social, biological,
knowledge graphs is that nodes connected by edges tend to
share the same labels they tend to have similar features.
Right? In citation networks,
papers from the same- same area tend to cite each other.
In movie recommendation, it's like people are interested in
given genres and you watch multiple movies from the same genre.
So these movies are kind of similar to each other.
So, um, and, you know,
why- why is this important for a simplified GCN?
Because, um, the t- the three- pre-processing step of
a simplified GCN is simply feature aggregation over a k-hop, uh, neighborhood, right?
So the pre-processing features obtained by- is- are
obtained by iteratively averaging, um, um, uh,
features of the neighbors and features of the neighbors of neighbors
without learned transformation and will- without any non-linearity.
So, as a result,
nodes connected by edges tend to have similar pre-processing,
uh, features, and now with labels, um,
are also clustered kind of across the homophilis parts of the network,
um, if the labels kind of cluster a- across the network,
then simplified GCN will work, uh, really well.
So, uh, basically, the- the- when does the simplified GCN work?
The premise is that the model uses the pre-process node features to make prediction.
Nodes connected by edges tend to get
similar pre-processed features because it's all
about feature averaging across local neighborhoods.
So if nodes connected by edges tend to be in the same class,
tend to have the same label, then simplified GCN,
uh, is going to, uh,
make very accurate, uh,
predictions, uh, so basically if the graph has this kind of homophily structure.
Now, if the graph doesn't have this homophily structure,
then the simplified GCN is going to fail,
uh, quite, uh, quite bad.
So that's kind of the intuition,
and of course, ahead of time,
we generally don't know whether labels are clustered together
or they're kind of the- kind of sparkled, uh, across the network.
So to summarize, simplified GCN removes non-linearity in GCN and then reduces,
uh, the- uh, to simple, uh,
pre-processing of the node features and graph adjacency matrix.
When these pre-processed features are obtained on the CPU, a very scalable,
simple mini-batch- batch gra- stochastic gradient descent
can be directly applied to optimize the parameters.
Um, simplified GCN works surprisingly well in- in many benchmarks.
Uh, the reason for that being is that those benchmarks are easy.
Um, uh, nodes of similar label tend to link to each other.
They tend to be part,
uh, of the same network,
and meaning that just simple averaging of their features, um, uh,
without any nonlinearities and without any weight or different weighing of them,
um, gives you, uh,
good performance, uh, in practice.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 18 - GNNs in Computational Biology.txt
Today, I will talk about graph neural networks in computational biology. And, um, there has been, um,
a tremendous surge in- of interest in leveraging, ah, graph neural networks and graph representation learning,
in particular, ah, for learning meaningful representations of biology and meaningful representations of datasets and
entities when co- we encounter in biology and medicine. And so over the last few years, we have seen, ah,
several prominent examples where GNNs have been used successfully, ah, to learn, uh, representations that have enabled critical predictions in a variety of downstream tasks,
including predictions about discovering novel interactions about- between proteins, information about, ah, uh, treat- uh,
succes- potentially successful disease treatments, um, opportunities about, uh, novel- regarding novel drugs and so on.
And so that has, um- that of course, is- is very interesting and particularly, um,
a natural question to ask is, why do we see that graph neural networks,
achieve such success in biomedical and biological data? And so to answer this question,
we need to think a bit about, um, [NOISE] biology and- and observe that biology is actually highly interconnected to the- uh,
in- in the sense that, um, the way drugs work when- when people take
them is not that they work independently of each other, but the way drugs work is they actually affect certain number o- of molecules, ah,
in humans cells that are typically proteins, and those proteins interact with each other and
then- then these effects are then propagated. The way they are propagating in- in human body and in
human cells is through those underlying biological networks. So we can really see that biology is interconnected at various different scales,
from the scale of genomic sequences where we see interactions between parts of the genome,
to the scale of interactions between proteins and molecules in- within a cell,
to- to interactions of external agents that are included in the human body, such as drugs and therapeutics,
[NOISE] all the way to the scale of populations or entire ecosystems where we have,
for example, health care systems and dependencies between different institutions, different hospitals, or different kinds of
symptoms within a heal- within healthcare records. So the effects of many of biological then interventions,
many of biological, um, e- entities are- they're really network effects. And because of that, it makes lots of sense to study the biology through the lens
of networks and interactions between biologically relevant entities. So to unpack this a bit,
we can try to understand why are networks in biology so powerful? And I'll look- I'll look- I will take a particular example to- to explain that.
So let's look for a- let's take a look at the human protein interaction network. This is a network where nodes,
um, represent human proteins, and two proteins are connected by an edge if,
um, the proteins physically interact. Meaning that at some point in time the two proteins are physically close in the cell,
and, um, you might want to know, well, why- why are then proteins interacting with each other?
Um, if we look at a particular sub-region of protein interaction network, which might be this particular subgraph on- o- on five human proteins where we see, ah,
five interactions, is there any significance between these two protein MXS1 and B9D2 so- that they- in,
uh- in- in the sense that, what does it mean and what are implications of them interacting with each other?
And to answer this question, what we can do is to overlay on that, um, protein interaction network
information about what are diseases that those proteins are involved in. And if we do that for this particular example,
we can see that the these three proteins that are highlighted here in red are all associated with the disease- with the same disease.
Meaning that when there are certain alterations in those two proteins, a particular disease would develop.
And so what is interesting to- to- to explore and obs- and observe is that if we overlay, um,
information about involvement of proteins in various diseases onto this- uh, onto protein interaction network,
we can see that proteins associated with the same disease tend to agglomerate or cluster in certain regions,
in certain neighborhoods of the network. And that has, over the last two decades, yield to very powerful paradigms in, um,
biology and medicine because it's powerful as it allows us to identify new proteins that might be involved in
disease by simply exploring the surrounding and lo- n- network neighborhoods of those proteins
that were already known to be associated with the disease. That particular longstanding paradigm is known as local hypothesis.
And in other words, it means that proteins involved in the same disease, proteins that when mutated are,
um, lead to the same disease, they- they have an increasing tendency to interact with each other,
meaning that they would be connected in our PPI network. So the implication of that local hypothesis is that, ah,
m- mutations in interacting proteins often lead to similar diseases. And now that we know that,
we can effectively leverage that information as inf- uh, as prior information for- for our- for our machine learning models.
While I'll explain this particular local hypothesis in the case of human protein interaction networks and diseases,
similar findings apply to a broad range of different kinds of networks that we can see in biology.
Going from protein interaction networks to networks where nodes are patients related and connected based on
familial or- relationships or similarity of the genomic profiles, to networks representing hierarchies of several systems,
networks representing disease pathways, um, gene interaction networks, cell-cell similarity networks, and many others.
And so that, um, notion of local hypothesis where we see that, um,
entities with similar properties all- all tend to cluster and agglomerate in certain network regions, um,
are- is really one of the core observations and motivation and guiding principle that can explain why
graph neural networks are so well suited for the analysis of biological networks and why they have already led to several very successful,
uh, applications to biomedical problems. So- so that's all great and fantastic vision that we can share.
It's a nice motivation for great- for why it makes sense to use graph neural networks for biological data.
But biological networks also present some fundamental challenges that we can solve through algorithmic innovation.
So there are a number of challenges, I will just mention tree here. Um, so the first challenge is that biological networks tend to involve heterogeneous interactions
that span from the mo- molecular level all the way to the level of whole populations or societies.
And so typically [NOISE] the challenge is how to computationally operationalize the data. What- how to even construct then- the graphs
that are then amenable to- to machine learning. The second challenge is that,
um, networks in biology, uh, contain data that you are co- that come from a variety of different resources,
including experimental readouts, curated annotations, and metadata. And we really need to consider all those data
jointly because no single datatype can really capture all the fact- factors that are necessary to
understand a particular complex phenomenon such as a disease. It really is, um,
that we wo- need to, uh, co- co- jointly consider the data from diverse resources to do so.
And finally, biological networks are by definition noisy and there by definition are incomplete because our understanding
of the nature and biology is incomplete. And they are noisy because inherent natural variations as well as
limitations of current biotechnological platforms that generate the data, which means that we need to operate with the assumption that our data will be missing,
it will contain po- repeated measurements, even contradictory observations that can plague the analysis,
and methods need to be robust against [NOISE] these issues. So the plan for today lecture is the following,
um, I will [NOISE], uh, talk about three vignettes that will highlight some of these challenge- some of the challenges, uh,
for- for wor- dealing with biological networks as well as propose solutions and show examples of
successful applications of graph neural networks in computational biology. So the part that I will touch are- are the que- first
will be the question related to modeling safety of drugs and, um, safety of combinations of drugs.
Second, I will talk about predicting patient outcomes and disease classification. And finally, we will,
ah, discuss, um, um, m- methods for identifying effective disease treatments and for
identifying promising therapeutic opportunities for emerging diseases. And along the way, I will also highlight some of new- uh,
technical work that was motivated by- by- by these specific biomedical problems. So let's start with the first topic, which is that of,
ah, [NOISE] modern drug safety and modeling drug combinations. So this work is really motivated by the problem that is known as polytherapy,
sometimes also called polypharmacy. It refers to the situation where patients take multiple drugs at the same time to treat complex or coexisting disease.
This is a problem that is very common, actually. 46% of people over 65 years take
more than five drugs that are prescribed to them by the do- their doctor at the same time. It's not uncommon to encounter patients that
take more than 20 drugs to treat heart diseases, de- depression, cancer, some other complex disease.
So the problem with that is that when a patient takes m- several drugs at the same time,
they're at much higher risk for developing adverse events. And there are estimates that
fi- around 50% of- 15% of the US population is affected by those unwanted side effects,
and that represents a huge burden both for individuals as well as for the entire health care system in terms of rising health care costs.
So when I said that individuals are at higher risk of developing adverse- unintended adverse events,
what did I actually mean by that? So when patients take m- multiple drugs at the same time,
because we now know that drugs don't act independently of each other, they might give rise to unexpected interactions between drugs.
So what does that mean? Let's look at this hypothetical scenario where we have a system with just two drugs.
We have a blue pill and a red pill. So when a patient takes blue pill alone, um,
there are no side effects observed and the desired therapeutic effect- e- effect is achieved and everything is- is- is well.
When a patient takes red pill alone, no side effects are observed and everything is okay. However, when a patient takes blue and red pill together,
then with certain probability, the patient would experience side effects, um, uh, when taking that combination of drug.
So that is surprising and unexpected because if the two drugs would act completely independently of each other,
then you would not expect to see any significant side effects when the two drugs are taken together.
So our learning task for- for- for this part of the talk, we will- we- uh, we- is the following.
We want to design a model that will be able to predict how likely a particular combination of drugs lead to a particular side effect.
So that is the- uh, the- the- the test that you want to solve. Okay, so na- natural question to ask is,
now that we know what is our question, is modeling drug combinations and modern drug interactions really a challenging problem?
As I answer to that is yes, it's a very challenging problem. Why is it so challenging? First, and that's perhaps the core challenge,
is that we can see a combinatorial explosion of possible combinations of drugs. Even if you take only a set of drugs that are approved in
the US and we consider only combinations that consists of two drugs, there are over 30 million possible combinations of two drugs.
There are over 20 billion possible combinations of three drugs and many more higher other drug interactions.
So the implication of that is- that is- is certainly infeasible in po- in- to test,
er, safety of those combinations of drugs in the lab, which effectively means that these drugs that are approved by- uh,
by FDA and other regulatory agencies, they get to the market, and then patients take them,
and then we see what happens. All right. So this is a huge opportunity for computational models to, ah, prioritize,
and a priori identify, and flag those combinations of drugs that might be unsafe before they are really used in real patients.
The second problem is that, ah, dr- er, un- unexpected drug interactions are by definition, non-linear, non-additive effect.
Right? An interaction is defined as an effect that is different from what wou- is an additive effect of individual drugs,
which presents its own set of challenges. And finally, you might think of this problem as a kind of big data problem because we'll be working
with the set of all drugs in the US and all adverse events associated with those drugs,
and there will be millions of those adverse event reports. But very quickly, we get down to the small data problem.
Where actually for any particular combination of drug there is just a small subset of patients that take that combination of drugs and,
um, there might be actually no information available about those combinations that are not yet used in patients.
And so that's another challenge. So to tackle this particular challenge,
what I will mention is an approach that is called decagon, that uses- that use graph neural networks to predict safety of combinations of drugs.
So decagon was defined on a polypharmacy knowledge graph that had the following structure.
It's a graph with, ah, two kinds of entities, drugs and proteins. So in this particular, uh, case, um,
drugs are represented by green triangles, an edge between two drugs has a particular type.
So an edge of type r_1 between two drugs would indicate that a specific type of interaction,
a specific adverse event, r_1 is observed when- uh, in patients that take the two, um, um,
drugs, ah, together and their side effects cannot be clearly attributed to either drug alone in that part.
Of course, in addition to knowing- um, um, include information about existing interactions between drugs,
we want to understand how those drugs actually work in terms of ha- what part of the body they affect,
and how the- they change activity of human cells. And so for that, the second mode information in
the knowledge graph connects drugs to their protein targets. And that, uh, proteins are represented by, ah, orange circles.
And then we know that proteins, ah, interact with each other, which gives rise to this underlying protein-protein interaction network.
So given this, ah, um, multimodal, ah,
polypharmacy network, the project I would describe it is called Decagon, ah, has, er, two components.
And so in the first component, in the encoder phase, it will take the multimodal network, it will learn an embedding, ah,
for every drug, and every protein out in that, um, graph, and then it will average those embeddings to predict labeled edges between
drug nodes that would correspond to predict its drug interactions or adverse events of combinations of drugs.
So let's just, uh, briefly take a look at, ah, that. Um, so the key idea for generating embeddings of
drug and protein nodes in- in that- that graph was to, um, explore the local and look at the local network neighborhoods for
every node in the graph that are and separates them by distinct edge types. So that is- is achieved to two phases,
where in the first phase for every node, say node v, um, we would de- determine a node's computation graph and
there would be a separate computation graph for each edge type that- uh, that- that exists in the local neighborhood of that node v. And in the second phase, ah,
the idea is to take those computation graphs and learn what is the best way to propagate neural messages along the edges of- um,
of computation graphs and trans- and- and then non-linearly transform them. And so in this particular case,
we see that for node v, node v has two- uh, two first-order neighbors that are of type r_3,
and three second-order neighbors that are of type r_3. So that would be- that would then define
a particular computation graph for node v for an edge type r_3 that would, ah, specify how information is propagated along the edges of that edge type.
So this gives rise to multirelational graph encoder. And in 2018, when the model was developed,
the- the architecture was relatively simple. And- and I'm sure you're very familiar with these kind of architectures by this far in the course.
So I'll very briefly describe it. Um, the idea here is that in- er, that in Decagon,
node embeddings were initialized by, uh, node features, um, which in the case of,
um, polypharmacy problem, were um, features representing additional information about
drugs and proteins such as drug structure, and protein memberships in pathways, and so on.
So embedding for node v and then at layer, um, 0 was just initial, um, node feature of that node.
And then at each layer, um, of- uh, of the GNN, the- the embedding for node v was- was
updated based on this particular equation which what it did. It look at all possible edge types that- um, uh,
that node v was involved in, and for each na- uh, neighbor of node v in that particular edge type,
for each neighbor u, um, the- the method took the embedding of that node u from the previous layer k minus 1,
it combined that embedding, um, by- er, by, uh, trainable, ah, weight matrix that was
specific for the edge type that you're currently looking at. And so to that aggregated result was that
co- was then combined with the embedding for node v from the previous layer and non-linearly
transformed to get an updated embedding for node v at layer k. Um, the weights for combining the, uh,
the- the embedding, the- each- each, um, each nodes embedding from the previous layer as well as the embeddings of the, uh,
neighbors at that point was defined by normalization constant, which was fixed based on the node degrees.
Today, what- certainly, we would want to use our attention mechanisms. But this particular equation was then applied,
um, some large K times where- where large K, would define the number of layers and the final embedding of the, uh,
the node is the embedding after K layers of the neighborhood propagation, uh, aggregation schema.
So that is fairly standard multi-relational graph encoder by now. So the- the result of multi- uh, uh, uh,
of the graph encoder were embeddings for drug and protein nodes. And so once em- em- embeddings for those nodes were obtained, um,
then the edges were- labeled edges between graph nodes were predicted. And for that the input was, uh, was,
em- consists of embeddings of two nodes, C and S. Those would be two particular drugs.
And so embeddings of those two nodes, C and S, were combined by, um, um,
a particular type of- of tensor factorization, um, that, um, was a- a style of the- tensor factorization that took embeddings for,
um, a pair of drug nodes, um, and a combined- and multiplied them by,
uh, two diagonal matrices that we're relation specific, and essentially, it encoded in a contribution of specific embedding dimensions towards a particular,
um, um, side effect. And then, um, a matrix R, which was, uh, a- a small, uh,
um, non- a fully non-zero matrix, um, that, um, allow the model to capture any kind of dependencies
between any pair of dimensions in the learned embedding. And so the output was then of- of these decoder were- were
then predicted edges of their- for every pair- for every particular side effect,
um, the model would return the probability that- that- that a given pair of drugs would interact through that particular side effect.
Okay. So in order to then apply this model to the polypharmacy knowledge graph,
then the first thing to do is to construct the actual polypharmacy knowledge graph. So the data that was used to do so was captured
molecular drug and patient data across all drugs prescribed to the US. And for that, a unique dataset was built that consisted of,
um, over 4 million drug-drug edges. Those are currently known interactions between pairs and drugs that was,
uh, that were retrieved from the FDA. Information on drug-protein edges and information protein-protein edges,
as well as site features on nodes. So that gave rise exactly to this multimodal knowledge graph that we have seen earlier,
which had over, um, around 5 million edges all together, and it was highly multi-relational as there
were 1,000 different edge types in that, um, graph. One is the model where the dataset was constructed,
um, and, um, Decagon was fully specified. Then, um, the Decagon, uh, uh,
model was trained on the polypharmacy, um, knowledge graph in an effort to answer
questions of the following form so the users could then pose, uh, a query, uh, for example,
whether a form of statin and ciprofloxacin where- which are two drug, when they come together would they lead to breakdown of muscle tissue?
That effect should effectively meant that in the backend, that the model was asked with predicting the probability that, uh,
these two drugs, um, are connected in the graph by an edge of type r_2,
if r_2 would indicate breakdown of muscle tissue. So, um, once the model was trained,
it was compared against, uh, several baselines, uh, baseline algorithms at that point in time.
So these see- the results are shown in the slide, where in the- on the y-axis we see performance,
high performer- higher values indicate better performance as me-measured by AUROC and average precision at top 50.
And, uh, the performance of Decagon model is shown who read performance of- of- other baselines,
um, is, uh, is shown in, um, uh, brown, blue, and green colors. So the particular baselines that we'll consider here was, uh,
a simple RESCAL tensor factorization, which can be thought of as Decagon, that does not use the deep graph encoder.
And so that test- that test has shown that it's really important to use graph convolutions to learn powerful embeddings.
So then Decagon was compared to a multi-relational factorization, which is as, uh, which, um, um,
any- the- the relative improvement of Decagon over that factorization showed that it's- it's really
important to be able to model side effects jointly because there are some correlations between side effects.
And- and finally, then, uh, Decagon was compared to a shallow network embedding model, um, embedding model which was a node2vec style model.
Um, and the improvement there showed that, um, the- the- the new- the message passing network is- is
much better in- in terms of performance in this particular application, uh, than, uh, some of the random walk,
uh, based, uh, embeddings at the time. So another test that was done was to,
um, do, um, apply Decagon in a temporal fashion. Meaning, the Decagon was trained on all the data generated prior to 2012.
And then, uh, the model was frozen and it was asked to make predictions,
um, that the model is most confident about. And we then test that, well, how many of those prediction have been confirmed after 2012?
So what is shown in this particular slide is our top 10 predictions made by the model, and so we- we recorded.
Each prediction comes in the form of a pair of drugs and the side effects associated with that pair of drugs.
And so for five out of those top ten predictions, direct evidence was found, published after 2012, um,
showing that- it- showing examples of real patients who were on a particular combination of drugs and have experienced
the kind of side effect the data has actually predicted would happen. So for example, for drug, uh, prediction number 8,
the- there- there are several reports of- of patients who took, a statin and amlodipine together and experienced muscle inflammation.
And those reports were made after 2012 and- which is- which provides evidence for- for- for prediction made by the model.
Some of the novel predictions in this space are novel hypotheses that can- that warrant further analysis and- and
downstream investigations in several collaborations with domain experts who are in- working
these disease and drug areas that has actually been done. Um, and- and, uh, a re- more recent follow-up, um,
on the Decagon study is concerned with modeling adverse events at the level of patient groups in an effort to
make predictions about what are adverse events that might be associated with certain group of individuals.
There were- those groups can be defined by age, by gender, or by individuals that have certain kinds of certain diseases.
And so, um, what- what- I'm mentioning this because, um, the data is already, um,
preprocessed and provided to the study, um, and- and so those who are interested in potentially leveraging
GNNs for doing more personalized level adverse event prediction, which is certainly something that can- that is possible now to do with
this dataset that contains over 10 million ad- adverse event reports. Okay. So [NOISE] this concludes my- my first, um,
vignette or first pa- topic- a large topic for today. And the next topic, we'll- we'll change the gears,
and we will move away from predicting safety of drugs. Instead, now we will, um,
we will think about diagnosing patients, which is important crucial question that, uh,
first need to be answered before any drugs can be actually- or treatments can be actually identified and recommended to the patient.
So let me start with the motivation of disease diagnosis. Uh, so, um, the core information for disease,
uh, uh, for diagnosing the disease, broadly speaking, is the information about phenotypes that are seen and observed in a particular patient.
So phenotypes can be defined as some observable characteristics that,
uh, that um, can be seen, er, uh, in- in a patient, and they result from various interactions between
the genotype and the genomic information within the patient as well as, um, environment in which the patient lives. So physicians typically utilize, uh,
various standardized vocabularies of phenotypes to describe human diseases. For example, a very prominent standardized voca- vocabulary phenotypes
is known hu- as human phenotype, uh, um, ontology. And, um, that codifies there- uh,
a variety of different phenotypes and how they map to human diseases. So by mo- by- by modeling diseases then as collections
of phenotypes or- or that represent the disease, we can then diagnose patients based on what are the phenotypes that they have,
based on what are the symptoms that those patients would have. So in machine learning, we can define the diagnosis task then in the following manner.
Uh, we will start with a graph. Consi- consider for- in this particular example, a graph that is built from the standardized vocabulary phenotypes.
That would be a graph where nodes are phenotypes that- those might be visible symptoms as well as various phenotypes of the molecular or genomic level.
And edges between those phenotypic nodes would indicate different kinds of relationships between phenotypes.
We can then- in- in- in that setting, the disease would then be represented as a set of nodes or
a subgraph in that, uh, phenotypic network. So the learning task would then be the diagnosis test,
is then concerned with- with predicting the, uh, the label or predicting the disease that is most consistent with,
uh, a particular subgraph S that describes the patient. So you can think of now that patients are
these different subgraphs of phenotypes the patients have, and the goal is to predict labels,
which in this particular case are colors of the subgraphs, and those labels are the diseases that, uh,
the- that- that are, uh, candidate diseases patients might have. Okay. So, if- given this formulation, uh, let's, uh,
let's discuss the algorithm for this problem a bit and then we will return back to the data in the diagnosis task.
So in the context of graph neural network, our problem formulation is the following. The goal here will be actually to learn embeddings for subgraphs such that,
uh, uh, those embeddings for, uh, the- the- for subgraphs that are learned, we capture well, uh, the topology of
interactions between nodes as well as where they are, where they exist in the context of,
um, the underlying graph. So that means that, um, that the input, um,
which then consists of the- some underlying graph G and a large number of subgraphs that have labels,
and the goal is to learn subgraph embedding such that two subgraphs is a pioneer Sub_j, that have similar subgraph topology,
and I will define next what that means. They should be embedded close together in the embedding space,
so representing by nearby points in the embedding space that you want to learn.
So why are subgraphs challenging? So why can we, for example, just take our favorite node level embedder,
and by now, I'm sure you have heard of many node level embedders in the course, and essentially, just learned node embeddings and- and
taken every- of those node embeddings or somehow, um, aggregate those node embeddings to arrive at a subgraph embedding.
So there are several reasons why we- we might lose a lot of information if you do so.
So one of them is that, um, the subgraphs that we would encounter in the diagnosis task, um, are subgraphs of varying sizes.
And, um, then the question is really, how to represent those subgraphs effectively that are not simple k-hop neighborhoods around a sing- a- a single center node.
Second, subgraphs could- could- would have reached connectivity structures both internally as well as externally to interactions with the rest of the graph.
So how can you inject that information into our GNNs? And finally, subgraphs that are given by,
uh, sets of, uh, phenotypes for patients can be localized in the sense that they would reside in a single region,
in a single large neighborhood of the graph, such as this hypothetical example where the subgraph is
indicated by this red blob that is core to the center of core of the graph, or subgraphs may be distributed across many multiple local neighborhoods.
And so, um, then, uh, a simple averaging of node embeddings might not be an effective strategy.
So to tackle this problem, uh, what was developed was a subgraph neural network model.
And so the problem that subgraph neural network is solving is the following. It assumes that given are some set of subgraphs S,
the set has an, uh, n subgraphs, S_1, S_2 up to S_n. SubGNN specifies then a neural message passing architecture that will generate,
um, D then- Dsub as dimensional subgraph representation for every subgraph in that set.
And would- they'll use those representations to learn a subgraph level classifier. So a function that will then map those subgraphs to some number of discrete labels.
And the way it will do so is by really taking into account different kinds of subgraph topology, in particular,
the- the parts of subgraph topology to- to- you're interested in, is related to capturing the connectivity within the immediate local neighborhood of, uh,
each subgraph, both, uh, the internal in- in- with- within a subgraph as well as at the border of the subgraph in the, uh, um,
the graph G. The information about the structure of the subgraph in the sense of other certain motifs that are over or underrepresented in a subgraph,
as well as where is that subgraph located relative to the graph G in- in- in which,
uh, it, uh, it exists. So a- as a quick note on problem formulation that I want to- to give
here is that SubGNN puts forward a definition of subgraph prediction learning task.
Um, this is different from other canonical tasks on graphs that are primarily concerned with a node level predictions
and representations where the goal is to predict properties, um, from- of an individual node in the graph,
or link prediction which is concerned with predicting properties of a pair of nodes, or graph level prediction,
which is concerned with predicting properties of entire graphs. So SubGNN is concerned with predicting properties of subgraphs of non-trivial size,
meaning, not subgraphs of size just 2, which would correspond on edge. The way this is done is that SubGNN consists of two parts.
So the first important part is, uh, message passing scheme that is hierarchical in nature,
and it specifies the way neural messages are propagated from anchor patches, which are helper graphs sampled from G to subgraphs S that, uh,
we want to embed, and how those messages are then embedded- um, aggregated to arrive at the final subgraph embedding.
And in part 3 the- uh, in part 2, that routing of messages is really down to three distinct channels where
each channel corresponds to a part- distinct aspect of subgraph topology, position, neighborhood, and structure if you want to capture.
Uh, very blief- briefly, let me describe these two parts. So the subgraph message passing, uh,
scheme is- has the following structure. Um, there are, um, um,
neural messages s- specified to those messages are specific for each of the three properties.
So they're denoted in sub x, and those messages are propagated from anchor patches to subgraph components.
Anchor patches are just helper subgraphs that are sampled randomly from the underlying graph G,
and there are three different kinds of patches depending of the- of the- to capture for, uh, to capture position, neighborhood, and structure.
So, um, then the- the- the message passing
is then formulated as the message from this anchor patch A to subgraphs S,
um, is, uh, is sent and it's scored based on its weight. Its contribution is weighted by similarity function between a subgraph component in
an anchor patch that- that- that represents a weight of how much an embedding of the anchor patch will,
uh, contribute to embedding of the subgraph. So this is visually shown here in the right part of the slide where the-
the subgraph that- that is part of the data that you want to embed is S_1. This subgraph actually has two disconnected components.
It- one is here, um, um, on the left part and the right part of the, uh, the-
the right, uh, uh, extreme of the graph, and in the message passing phase, uh, those helper patches- um,
anchor patches which are in gray, send their messages to the subgraph component. Those set of messages that are received at the subgraph component are aggregated,
and then the- the aggregated message of the subgraph component is combined with its previous components- uh,
with its previous embedding to get a property specific representation of, uh, subgraph component.
So H_x c is then an embedding for, um, subgraph component c,
so it could be just for this fir- one component of the subgraph S_1, ah, that captures the aspect x,
which can be position, neighborhood, or structure. So the way this message passing is done,
is done in three separate channels. And each channel, it describes, um, a particular- uh,
it's designed to capture one aspect of topology, position, neighborhood, and structure. Each channel x has three key co- elements.
Those elements are similarity function that weighs, um, messages exchange between anchor patches and subgraph components.
Then a sa- uh, an anchor patch sampling function that specifies how these helper patches need to
be sampled from the underlying graph in order to capture, um, neighborhood or see- and- and capture
diverse neighborhoods and diverse structure and diverse positions. And finally, how to encode those anchor patches,
and these functions can be learned or predefined. And so the recap of that,
is that we can think of this problem as, um, uh, as the- as the problem of learning subgraph representations as, um, uh,
as- as a message passing network that has three distinct channels, and those channels output, um,
channel specific embeddings that are then aggregated to produce
a final subgraph representation Z_s for subgraph S. So let's now return to our problem of, uh, disease diagnosis.
And the first thing we will do, we will, uh, ask- we will- we will test whether that method works on
some simulated synthetic datasets to really see whether the method can truly captured the distinct aspects of subgraph topology.
And the set up for that was the following. In each case, each dataset started with the base graph, um,
and base graph was then, um, sprinkled, um, with a large number of subgraphs.
There were two strategies to define subgraphs, either to- one strategy was- uh, was the planting strategy where a subgraph was generated
and it was planted on top of the base graph, or it would be essential- essentially just joined in the form of- uh,
in similar way as it would be stable together with the base graph. In this particular subgraph- er,
in this particular synthetical datasets, each sub- for each subgraph, a label was defined.
And so labels were defined based on certain network structural properties of those subgraphs.
So one particular synthetic dataset was called density, where- that was a dataset where, um,
that consists of the underlying base graph with the- and then a large number of subgraphs,
the labels of those subgraphs were binned values of density network, density metric.
And similar datasets were constructed for- that, uh, based on- on cut ratio metric, coreness, and component.
Why would you want to do that? The idea was to do that to test whether
really the most informative channels for certain types of labels are those channels that really truly determine the label.
So this is really a controlled environment of testing performance of the model. And so- thus results show that SubGNN is an effective strategy for
learning subgraph embeddings that outperforms various simple adhoc measures that one could think of.
For example, um, taking a simple average of node embeddings at- and- and obtain subgrapgh embeddings in this way,
or introduce a meta artificial note and then embed those meta nodes that could serve as proxies for subgraph embeddings or even doing graph level embeddings.
Finally, this- these results also showed that the SubGNN can capture well different aspects of subgraph topology.
That was can be studied by looking what- which of the three channels; neighborhood,
position, or structure, would be most informative for predicting certain kinds of labels.
So for the case of density, um, the label would primarily be determined by,
uh, the neighborhood of the subgraphs. So what was then tested is whether the neighborhood channel is most informative for predicting the density labels,
and indeed, that was the case. Okay. So now that, uh, we- we know that SubGNN works well,
let's return to our real-world dataset. So there were four real-world datasets that were designed for the problem of,
uh, um, clinical diagnostic tests. So each of the four datasets consists of a base graph,
and sub-graphs with associated labels. So the two, uh,
datasets that I want to bring your attention to are called HPO-METAB and HPO-NEURO.
So these are the datasets where the base graph is the graph of- um, from the human phenotype ontology.
So this is a graph where nodes are human phenotypes, and just are hierarchical relationships between
phenotypes and symptoms that we see in human populations. And then labels are defined by certain- uh, by, uh,
then subgraphs are defined by subgraphs of metabolic, um, uh, diseases and metabolic disorders that, uh, affect humans.
And in the HPO-NEURO dataset, then subgraphs represent, um,
uh, various neurodegenerative diseases. So the goal of those two- uh, uh, those- these two particular datasets is to, um, define- er,
is to predict subgraph labels that correspond to various metabolic or neurological diseases
that are consistent with the underlying phenotypes. So this simulates the environment of diagnosing
patients that might have metabolic or neurological disorders. So let's see performance of SubGNN,
uh, across those, um, four datasets with a particular focus on the,
uh, human phenotype ontology, neuro and metabolic dataset. And so what we can see is that the performance of SubGNN when- when,
uh, implemented with, uh, um, uh, when, uh- when compared to, um,
baseline motto- models is substantially better, and SubGNN outperforms though- those, uh,
baseline met- models considered here by up to 125%,
which is a- which is a really incredible improvement in performance, which clearly motivates the use of subgraphs rather than simply,
um, considering note level or graph level aggregation.
Okay, um, so this concludes the second part of, uh, the stock, which was concerned with,
um, using the power of subgraph embeddings and GNS for disease diagnosis tasks.
And in the first- the third part of the talk, I will, uh, focus on se- important endpoint for drug development,
which is that of drug efficacy. So in the first part we talked about drug safety but, uh,
in drugs and safety is one of two cornered endpoints that we care about in drug development process.
In addition to drug safety, the second important endpoint is efficacy, where the question is to understand,
whether a certain molecule that might be candidate drug wi- will really have positive therapeutic effect in patients.
So let's, uh, start with this third part of the talk. The motivating factor- the- the motivating,
uh, problem for this, uh, task was, uh, the question of how to find cures for emerging diseases.
This particular, uh, work was motivated by the onset of the COVID pandemic last year,
uh, where- um, it was- became clear that, uh, at the- them, uh, at the beginning of the pandemic, uh, there, um,
the need was for rapid de- deployment and identification of drugs that might have positive therapeutic effect in, uh,
in COVID-19 patients but simply traditional approaches of iterative development,
experimental testing, and clinical validation, and approval of new drugs were not feasible because they- they, on average,
take over a decade to bring and des- design a new drug from scratch and bring it to market and that was simply not feasible.
So a more realistic strategy relied on repurposing of drugs, requiring has to essentially identify
what other drugs that are clinically approved that might have therapeutic effect in COVID-19 patients
and so the particular study that I will be mentioning is, uh, is the- is the one that was
conducted bet- that it started at the beginning of March last year, and the most parts of it were done between March and, uh,
last summer and it really demonstrates, uh, an opportunity of how, in this particular case,
graph neural networks were able to compress years of work into actually weeks,
in some cases actually days of work by- by- through very close collaboration between machine learning models,
predictions that are provided by those models and end-users, which in this case, were virologists and experimental biologists who took
predictions and experimentally screened and intellect. Okay, so that's- this is our goal for- for this part of the talk.
Let's- let's now try to, uh, uh, unpack and define the problem of drug repurposing first and then describe our technical approach for- for- for this problem.
So as I mentioned, the discovery of a new development of new drug from scratch is a very lengthy resource intensive process.
It, on average, takes more than a decade and it costs 1-2 billion dollars to design a new drug from scratch.
And so, faced with the skyrocketing costs for developing new drugs, researchers are looking at ways to repurpose older ones,
to essentially take drugs that are already on the market and ask, are there any other kind of diseases that those drugs might be effective in?
And that is very appealing strategy because it would be much shorter and potentially much more effective.
Most famous example of a drug that was repurpose on the market is- is- is
Viagra that was initially designed for treating cardiovascular diseases but then during clinical trials,
it turned out that there might be other indications that it would be useful for. And so while in the past, those repurposing, uh,
strategies were primarily coincidental findings, there is now a concerted effort of,
"Can we systematically go through drugs that are, uh, clinically approved in the market and identify possible therapeutic opportunities?"
So the core of this question or the core of this problem is the following question, which we can think of it as a link prediction problem in- in a bar touch graph,
where on the left we have a set of drugs. Those might be all approved drugs plus investigational experimental drugs, um,
some drugs that fails in clinical trials, but otherwise are not hugely, uh,
unsafe for patient, and on the right, we would have tens of thousands of diseases which are codified diseases,
uh, that could be described, for example, two set of phenotypes that we were discussing in the second part of the talk.
So edges connecting drugs to diseases here would indicate the known treatment relationships.
The problem that we have here- our goal is to have a model that would predict what diseases,
uh, a- a new molecule or an existing drug might treat. So is this a challenging problem?
Why is finding treatments for a new disease in particular, such a challenging problem.
So when we're conc- we're- when we're concerned with the question of ho- finding treatment for a new emerging disease,
that means that we want to identify drugs that may treat the disease,
but there are no currently known drugs for that disease. So, and COVID was a very, very prominent example of that na- i- the- uh,
la- early, uh, last, uh, year. And so conceptual- technically that means- that- that's not surprising because we know in machine learning,
that generalizing to new phenomena is typically hard. Prevailing graph neural network methods still assume and require
abundant label information in order to be really trained effectively and achieve high accuracy.
However, I would argue that one of the definitive factors of frontier of biology and medicine are
problems were labeled examples are incredibly scarce and this includes emergent diseases such as COVID-19,
rare diseases, hard to diagnose patients, modeling novel drugs in development and many, many other problems.
So what prevailing methods assume is that we start with drugs that are already richly labeled.
That assumption is heavily violated in most exciting problems where in reality what happens in the world that we only have a handful of labels,
uh, for a particular class, if any at all. So too- for- for the- for the problem of drug repurposing, that, uh,
in the- in the meta da- and the gene and methodology that I will describe, it really relies on few-short learning and meta-learning.
So I have two slides now with just a very brief high-level overview of that, uh, of what metal learning is and I assume that many
have already heard about this in o- in other courses. So, uh, meta-learning is
very effective strategy for- for learning in the cases where there are a very few small number of
labeled examples available for- for- for tasks and so typically, the idea of, uh,
meta-learning strategies is the following: we assume that we have a- meta- we train a meta-learning model over a variety of
learning tasks and that model is optimized for- to achieve the best performance on a distribution of tasks,
including potentially unseen tasks. The way to think of meta-learning is that think of
each learning task being defined and associated with the small dataset, D, that contains both data instances,
feature vectors, and some true labels. And so meta-learning is then, uh, concerned with ide- um,
finding the optimal parameters Theta star for the model, uh, that would minimize the loss function across those,
uh, large number of datasets, each describing a different task. So if those models can define success with those parameters,
Theta star can define successfully, that indicates that the model was able to really successfully generalize from one task to the other,
from one dataset to the other, and so if can- if it can do that effectively to the process known as Meta training,
then we can hope that also in the case of at task time, it will be able to very quickly,
very effectively adapt to a new task, in our case, that will be a new disease, COVID-19.
So this looks very similar to just normal, regular machine learning task, but you can think this actually one data sample
is not a single note, it's not a single, uh, uh, edge in the graph but essentially, a data sample is- is small dataset,
which in our case would be a small part of the graph on which the model is straight.
So, as- uh, uh, uh, a second slide of the background of meta-learning is- is about few-shot learning,
which is just a specific instantiation of meta-learning in the field of supervised learning and I mentioned in there just because I want to introduce a no-
s- a one notation and that notation is, uh, uh, describes the difficulty of a few- a few-shot learning tasks,
where we will say that we are interested in solving a k-shot n-class-
classification problem the- which meaning that we want to solve a task for that,
uh, in a problem for which we only have k labeled examples for each of n classes.
So what is showing here in this slide is an example of a two-shot three-way image classification where we would have- uh,
where each task would consist of classifying im- class- uh, images into three classes.
That's why it's two-way image classification and for each class, we have two examples. And so the process here would be the during training thought- phase we
def- we sample and define a large number of training tasks, each of them is an instance of two-shot three-way image classification and
so if the model can be successfully primed to- to solve these, uh, to, uh, or to- to solve this task during meta-training phase then in,
during Meta testing time, then, uh, the model would- would quickly adapt to
completely new set of classes for which only a handful of labels are given as well.
The goal that we will have here now that we have this background on few-shot or making meta-learning is how to- how to make predictions,
uh, on a new graph or a new label set. So essentially, we want to use few-shot learning,
not on- for image classification- we want- but we want to use it- use it for graphs. Okay?
Um, so the problem formulation, um, is- of this problem is the following, uh, and,
um, we are interested in designing a meta-learner. Uh, that meta-learner will be concerned with, um,
will need to be able to classify an unseen label set, so that could be lab- a blue label set,
by observing some other label sets in the same graph. So during, um, training, uh,
the model will be lear- only on yellow labels and we would expect that the model would be- would be able to generalize to blu- blue labels that are defined within the same graph,
um, during, uh, test time. And so the meta that does that is called,
um, is- is called the G-Meta. The overview of G-Meta, if I present it in similar way as I have done for few- few-shot learning, uh,
just a few slides ago, it's very similar to the general few-shot learning setting, but now we are not concerned with, um,
solving image classification task but we are concerned with solving graph, um, um, um, learning problems.
Um, in- in- for the case of this, uh, explanation those would be node classification problems. But during training time,
we would define a large number of- of tasks, each, um, from a different- from a particular label set and at task time,
the new task will be presented on label set 3 that would not be seen, that was not present at training time.
So the key idea that G-Meta uses to solve these, uh, to solve the few-shot learning problem on- on graphs is that it was specified,
uh, um, you know, a signature function for each learning task.
That signature function will be, uh, defined based on the structure of- of,
um, subgraphs that encompass, uh, labeled examples in each task and then
those subgraphs signature functions will essentially lear- uh, that, uh, will learn how to map the structure of sampled subgraphs
representing tasks and how- and effectively, uh, serve, uh, sen- initialization for a GNN.
And so this treated, it can be really effectively- the strategy of extracting subgraphs that enclose
labeled example and then applying GNN to each subgraph individually is effective strategy for doing- for doing few-shot learning.
A natural question to ask now is just conceptually, which is an interesting question, what is the value of these local subgraphs that we are using
now to transfer knowledge from one task to the other? And the reasoning for why subgraphs, um,
um, are useful here is the following. You can think of, there are two fundamental core sources of information that the GNNs are using.
One is based that on label propagation, where we can think of nodes that have the same label,
they tend to be nearby in the graphs. We have seen at the beginning of the lecture today that proteins that tend to
interact the- they are also tend to lit- uh, be, uh, affiliated with similar phenotypes in similar diseases.
The other core source of GNN power is structural similarity, where nodes that have the same label,
they might not be nearby in the graph but they might have similar network shapes in their local neighborhoods.
Okay. So now that we know that, we know that in the case of few-shot learning where labels are really
scarce that we- that we cannot rely much on label propagation. It's simply not sufficient.
It's- it's where only a handful of nodes are labeled. It's very challenging to effectively propagate labels to the entire graph.
So instead, we can much better leverage structural information and structural equivalence,
which is exactly what these local subgraphs, uh, capture as a mechanism for transferring knowledge across tasks.
And it is actually possible to show theoretically that local subgraphs that are a fine- defined around the,
uh, labeled examples capture much of relevant information for prediction. And beyond theoretical findings,
it has been well, um, demonstrated by many studies, ah, uh,
in the context of biological networks that typically most useful information for,
uh, that to describes a particular task, um, in the context of biological network exist in some two,
three-hop neighborhoods around target nodes, so the use of local subgraphs is then highly motivated.
Okay. So let's return now finally, uh, to our COVID-19 repurposing problem.
And so I- I will start with description of the dataset on which this, with the- the- the- this transfer
of this- this few-shot learning strategy that I described was applied. So the repurposing dataset had,
had three major components. The first was information on how to even represent COVID-19.
Okay. How can we represent it in the form of a graph or some structure that- that is machine-readable that we can actually compute over?
And so to do that, we relied on the study that, uh, was published in, um, last year in, uh,
in Nature by Gordon et al and colleagues from UCSF and they- what they have done, they have, uh,
expressed 26 out of 29 proteins in SARS-CoV2 virus and they have checked
what are human proteins that those viral proteins take. And so in doing so, they identified
332 human proteins to reach the SARS-CoV2 virus binds.
And so that can be used as essentially computational- co- computable representation of COVID-19.
COVID-19 can be then represented by that set or subgraph or structure of
332 human proteins that are directly affect by the virus.
So these are known as viral-human protein-protein interactions because they are actually interaction between the virus and humans.
The second important information was that on, what are other proteins that those a- pro,
uh, proteins interact with? So that is the human interactome network or the human protein-protein interaction network.
And the third piece of information was up, um, the information about, um,
existing drugs on the market. So for each and every dru- existing drug we put information on its known targets,
uh, on proteins that those drugs bind to, and that gave, um,
rise to those drug modules, which would- would be the set of proteins for each drug that
are- that are targeted when the patient takes the drug. So COVID-19 ultimately was represented as a network neighborhood of
human PPI network targeted by SARS-CoV-2 virus. And the few-shot learning strategy was then applied to learn the embedding for COVID, uh,
COVID-19 that was the new task which is that we wanted to, uh, rank drugs for based on how promising they might be for COVID-19.
The way the model was optimized, it was optimized based on known drug disease indications for
other drugs and other diseases that are much better understood than COVID-19.
So that process yield an embedding space for approved drugs and diseases.
Uh, one point in that embedding space was the point that represented COVID- 19. So an effective way to identify oppor- repurposing opportunities for
COVID-19 was to look at what are other, um, what are drugs that are embedded close to the COVID-19, this em- embedding,
and then prioritize drugs based on that closeness between,
uh, COVID-19 embedding and the drugs embedding. And that- that gives rise to a ranked list of
drugs where those are the top represents most promising algorithm- uh, most promising opportunities for repurposing in the context of COVID-19.
The first test to do when you have such a model is to try to get a bit- uh,
some understanding into how accurate those predictions are. So what this plot is showing,
it is an AUC-ROC, uh, plot, um, that, um, shows on the x-axis false positive rate,
on the y-axis true positive rate of a prediction problem, which was defined as a question of,
can- can these models effectively identify those drugs that were at, uh, in April 2020 investigated on clinical trials for COVID-19.
And so a number of baseline methods were, uh, tested for this, uh, question. Uh, the three GNN-based method- the four GNN-based methods are called,
uh, A1, uh, are denoted A1 up to A4, and they showed substantial improvement in performance relative
to some other networks medicine-based, uh, strategies based on diffusions,
random walks, network proximity, and others that are more commonly deployed in the field of drug repurposing.
Then, finding it by itself, however, while promising, it's not particularly exciting because the fact that a drug is
investigated in clinical trials for COVID-19 does not really mean that it's successful. So a real test of the accuracy and
the ability of the predictions being useful is to experimentally validate predictions. So for that, biologists and
virologists from the National Emerging Infectious Disease Laboratory, so this is a national laboratory that can work with live viruses and it looks like this.
So it's looks like very kind of almost like some kind of environment in a space where nobody can enter except those with very particular access.
And they're able to work with live viruses there. So virologists there took the top predictions made by the model,
and then test- screen those predictions for their efficacy in first African monkey cells,
which are VeroE6 cells, a very prominent cell line for COVID-19 tests and later in mouse in human cells.
So the result of that was that, uh, 77 drugs that were predicted showed
strong or weak effect and they were active over a broad range of concentrations. Importantly, those predictions that were
obtained by GNN and actually an assemble of methods, including some of the other network medicine methods,
gave an order of magnitude higher hit rate among top predictions than prior work.
So prior work would be just brute force tests of all drugs in,
uh, in, uh, in a- in the lab. And if that is not done in a brute force manner,
but it's rather informed by prioritized list of drugs produced by ML models,
then an order of magnitude, uh, better hit rate is something that we could expect.
Since we are in the, um, in a course which is concerned about networks, there is an interesting finding that I want to mention here.
Um, so a natural question to ask is great. So this analysis identified 77 drugs.
Is there something common to those drugs that inhibited viral growth?
So the answer to that question is yes. So 76 out of 77 drugs that were predicted
and experimentally showed that they successfully reduced viral infection, were though kind of drugs that do not directly bind
to those human proteins that the virus directly attacks. Instead, those drugs rely on network-based action.
Those drugs, such as D3 here, would target some human proteins that are not directly attacked by the virus.
And those human proteins in turn would interact with some other proteins who in turn, finally, would be exactly those proteins that the virus attacks.
Why- why am I mentioning that? It's because these kinds of drugs,
which we now call network drugs are- could not be identified by traditional strategies for drug purposing,
which in computational biology and computational pharmacology are based on docking.
Okay, so since I'm already running over time, I will wrap up here.
So to summarize the lecture, um, today, uh, we talked about interesting applications of
graph neural networks in computational biology and we touched two top- three topics. We talked about using GNNs for modelings in
drug combinations and identifying adverse events and concerns regarding safety of drugs.
We then talked about opportunities for- that GNNs present for, uh, finding diagnostic tests and classifying diseases.
And finally, we talked about ways of- for using GNNs to identify, uh,
promising therapeutic opportunities with a particular angle, which was that of opportunities for never before seen emerging diseases.
So, uh, this concludes my last lecture here, I'm very happy to take questions. Thank you.
Uh, thank you very much Marinka, uh, this was awesome. Um, would you want to comment a bit more about what do you think are, uh,
big challenges, uh, in bio-medicine that are ready to be attacked by machine learning AI methods?
Wow, that's a very- uh, that's a very broad question.
So the way I would think of bio-medicine is, um, in the following way.
So we can broadly think about certain core problems. So one core problem is, um, related to diagnosing patients.
Before any kind of treatment can be identified or any kind of drug can be recommended,
we first need to know what are, uh, what is the disease that the patient has. So a core challenging problem that is- that is ready to be
tackled from the AML standpoint here is in particularly, um, uh, we need methods for rapid diagnosis of patients.
In particular, those patients that have, uh, that are hard to diagnose and have many rare novel diseases.
You might say, well, that might not be lots of patients, but it turns out that 20% of all patients in the US have these kinds of diseases.
On average, a patient or patients we treat these rare diseases need to visit seven specialists before they are correctly diagnosed.
That's again a huge problem for healthcare systems, for individuals, etc, etc. So now the data-sets are available to expedite the diagnosis tasks considerably.
So I think that's one exciting problem in direction to pursue. Sorry, just to ask a follow up.
So do you think of this as a active learning type problem where you also recommend what other tests,
uh, to take so that you can diagnose the disease?
Probably not. Because, uh, the problem is that if you think of- if you look at those patients,
they already went to many, many tests. So they more or less, they took lots of different tests and we
don't know what- what else to do. So it's not that much a question of identifying what lab tests should they conduct,
but the problem is that you- we can ask ourselves why we cannot diagnose patients easily.
The problem is that they are- they are conflicting in some way, meaning that a patient would come and would present- presents with
phenotypes in symptoms that don't match with any known disease classification, right?
So the way patient comes to the physician, the physician has a huge book then they- they- they- they- open and they say,
"Okay, this is a- these are the symptoms that I would expect for disease A." And they compare that with patients in terms, there's no match.
Okay, so no disease A, let's go to disease B, let's go to disease C. And the problem that it's hard to diagnose patients is that they
somehow have a mixture of symptoms that don't align well with any of the disease.
So it's- it's really more about essentially diagnosing them and the question is, what is a good diagnosis?
Probably, it's not just saying a patient has disease A, but it has these- these set of diseases which
or can be learned so kind of distribution over known diseases that capture the patient well.
Uh-huh. It's just the first task of identifying what's- what's wrong with- with the individual, right?
Immediate next question is, can we identify successful treatments? So here, all the questions related to drug discovery and
development apply in the sense of can we re-purpose existing drugs? Can we identify combinations of drugs that might work for a patient?
Can we do that in a more personalized way? And it's possible now to date, meaning that the data-sets are ready and available to move away from the one size fits
all treatment recommendation which is still more or less the kind of work that I've been describing in the lecture today.
It's essentially one size fits all treatment recommendation. But it's now possible to go to a more precise level,
at least at the stage of patient groups. Meaning what would work for certain genders, certain ages,
certain combinations of gender, age, or molecular markers. Um, I think then- there is another large group of
very important high impactful question in biomedicine to ask, which are related to, um, uh,
temporal aspects of how diseases progress and how to monitor the, uh, patients and help them improve their, uh, self-care.
So that- that's- those are important questions to- to tackle for which datasets are available as well.
And then close collaborations with health care systems would offer lots of opportunities for kind of, um, tedious work.
[LAUGHTER] Tedious work would be optimizing clinical, uh, workflows, identifying medical errors.
In, um, uh, then proposing systems that have,
at the end that use active learning to then identify, uh,
what- when to- um, uh, [NOISE] when to add another drug to patients,
remove a drug from the patient, when to call the patient to come to the- to the clinic.
Um, so those are all exciting questions and some new questions have emerged during the pandemic.
And type- an example of a question that is new is that of,
um, [NOISE] uh, remote medical visits. Um, that is particularly very exciting topic nowadays in the interface of,
uh, AI in medical space. In particular, the question is, can we design various AI systems that could help, um, uh,
doctors to perform visits remotely in the sense that,
um, one would have, uh, uh, uh, it would be possible to take an image with, uh,
the- the- of- um, a patient would take, um, an image with- with their app, and that image would be analyzed automatically, um,
on the patient side and then communicated to the physician. Or and- and here in particular,
active learning strategy is- is most important to ex- for example, identify what part of the data is most crucial to have in order to make diagnosis, right?
And then can that part of the data being acquired without the patient necessarily visiting with [NOISE] hospital?
And [OVERLAPPING] that would- that would enable, um, uh, remote diagno- diagnosis and management.
Uh, this is super cool. So, uh, I guess a follow-up, uh, question, uh, is, uh, especially for the first few,
uh, problems that you are mentioning. Why are graphs essentials? Like- like, uh, why are these,
uh, graph problems and, you know, not natural language problems or, uh, you know, computer vision problems?
Why do you think graphs are essential in this case? That's a great question. I tried to shed some light into this question at
the very beginning of the talk- of the very beginning of this lecture. And, um, so the- the answer to your question would
have- would have- would have be- would have two- two parts. The first part is that it's very easy to see that,
um, most of the problems in biology are co-dependent,
or in- in the sense of whatever task we pick up, typically,
there- there are substantial relationships between important entities in that task that need to be modeled and decades
of kind of fundamental biological research has shown that those dependencies are really important and cannot be ignored.
So the relational structure of the dataset is very universal. So we have seen that for essentially all three problems discussed in this talk,
that that's indeed the case and it has been shown over and over again experimentally from protein interaction network,
CRISPR gene editing, disease diagnosis tasks, and so on. So- so these are relations are an important component of the dataset.
So we would need to leverage models for for those tasks. Second is, not only that, um,
these relations exist, but they are, in most cases really informative, sometimes even more informative than- than structural data,
for example, for the problems of predicting, uh, proteins, uh, structure.
The recent models put out by DeepMind actually leverage relational, uh, information about proteins to infer what their structure is, uh, about.
And- and- and so not only the datasets, uh, and problems are- have strong relational structure,
but that relational structure correlates well with predictive tasks that we want to produce.
And so then some form of graph-based machine learning, um, seems very suitable.
Uh, thank you- thank you so much. Uh, one more, uh, question, um, is about, uh,
how effective has been to get the medical community to adapt to these methods?
Um, and, uh, uh, yeah, so that I guess is, uh, the question. So that's a great question and
the short answer to that is would be it's- it's challenging. [LAUGHTER] So first to ans- to answer that,
by now we have seen several examples of successful deployments of, um, this algor- um,
ML algorithms in general, in clinical settings as well as in large pharma companies. So that is primarily true,
so far, for image-based models. Um, there aren't many or I'm not aware of really successful graph-based machine model,
that is applied and used routinely in clinical contexts, or in, um, at least in public clinical settings.
What- where- where we are currently at this stage is that typically individual researchers engage with collaborators would
be physicians or clinical researchers and so on and they would work hand in hand on their particular problems and one could think of that as
validations- experimental validation of predictions but currently, we are not there yet where these methods would be routinely used,
where graph neural networks or graph ML would be routinely used in, um, in real-world.
Why is that the case? So I would argue that the main gap that is significant that exists between say,
models publishing Europe's papers, and then these models being deployed in real-world,
is this lag debt. Typically, on- when we apply these models to datasets,
they don't generate predictions that users would think of as actionable predictions. What that means is that,
it's not enough to simply predict whether a drug will treat a disease.
It's very hard to convince somebody that will follow up with very expensive downstream experiments in the lab
by this high level pointwise based prediction. So what- what I didn't discuss in the context of COVID repurposing project,
it was extraordinarily important to essentially provide explanations, allow users to provide feedback to the model,
and essentially build some trust and confidence in the model and so that has now given rise to this- some relative term of trustworthy machine learning,
which I think has- has some bearing in the context of, uh, um, biomedical applications.
Uh, thank you. Uh, then, uh, uh, two more quick questions. Uh, could you elaborate on the node feature engineering for drug- drug interaction,
uh, the drug combination in the first, uh, part of your talk? Um, and then we have one more.
Yes. So, uh, for node features in the first part of the talk, we had node features for drugs and for proteins.
Node features for drugs were, uh, SMILES fingerprints of, um, the drugs and, um,
they were- there were not manually engineered by- by us, but this is a very common descriptor of drug structure that is used in cheminformatics.
It was retrieved from a database called DrugBank. Uh, for protein nodes, the node features were
binary vectors that contained information about what molecular pathways those proteins are involved in.
For example, if we know that a protein A is- uh, participates in a cell apoptosis pathway,
this is a pathway that's incredibly important because it basically tells us- it- it has a control over when a cell should die or stay alive, right?
So if a protein is a member of that pathway, then there would be a- a simple indicator one at that,
uh, in particular- in the relevant dimension of the protein feature vector. All right, thank you. And then,
uh, uh, one, uh, I'll, uh, next question is about, uh, the G-Meta learning, how, uh, uh, somebody is interested,
how does it differ from Bayesian network learning and the EM algorithms? Could you discuss pros and cons?
Oh, I see. Interesting. So we actually didn't consider any Bayesian network learning method for the problem of- uh, of G-Meta.
We compared it primarily with, uh, current work on- um, that is graph neural network-based for few-shot learning.
I- I can clearly see from this question that it might be interesting to use, um, some Bayesian model and then learn a distribution of drug effectiveness across diseases,
and then use that to sample a new disease that would approximate COVID-19.
Um, the pros of that would be that using the Bayesian models would perhaps,
uh, give us a better ranking of drugs. Because ranking of drugs for that problem was determined on
predicted scores returned by GNNs, and we know- uh, and returned by G-Meta in particular.
Uh, from several applications, we know that often the scores that are returned by the model are not necessarily well calibrated or indicative of probabilities.
So I could imagine that the Bayesian network model might work better for that. So that would be the pros.
The cons of that, uh, would be that, um, what he found in the context of,
uh, COVID repurposing is that, uh, the G-Meta or graph neural network-based models when implemented with very recent,
um, neighborhood sampling strategies from like Cluster-GCN, or GraphSAGE for example, in other,
they can scale very well to very large datasets, um, including, for example, one dataset where we looked at, um,
information not only for humans, but from- from our 1,800 other [NOISE] species, and try to translate information from other species and model organisms like zebra,
fish, and monkeys to human. And I- um, at least in my experience, in- GNN models scale better to a larger dataset, than some Bayesian models.
Thank you so much, uh, [NOISE] Marinka for, uh, the excellent lecture and, uh, thank you everyone for asking great questions.
Uh, appreciate it a lot. Uh, thank you so much, and, uh, uh, all the best to Boston.
Great. Thank you for inviting me. Yeah, thank you. Bye bye. Bye. Bye.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 19.1 - Pre-Training Graph Neural Networks.txt
Thank you for the opportunity and I'm excited to talk about
my work on pre-training graph neural networks.
Okay, so, um, here I, uh,
I want to talk about applying
graph neural networks to application in scientific domains.
Uh, for example, uh, in chemistry,
we have this molecular graphs where each node is
an atom and each edge represents the chemical bond.
And here we are interested in predicting the properties of molecules.
For example, given this kind of molecular graph,
we want to predict uh,
whether it's toxic or not.
In biology, there's also a graph problem.
For example, uh, we have a
protein-protein association graph where each node is a protein and the edge represents
certain association between proteins and the associations are
very important to determine the functionality of the protein.
And our goal is to given this, uh,
subgraph centered around the protein node,
we want to predict whether
the center protein node has certain biological activity or not.
So what I want to say is that in bio- in many scientific domains, this, uh,
graph problem appears a lot and we want to apply GNNs to solve this problem.
And just to review how GNNs can be used for graph classification.
So GNNs obtain an embedding of the entire graph by following two steps.
First, um, given this,
the molecular graph, let's say graph, let's say molecule,
we obtain the embedding of node by either the aggregate neighboring information.
Once we do that,
we- once we obtain this node embedding,
we can pool this node embedding globally to obtain the embedding of the entire graph.
And once you obtain this embedding of the entire graph, we can use it,
or we can apply a linear function on top of it to predict whether this entire graph is,
let's say, toxic or not.
And just to, [NOISE] uh, say what this means is that, uh,
here the node embedding is trying to capture
the local neighborhood structure, uh, around each node.
If we apply k-hop GNNs,
we will just know that it should capture k-hop local neighborhood.
And then the embedding of the entire graph can, uh,
is basically aggregating this local structure though,
the node- node embedding that is capturing the local neighborhood structure.
And, uh, and, uh,
this is how GNN works,
and now I am going to talk about challenges of how- applying machine learning to,
uh, the scientific domains.
And there are basically two fundamental challenges.
The first challenge of applying machine learning to
the scientific domains is that all the scarcity of labeled data.
Obtaining labels in these scientific domains requires expensive lab experiments.
For example, to, or say whether a molecule is toxic or not,
you need to perform wet lab experiments,
and these are very expensive.
As a result, we cannot get that,
a lot of training data and machinery models tend to overfit to this small training data.
Another important issue is that out-of-distribution prediction, meaning that,
test examples we want to make a prediction on
tend to be very different from training examples.
And this is really the nature of scientific discovery because in scientific discovery,
we want- we want to discover some new molecule that is
inherently very different from- from your training molecules.
And in these domains,
machine learning models are extrapolate poorly.
And especially these two challenges are becoming more challenging for deep learning.
The first, uh, the first point of the label scarcity,
deep learning models have a lot of parameters to train,
uh, typically in the order of millions.
And uh- and in this regime,
the number of training data is much less than the number of parameters.
And deep-learning models are extremely prone to
overfitting on small data- small labeled data.
Another issue is that deep learning models are known to extrapolate poorly.
And that's reported that models often make, uh,
predictions based on spurious correlation in a dataset without
understanding the true causal mechanism of how to make prediction.
So let's consider this a toy example of
a- a image classification between polar bear and brown bear,
uh, this image is shown here.
So during training, let's say, uh,
our training data for- in our training data,
most polar bears have snow background like this,
and most brown bears have a grass background like this.
And as a result, the model can learn to predict,
make prediction of whether it's, uh,
the given image is polar bear or brown bear based on the image background rather than
animal itself because that's sufficient
to make predictions over the- over the training dataset.
But what if at the time same,
if we see polar bear on the grass,
then the model will, uh,
because the model is not understanding the- this prediction task,
the model will perform poorly on the- on
the data or test data that is different from training data.
And the model is, uh, just capturing this kind of
spurious, spurious correlation in the training dataset.
And our key idea or the goal, uh,
given these challenges is that we want to improve
model's out-of-distribution performance even with limited data.
And the way we do this is to inject domain knowledge
into a model before we apply them on scarcely-labeled tasks.
And that, this work- this may work because
the model already knows the domain knowledge before the model is
training on- on our downstream data so that the model can generalize
well without many task-specific labeled data and uh,
and uh, the model may be able to extract essential,
non-spurious, uh, pattern in the data, uh,
which allows the model to better extrapolate to the-
to the test data that still- that still very different from training data.
And a very effective solution to inject
domain knowledge into model is called pre-training.
And we pre-train a model on relevant tasks that's different
from a downstream task where data is abundant.
And after we pre-train the model,
the model's parameter, uh,
already contains some domain knowledge and once this is done,
we can transfer this pre-trained model parameter to the downstream task,
which is what we care about and which where we have small number of data.
And we can start from the pre-trained parameter and fine-tune the-
the parameters on the downstream tasks.
Um, and just to mention that pre-training has been hugely successful in- in the domain of
computer vision and natural language processing and it's
reported that pre-training improves label efficiency.
Also, pre-training improves out-of-distribution performance.
And because of this and within
pre-training can be a very powerful solution to the scientific applications.
And those two challenges are scarce labels and out-of-distribution prediction.
So now we motivated
this pre-training GNNs to solve an important problem in scientific applications.
So let's consider, you know, actually pre-trained GNNs.
And our work is about designing
GNN's pre-training strategies and we want to
systematically embedded- investigate the following two questions.
How effective is pre-training GNNs,
and what is the effective pre-training strategies.
So as a running example,
let's think about molecular property prediction task,
prediction for drug discovery.
For a given molecule, we want to predict its toxicity or biological activity.
And the very naive strategy is multi-task supervised pre-training on relevant labels.
This means that in the, for example,
chemical database we have, uh, all the,
uh, experimental measurements of
tox- various toxicity and biological activities of a lot of molecules.
And we can first pre-train GNNs to predict those,
uh, very diverse biological activity of toxicity.
And then we expect GNNs parameter to capture
some chemistry domain knowledge which can be tran-
and then we can transfer that parameter to our downstream task.
And the setting that we consider is, uh, to,
to study whether this naive strategy is effective,
we consider this setting.
We consider this binary classification of molecules.
Given molecule, we want to judge whether it's negative or positive.
And- and we, for the supervised pre-training part, uh,
we consider- we consider predicting
more than 1000 diverse binary bioassays annotated over 450,000 molecules.
So there are a lot of data,
uh, in this pre-training stage.
And then we apply transfer the parameter to or downstream task,
which is eight molecular classification datasets,
those are relatively small, uh,
about 1,000 to 100,000 molecules.
And- and for the data split,
uh, for the downstream task,
we consider the scaffold split,
which makes the test molecules out of distribution.
And it turns out that the Naive strategy of
this multi-task supervised pre-training on relevant labels doesn't work very well.
It gives a limited performance improvement on downstream tasks and even, uh,
leads to negative transfer,
which means that the pre-trained model performs worse than randomly initialized model.
So here's the, uh, table,
and here is a figure.
So we have our eight, uh,
downstream datasets and -and
the y-axis is the ROCAUC improvement over no pre-training baseline.
So this purple is a no pre-train baseline.
And we see that Naive strategy sometimes work well,
but for these two datasets,
it's actually performing worse than non pre-trained randomly initialized baseline.
So this is kind of not desirable,
we want pre-trained model to perform better.
So how- what is the- then we move on to our next question,
what is the effective pre-training strategy?
And our key idea is to pre-train both node and graph
embeddings so that GNNs can capture domain-specific knowledge about the graph,
uh, at the both, uh,
local and global level.
So- so in the naive strategy,
we pre-train GNNs, uh,
on the- at the level of graph, use this, uh,
graph embedding to make a various prediction.
But we think, uh,
it's important to also pre-training, uh, this, uh,
node embedding because these node embeddings are
aggregated to generate the embedding of the entire graph.
So we- we need to have a high-quality node embedding
that captures  the local neighborhood structure well.
So and the- the intuition behind this is that- so here's the figure,
the- the upper side means the node embedding
and these node embedding are proved to generate the embedding of the graph,
and we want the, uh,
the quality of the node embedding and graph embedding to be both high quality,
capture the semantics in the- in the- this is,
uh, different from the naive pre-training strategy where, for example,
if we- on the node-level pre-training,
we capture the semantic at the level of nodes,
but if those node embeddings they are globally aggregated,
we may not be able to obtain nice, uh,
graph embedding, um, or if we only pre-training,
uh, GNNs at the level of graph,
we can obtain nice graph embedding,
but the- there's no guarantee that the- before aggregation,
those node embeddings are of high-quality,
and this may lead to a negative transfer,
um, because the node embeddings are kind of not robust.
So- so in order to realize this, uh,
strategy to pre-train GNNs at the level of both nodes and graphs,
we- we designed three concrete methods.
Uh, two self-supervised method, uh,
meaning there is no need for external labels for node-level pre-training,
and one graph-level of prediction of pre-training strategy,
pre-training method, um, that's,
uh, that's, uh, supervised.
So let me go through these three concrete methods.
[NOISE] Okay.
So the first method is the attribute masking.
It's a node-level pre-training method.
Um, the algorithm is quite simple.
So given that input graph, we first mask,
uh, uh, rand- randomly mask our node attributes.
So let's say we mask these two node attributes.
In the molecular graphs, this node that we are
basically masking the identity of the atom.
Um, and then we can use GNNs to generate, uh,
node embeddings of these two mask nodes,
and we use these node embeddings to predict the identity of the mask attributes.
So here we want to use the node embedding to predict the- that
this X is the- is the oxygen and this X is the carbon.
And the intuition behind this is that
the- through solving the mask attribute prediction task,
our GNN is forced to learn
the domain knowledge because it kind of needs to solve this quiz, and, uh, and, uh,
and- and through this,
solving this kind of quiz, GNN, uh,
can learn the- the parameter of the GNN can capture the- the chemistry domain.
The next self-supervised task that we propose is called the context prediction.
And, uh, and here the idea- the algorithm is as follows.
So for each graph,
uh, we sample one center node.
Let's say this is a,
um, um, red nodes,
and we, for this, uh,
center node, we extract neighborhood and context graph.
So neighborhood graph is just the k-hop neighborhood graph, uh, in this case,
two-hop neighbor graph like this,
and then the context graph is the- is
the surrounding graph that's directly attached to this,
uh, k-hop neighborhood graph.
Once we do extract this graph,
we can use two separate GNNs to encode this neighborhood graph into, you know, uh,
one- use one GNN to encode this neighborhood graph into a vector,
another GNN is to encode this context graph into a vector.
And our objective is to maximize, uh,
or maximize the inner product between the true neighborhood context pair
while minimizing the inner product between the false or negative,
uh, uh, false on neighborhood context pair,
and these false pairs can be obtained by, uh,
randomly sample context graph from other neighborhoods.
And here the intuition is that we're using- we are assuming that
subgraphs that are surrounded by similar contexts are semantically similar.
So we are basically pushing, uh,
the embedding of the neighborhood to be
similar if they are- have similar context graphs.
And this is the kind of widely used, uh,
assumption in natural language processing,
it's called distributional hypothesis.
So words that are appearing with similar- similar contexts have similar meaning,
and it's exploited in the famous, uh, Word2vec model.
Um, so finally, um,
I'm going to talk about this, uh,
graph-level pre-training task, which is essentially what I introduced before.
So multi-task supervised pre-training on many relevant graph level- labels.
And this is really the direct approach, uh,
to inject, uh, domain knowledge into the model.
[NOISE] So to summarize the overall strategy,
we perform- we first perform node-level pre-training,
uh, to- to obtain good node representation.
We- we use that, uh,
pre-training parameter, uh, to- and then, uh,
to- to further perform pre-training, uh,
at the level of graph using supervised graph-level pre-training.
Once- once we've done,
uh, this, both steps,
we fine-tune, uh, the parameters on the downstream task that we care about.
So this is the overall strategy.
And it turns out that our strategy works out pretty well.
Uh, as you can see, these, uh,
green dots are our strategy,
and our strategy first avoids the negative transfer in these, uh, two datasets,
and also consistently perform better than- than- than this,
uh, orange naive strategy baseline across the datasets.
Uh, and also, um,
another interesting side note is that, uh,
we fix the pre-training strategy and the pre-training different GNNs models,
uh, use different GNNs models for pre-training.
And what we found out is that the most expressive GNN model,
namely GIN, that we've learned in the lecture,
um, um, benefits most from pre-training.
Uh, as you can see here,
the gain of pre-trained model versus non-pre-trained model is the- is the largest,
um, in terms of accuracy.
And- and the intuition here is that
the expressive GNN model can learn to
capture more domain knowledge than less expressive model,
especially learned from large amounts of,
uh, data during pre-training.
So to summarize of our GNNs, we've, uh,
said- learned that the GNNs have important application
in scientific domains like molecular property prediction or,
um, protein function prediction,
but, uh, those application domains present the challenges
of label scarcity and out-of-distribution prediction,
and we argue that pre-training is a promising,
uh, framework to tackle both of the challenges.
However, we found that naive pre-training strategy of this, uh,
supervised graph level pre-training gives
sub-optimal performance and even leads to negative transfer.
And our strategy is to, um,
effective strategy is to pre-train both node and graph embeddings,
and we found this strategy leads to
a significant performance gain on diverse downstream tasks.
Um, yeah, thank you for listening.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 19.2 - Hyperbolic Graph Embeddings.txt
Uh, in this lecture, I will introduce the use of hyperbolic graph embeddings in learning,
uh, representations for graphs that exhibit certain hierarchical and tree-like structure.
Um, so in previous lecture, uh, we have focused on graph representation learning in the Euclidean space,
uh, which is the R_ n. Um, however, Euclidean embeddings cannot always capture complex graph structure.
So here we have an example of a tree structure, uh, a, uh, complete binary tree,
where the first level we observe there, uh, there's one node, and the second level we have two nodes, and the third level we have four nodes, etc.
So in this kind of tree-like structure often, there, uh, there is an exponential increase in the number of nodes,
uh, as the depth of the tree increases. So these kind of trees are, uh,
difficult to be embedded in the Euclidean space, uh, as you have seen, uh, because the slide itself is a 2D plane.
And we can see that, uh, as the depth increases, uh, the nodes that are, uh,
leaves become closer and closer to each other to the point that they can no longer,
uh, the embedding space could no longer represent, uh, the distance between them, um, faithfully, uh,
because they are very, very clustered together due to the exponential increase in the number of nodes.
So here we are focusing on these kind of, uh, graphs with, uh, such tree-like structure.
Um, so we define these kind of tree-like graphs as, uh, graphs that are similar to a tree.
For example, the binary tree as shown above, but it can still contain a very few cycles.
Uh, for example, in real-life, uh, uh, situations, we can have things like knowledge graph that are organized hierarchically because,
uh, many of the human knowledge are organized hierarchically. And, uh, at the same time, there could still exist a certain kind of loops in- in- in the structure.
So it's most- mostly a tree but with a certain, uh, a number of loops in- in the graph itself.
So here, uh, we're considering this kind of structure and, uh, we want to notice that embedding geometry has been very important in terms of,
uh, capturing this kind of special graph structure. Um, here we consider some, uh, other, uh, interesting graph structure.
For example, if we have graphs with, uh, very, uh, many, very large cycles. Uh, uh, these kind of graphs are best embedded into spherical geometry.
Because in a spherical geometry with the positive curvature, the, um,
the graphs, uh, uh, we can actually embed many, many cycles because the sphere contains many, many cycles.
Um, whereas a- if you- if we have a grid-like graph, um, Euclidean space is the best.
Uh, because as you can see, the Cartesian coordinate itself is a grid-like structure.
So it could, uh, a Euclidean space with zero curvature would be the best. So how about hierarchical tree-like graphs?
Um, for these kind of graphs, we are considering the use of hyperbolic space, which is an alternative, uh,
embedding space to model, uh, embeddings for trees with tree-like structure. Um, the advantage of hyperbolic embedding, uh,
space, um, allows it to, uh, naturally model trees. And we will, um, elaborate on these advantages, uh,
in the later, uh, later slides where we explain the definition. However, there are certain challenges, uh,
when we combine deep learning, and hyperbolic spaces. The primary reason is that existing deep-learning toolboxes can only be applied,
uh, to, uh, uh Euclidean space. Uh, whereas in hyperbolic space, we need to define special operators to,
uh, to perform this kind of deep-learning, uh, operations. Um, uh, let's first define what is a hyperbolic space.
So hyperbolic geometry differs from Euclidean geo- geometry by its, um, uh, fifth axiom of, uh,
Euclidean, uh, geometry, which is the parallel postulate. So in Euclidean geometry, the parallel postulate says that in a given plane,
given a line and a point that is not on the line itself, at most one line parallel to the given line can be drawn through the point.
Um, and- and this- this seems very obvious in the Euclidean space. If we have, uh, if we- if we have a line then the line parallel to itself is, uh,
kind of unique, uh, if we only consider, uh, lines that pass through one point.
Um, however, in, uh, hyperbolic geometry, uh, there are infinites number of lines that are parallel to a given line through the point.
This might not seem very, uh, intuitive because we are very familiar with Euclidean space.
But here is a representation of a hyperbolic space in terms of Poincare ball. So we will explain what is a Poincare ball there.
Um, but it's basically a way to visualize the hyperbolic space in Euclidean space. And you can see the blue line here.
Um, it looks like a curve, uh, but it is actually a straight line, uh, uh, in- in the hyperbolic space.
And we can- we can also define a redpoint here, which is another point that is not on the line.
Um, and hyperbolic geometry states that, uh, there are infinite number of lines that are parallel to the line.
And these lines are actually drawn in the visualization. So all these are curved, uh, black lines that's, uh, we draw here.
They're actual- actually straight lines, uh, hyperbolic geometry, and they also pass through the redpoint.
And at the same time, they are also parallel to the blue curve, uh, sorry, it- to- to the blue straight line in the hyperbolic space.
Uh, and that's, uh, mainly how it differs from the Euclidean space. Uh, so because of these special properties,
hyperbolic space cannot be naturally represented in the Euclidean space because it violates the fifth,
uh, uh, the fifth axiom of Euclidean space, which is the parallel postulate. So, um, we currently use
two geometry models to represent or to visualize hyperbolic space, um, by embedding it as a subspace of the Euclidean space.
And, uh, they are also equivalent. So we can also map between these two models that visualize hyperbolic space and Euclidean space.
So the first model that we're going to introduce is the Poincare model. Uh, this is a model,
uh, where we represent, uh, uh, where you represent the hyperbolic space as a open, uh, an open ball.
So, um, a visualization of it is, uh, on the right where, uh, we can see a ball.
Um, and this, uh, by open, we mean that's- the set of all points in this hyperbolic space,
uh, do not ex, uh, do not include the boundary of the ball. So the- the outer circle, uh, is not part of the, uh, ball,
but everything inside the circle, uh, is part of the Poincare, uh, Poincare ball. And the radius is proportional to the square root of
K. And we will introduce what is K later. Uh, K is actually the inverse of the curvature.
Um, and- and, uh, the important property here is that each triangle in the figure has the same area.
So, uh, it might not, uh, again, it's not very intuitive in Euclidean space, uh,
as we see like all the triangles in the middle tend to have a larger, uh, seems to have a larger area in the Euclidean definition of, uh, uh, Euclidean metric.
Uh, but, um, uh, but every triangle here actually have the same area.
And, uh, you can see why this space is very amenable to embedded, uh, hierarchical or tree-like structure.
Because you notice that in the middle or in the center, uh, of the, uh, uh, Poincare ball, there are very few of these triangles that are, uh,
in- in hyperbolic space, whereas, uh, uh, towards the edge of the, uh, or of the circle, uh,
you can see like there are exponentially many of these, uh, smaller triangles, uh,
albeit with the same, uh, area, um, that are embedded, uh, towards the edge of
the hyperbolic space or the- to- towards the edge of the Poincare ball. So this exponential increase in- in terms of the area or the volume, um, uh,
as the, uh, the r- radius or the norm increases, allow us to embed, uh, hyperbolic trees.
So basically trees, um, can be embedded by embedding its, uh,
roots in the center, um, because there are very few of them. Um, and embedding all the leaves on the edge of the Poincare ball, um,
because there are exponentially many of these leaves, uh, uh, at the, uh, as the depth increases.
Um, and we're going to introduce another alternative model that, uh,
models the hyperbolic space, which is called the hyperboloid model, um, and also called Lorentz model.
Um, this is basically an upper sheet of a two sheet hyperboloid and that is visualized in the figure below.
And this is also a set of Euclidean space. So, um, unlike Poincare model as you can see,
there are exponentially many, um, triangles towards the edge of the, um, Poincare, uh, Poincare ball.
And, here hyperboloid model is numerically more stable, meaning that you don't have to have
extremely high machine precision to represent all these, uh, small triangles, uh,
in terms of their embeddings, uh, towards the edge of the Poincare ball. Uh, here, um, it is basically in the hyperboloid and there is no upper bound.
So it can go, um,- um,- um - the numbers can grow very large, but,
uh,- uh, it does not require a very high number of, uh, floating-point precisions to represent the, um, uh,
exponentially many, um - um, uh, points in that - that are towards the edge of the Poincare ball.
So, um - um, papers have shown that hyperbolic model is numerically more stable and,
um, it also have the advantage of, uh, having a slightly simpler formula to represent its, uh,
metric and, uh, as well as, uh, distances, um, in - in this hyperbolic model.
So a lot of times, uh, we do derivation and we, uh, represent the, uh, um,
neural network architecture inside this hyperboloid model. Uh, so these two are,
um- so the Poincare ball and the hyperboloid model, these two are equivalent in the sense that we can have a mapping, um,
that, uh, that are between the Poincare ball and the hyperboloid. And this mapping is one-to-one,
so, um, it's a projection. Um, so - so given uh,
the brief introduction of the, uh, uh, hyperbolic space, uh, we want to also define our task here.
So the task is the typical graph representation learning, uh, which we have, um, uh, we learned, uh,
throughout this lecture, and tasks could be things like link prediction or node concentration.
Um, and an example, um, um, example embedding space that achieves this, uh,
kind of, task is the, uh, Poincare embedding, um, um, by nickel, uh, in 2018.
Um, so in this visualization, which is in the form of a Poincare ball, we can see that entities are embedded inside this, um, Poincare ball.
Um, and towards the center of it is, um - um, they contain the nodes with very high hierarchy
or nodes that there are toward the roots of the tree. So for examples of entities,
we have, uh, physical entity, we have, uh, abstraction or - organ, uh, organism.
These are all very abstract and high level nodes that can contain many many different childrens.
And - and towards the edge of the, uh, uh, embedding space, there are many many different,
uh- exponentially many entities can be, um, um, embedded there, and these are typically nodes that are in
the lower hierarchy or towards the leaves of the tree like graphs. For example, things like, um, uh,
arrival, metabolic, uh, metabolic rates. These are very concrete and, uh, very detailed, uh, um - um, uh,
description of certain entity, um, and there are exponentially, many of them which can be embedded towards the edge of the Poincare ball.
And this is, uh, basically the, kind of, embedding we want to achieve so that we can perform things like link prediction,
which means that we can predict whether an entity and, uh, another entity, uh, in this,
uh, in this knowledge graph are connected. Um, um, meaning that, uh, uh, for, uh,
for example, whether the bread and food are connected, it could be connected through an hierarchical relations,
or it could be connected through a non-hierarchical relation. So, um, um, this is link prediction and we can also perform node classification,
um, by, um, um, predicting a label for each of these, uh, nodes in the graph.
Okay. And, um, before going into, uh, the detailed architecture, uh, let's first,
um, um, also, um, look into, uh, how hyperbolic space are mathematically represented.
So formal definition of the hyperbolic, uh, geometry is a manifold, um, um,
which is, uh, which is, uh, basically a high dimensional surface, a 2D surface in - but in high dimensional.
Um, and a Riemannian Manifold, um, is a special manifold, uh, which is equipped with two things.
One is the inner product. So the inner product, uh, between two points had to be defined, and it is, uh, uh,
it will define a metric space or like how distances are computed in - in this, uh, Riemannian manifold.
And the other thing that is, uh, um, that is necessary for Riemannian manifold is the tangent space, um,
which is a Euclidean space that approximates the manifolds, uh, but only locally.
So you can I - imagine it is tangent to the surface, um, at a certain point, uh, touching the surface at a certain point at xX. Um,
both function has to, uh- Riemannian manifold, states that both functions, uh, which is the inner product and the tangent space,
has to vary smoothly, or, uh, called differentiable on a manifold.
Um, there are also other, uh, concepts that are, uh, needed. So here, uh, we also considered the geodesics.
Geodesics, um, is defined as the shortest path in the manifolds, uh, which is analogous to the straight line in R^n.
So if you remember the - the plot where we have the parallel postulate, uh, in hyperbolic space,
um, all these curves are actually geodesics, so they are the shortest path between two points in the - in the manifold.
And that's why, um, we can say that there are infinitely many straight lines passing through the points, um, that are parallel to a given line.
Um, the hyperbolic space, uh, is a Riemannian manifold with a constant negative curvature.
Um, here we represented as negative 1 over K, where K has to be greater than 0.
Um, so as you can see, uh, when becomes very very large, um, um, the hyperbolic space curvature becomes very very close to zero,
um, and zero just means a flat space. So it will become more and more Euclidean as to,
um, uh, as K increases. Uh, but , uh, the more negative is the curvature. So when K is small, uh,
the more curved the space will be. And, um, here we look at a little bit of how does
the curvature affect the - affect the embedding space. Um, so we use our negative 1, uh,
1 over K to denote the curvature and, uh, uh, root K is also the radius of the Poincare ball.
Um, um and that's how, uh, uh, the radius is related to the curvature. So, um, um, yeah.
Uh, uh, if we look at the hyperboloid model, um, the - the - the formula is actually very simple.
So this is the, uh, inner product definition, which is also called the Minkowski inner product, uh, in hyperbolic space.
Um, so if we have a d-dimensional hyperbolic space, the hyperboloid model, um, will have a d plus 1 dimension.
And the, um, the inner product is defined, um, by these two things. So, um, there is the negative parts of the inner product,
and there is also the positive part of the inner product, which behaves almost identical to the Euclidean components.
But we also have the negative part, which is, uh, um, which has negative sign, uh, in this inner product.
And based on this, uh, inner product definition, the, uh, distance between two hyperbolic points,
um, can be computed as such. So given two points, x and y in the hyperbolic space, um,
you - we can compute this, um, uh, Minkowski inner products and divide by K, which is related to the curvature.
And the arcosh means the, uh, the inverse of the hyperbolic cosine function.
So we use this function, uh, to compute the distance between two points.
Um, and another important concept is called the tangent space. Uh, so as we mentioned that,
uh our tangent space is basically a Euclidean, um, uh, Euclidean approximation on a local,
uh, point of the manifold. So a local neighborhood of the manifold. Uh - uh we can have a Euclidean space that is tangent to the manifold,
but only touching the manifold at, uh, uh, at this one point. So for example, in this visualization,
we have this hyperboloid model, again, um, um, representing the hyperbolic space, um,
and, um, the, the tangent space, uh, at the North Pole, which is the bottom of the hyperboloid,
uh, is drawn over here. Um, and we can, uh, define certain mapping here. So this, um, this line,
um, um, this is actually a straight line. Of course it looks like a curve in a hyperbolic space, but if we also map it to the tangent space, this,
uh, becomes a straight line because the tangent space is the Euclidean space. And the tangent space is defined, again,
through this, uh, Minkowski inner product. Um, uh where, um, if we do inner product between v and x,
um - um, it is 0. So that is the, uh, that is the curve - that defines the,
uh, the set of all points, uh, v such that the inner product between v and x equals to 0 defines the entire, um, tangent space, uh,
at the point x. Um, and finally, we, um,
we discuss a little bit about geodesic distances. Um, geodesic distance, as- as we mentioned
is analogous to the shortest path in the Euclidean space, and, um, it's also the concept of straight line in hyperbolic space.
And the more negative the curvature, uh, the more the geodesics will bend inwards towards the center.
So - and - and the distance between, uh, two points will also increase. So here is an example plot where, uh,
we compute distance between, uh, two points, x and y, with the same coordinate. So, um, uh - uh - uh, say, uh,
like imagine something like x is 0.5, 0.6, y is 0.2, 0.3, and the distance between them actually increases
if the curvature increases- sorry if the curvature becomes more negative. So, um, towards, uh, the more active parts,
the - the distance between them actually increases more. So it can be more visualized in - in - in this figure, where,
um, we have these, uh, two points here, x and y in hyperbolic space. Here we just represent it in the Poincare, um, Poincare ball.
And here, uh, 0 is the origin, um, uh, root K is the, uh, is the radius,
which is also inversely proportional to the curvature of the space. And the dark blue, um,
curve means, um, is the boundary of the uh, hyperbolic space where, um,
the curvature, uh, is more negative, right? Uh, and the more negative the curvature is, uh, the- the smaller the radius will be.
So, uh, you can see that the geodesics are also drawn here. So, um, if the, uh,
in the dark blue - so in higher - higher curvature of space the - the geodesics is more bends towards the center,
towards, uh, the center of the hyper- uh hyperbolic space. Uh, whereas, uh, for the lighter blue, um,
version or the lighter blue space which has larger curvature, the, uh, geodesic is less bends inward.
And, um, in the extreme case where, um, the, um, x, y, uh, uh, the space is Euclidean meaning that the Poincare ball is kind of infinitely large,
then the geodesics does, uh, does not bend at all, which recovers our Euclidean concept of shortest path,
which is basically a straight line here. Um, and another important concept, um,
based on, uh, all these definitions is the exponential and logarithmic manner. So this is a map that we can map from the tangent space to the manifolds and vice versa.
So the exponential map maps points- any point in the tangent space to the Euclid- um,
um, uh, from the tangent space to the Euclidean- um, uh, sorry, from the Euclidean tangent space to the hyperbolic, um,
manifolds and the logarithmic maps, uh, from the, uh, manifolds, um, back to the Euclidean tangent space.
Um, so here, uh, it- it is a visualization, and we use the norm, um, to represent the inner product between,
uh, two points, uh, u- using this, uh, special, uh, Minkowski in the product definition.
Um, so we can uh, conceptually, um, visualize it as, um, you know, uh, we have v in the tangent space of x.
And if we want to map it back to the hyperbolic space, it becomes the point y, which is on the hyperbolic space.
Uh, and inversely, we can use logarithmic map to map from this y, the blue point,
back to the red point v, which is in the Euclidean space, is a Euclidean, uh, coordinate, and it's in the tangent space of x.
So here this notation, we use, um, x here as the- the point that is touching- uh,
so the tangent space of x, um, is the- is the Euclidean space that we want to map to and from.
And K is the curvature, uh, so it is the- uh, this- um, the inverse of the curvature that we have, uh, defined.
And here, notice that we have to use this cosh and sinh function, which are the- uh, which are the hyperbolic,
uh, uh, trigonometry here. Um, the formulas are- are really very complicated. Um, um, um, but, um, um,
if the high-level concept is that there exists- you just need to know that there exists such mapping between the- uh,
the tangent space as well as the, uh, um, uh, manifolds. And, um, uh, we can use such mapping to, um,
perform operation in the Euclidean space and ma- map it back to the hyperbolic space.
Um, so given all these preliminaries, we can now introduce the, uh, hyperbolic GNN.
So the main challenge that, uh, we are focusing on here is, uh, that, uh, input features are usually Euclidean.
Um, um, but we want to map it, uh, to the hyperbolic space to- uh, to compute the node embedding on the hyperbolic space.
And the second challenge is that, uh, we need to perform aggregation in the hyperbolic space, uh, uh, so that we can perform message passing.
And, uh, the third challenge is that a hyper- uh, we do not know what is the right amount of curvature
for the hyperbolic space that can perfectly embed the graph. So, uh, this is also something that we need to learn in the meantime.
Um, so if you recall, the- um, the, um, uh, GNN, uh, formulation is mostly very similar.
So, uh, here is an outline of the, uh, GNN architecture where we first compute
the messages between all the neighbors as well as the central nodes. And- and then we aggregate all your neighbors', uh, messages, uh,
to compute- uh, to compute a representation- uh, the hidden representation after the aggregation.
And finally, uh, we will, uh, make an update, uh, um, of the representation, um, so that, uh,
the- the next level representation will- um, will, uh, uh, capture the information of the previous layer representation.
Um, so this is- this will be the, uh, typical like message, aggregation, and update function,
um, but in a hyperbolic space. So notice that here we are writing, um, some, uh, superscripts.
For example, like K_l minus 1, so this will indicate the curvature or the inverse of curvature at the layer l minus 1.
And here, it is K_l minus 1, K_l, meaning that the update function needs to convert the point from the,
uh, uh, hyperbolic space of the previous layer, so, uh, in the layer l minus 1 to the hyperbolic space of a layer l. Um,
and the high-level concept, uh, is- is also visualized in the- um, in the figure here.
So we have a bunch of points, uh, which are the previous layer embedding of the graph in the previous hyperbolic space.
And we perform, um, a message computation of these- um, uh, uh, between these points as well as this center point here.
Um, and after we perform these messages, we need to aggregate all these messages. And we perform this aggregation in this,
uh, tangent space of the, uh, hyperbolic space because there is no natural definition of neural network operations in the hyperbolic space.
Um, but if we map them into the tangent space, then we can, uh, do something like a center of gravity inside this tangent space,
uh, at the point x. So we perform this, uh, aggregation in the tangent space.
And after the aggregation, we have the green point here which is, um, then mapped back to the- um,
an- another hyperbolic space of a different curvature. So you notice that here, it is the layer l minus 1, sorry,
layer l plus 1, and in the previous layer, uh, it will be layer l. So this is a completely different hyperbolic space with a different curvature that we- um,
but we can use the, uh, exponential and logarithmic map to map the aggregated results back to the- uh,
the- um, the hyperbolic points in the new embedding space- in the new embed space with a different curvature.
Um, so the, uh, formula for all these computations are very complex,
um, and un- understanding them requires like, uh, uh, entire lecture on differential geometry.
So I won't go very much into the detail, but there are some high-level, uh, concepts that you can get from- uh, get- get from here.
So, for example, in the message computation, because, um, hyperbolic space has no natural neural network operations that can be defined here,
we can use this, um, um, um, linear and- uh, so linear layer basi- the linear and addition, um,
of the bias term, uh, in the tangent space of- um, of the, um, um,
of the hyperbolic space. So here, we use this, um, circle and multiply it, uh, to represent this- like multiply all the linear- linear operation, and this, uh,
circle and- um, and, um, plus, uh, inside it, uh, to represent the addition,
so the Mobius addition. Uh, and- and these are very ea- um, and- and as you can see, these are, uh, defined through this exponential and logarithmic map here.
So we first use the logarithmic map to map the points- the hyperbolic points to the Euclidean,
um, tangent space, um, on that point- uh, on the origin. So this- here this o means origin.
So we first map it to the orig- um, the tangent space of the origin. We perform this ma- uh, matrix multiplication in the tangent space.
And because tangent space is Euclidean, this operation is well-defined. And after we perform this linear operation,
you can then map it back to- uh, to the- uh, um, uh, to the hyperbolic space through this exponential map.
So, um, compared to the Euclidean operation, basically ask this exponential and logarithmic back to
convert to and from the tangent space to perform this operation. And similar is also true for,
um, addition, but here, we need to have a special operator called parallel transpor- transport,
uh, which- um, which tells you how to move the vector around in the space. So, um, this might not seem very intuitive because in Euclidean space, um,
a vector, uh, can be moved around the Euclidean space without affecting its length. But actually, in hyperbolic space,
you cannot freely move the vector because as you can see in the visualization, if we move the- uh, if we- we move a vector,
um, out towards the edge of it, it will seem smaller, right? The coordinates will- um, the- the norm of that vector will have to change.
So there's this also the parallel transport here, um, um, and that, uh, represent the analogy of the,
uh, uh, addition operation in the hyperbolic space. Um, and all these- notice that all these functions,
although they are very complex, are differentiable. So we can also use PyTorch and any, uh, different- uh, other differentiation software to perform these operations.
Um, and in the aggregation, this is basically very similar to graph attention networks that we have learned.
So we have this attention, uh, weights here. And we also map it- uh, map the points, uh,
to the tangent space, uh, use the, uh, attention weights to aggregate them and then again map it back to the hyperbolic space.
And in the hyp- uh, update function, it again follows the same- uh, same, um, uh, rule where we map it to the tangent space,
we perform the non-linearity. But note that here, we, uh, map it back to a different curvature, right, um, because the next level has
different coverage around different space definition. Uh, so finally, just to show a little bit of results,
um, uh, here we can see a GNN embedding of a tree-like structure.
Um, and as you can see, because there are exponentially many, uh, uh, points in the space and, um, uh,
the- the- the geometry is not very nicely preserved, uh, because, uh, uh,
here we visualize the nodes as like the lighter blue nodes represent the leaves and the dark blue nodes represent the, uh, roots.
And you can see that some of these leaves are actually very embedded very close to the root, which does not respect the tree structure.
Whereas in the hyperbolic geometry, uh, using the hyperbolic GCN, we're able to, uh,
recover this tree-like structure very nicely, uh, and, uh, all the points are, uh, embedded in- in their respective hierarchy.
And we also discovered that curvature is very important, and there is a sweet spot where we can embed the- um,
a tree with a low distortion, but also very unstable- uh, numerically, uh, very stable.
And, uh, finally, um, we also evaluated in a bunch of, uh,
other- um, other, uh, standard datasets, uh, graph datasets. And we basically use this hyperbolicity to measure, like,
how good, uh, is the hyperbolic GNN, uh, will perform in a given dataset.
So this metric can be found in this, uh, Gromov, uh, hyperbolicity paper.
Um, and basically the lower, uh, hyperbolicity, uh, the graph becomes more tree-like. So here, for example, delta equals to 0,
uh, that means that the graph is, uh, kind of a perfect tree. Whereas here, if Delta is 11,
then the graph is not very hierarchical or not very tree-like. So we can see that, uh, the hyperbolic GNN will per- tend to
perform very well or extremely well when hyperbolicity is 0, so which means that the graph is, uh, very tree-like.
Whereas if the graph, uh, does not look like a tree, then the- uh, uh, uh, the performance tend to drop.
So, uh, that's why the hyperbolic GNN is especially suitable for hierarchical and on tree-like graphs.
Uh, just to summarize, uh, here we introduce the hyperbolic geometry. And we have, uh, two models,
the Poincare ball and hyperboloid uh, model, uh, to, uh, visualize and represent the hyperbolic, uh,
space in the Euclidean- as a subspace of the Euclidean space. And we can use exponential logarithmic map to map points to and from the tangent space,
which is Euclidean, where neural network operations can be performed. And finally, we show that we- uh, we can use a learnable,
um, uh, curvature, um, um, at every layer of the GNN so that we can trade off performance and stability.
Uh, thank you, and that's- um, that's all for, uh, this part of the content.

Video URL: Local File - Stanford CS224W Machine Learning with Graphs 2021 Lecture 19.3 - Design Space of Graph Neural Networks.txt
Uh, hi, everyone. My name's Jiaxuan,
and I'm the head TA of this course.
And really an amazing experience to work
with you guys and I hope you learn a lot from the course.
Uh, today I'm excited to present, uh,
my recent, uh, research, Design Space of Graph Neural Networks.
[NOISE] So in this lecture,
uh, we cover some key questions for GNN design.
Uh, specifically, we want to answer how to
find a good GNN design for a specific GNN task.
Uh, this problem is really important, but also challenging, uh,
because domain experts want to use state-of-art GNN on their specific task.
However, there are tons of possible GNN architectures.
Uh, for example, in this lecture,
we have covered GCN,
GraphSAGE, GAT, GIN, etc.
Uh, the issue here is that
the best GNN design in one task can perform badly for another task.
And redo the hyperparameter grid search for each new task is not feasible.
And I'm sure you have some hands-on experience in your, uh,
final project and, you know,
tuning the hyperparameter of GNNs is notoriously hard.
Uh, In this lecture, our, uh,
key contribution in this work is that
the first systematic study for a GNN design space and task space.
And in addition, we also released the called platform GraphGym,
uh, which is a powerful platform for exploring different GNN designs and tasks.
[NOISE] Uh, to begin,
we first introduce the terminology that we'll use in this, uh, lecture.
Um, so a design means a concrete model instantiation.
For example, a four-layer GraphSAGE is a specific design.
Design dimensions characterize a design.
For example, a design dimension could be the number of layers,
L, which could take values among 4,
uh, 2, 4, 6, 8.
And design choice is the actual selective value in the design dimensions.
For example, the number of layers, L equals 2.
Design space, uh, consists of a Cartesian product of
all the design dimensions relate to enumerate all the possible designs within the space.
A task is the, a- a- a specific type of interest,
which could be, for example,
uh, node classification on Cora data set,
graph classification on ENZYMES data set.
And the task space consists of all the tasks that we care about.
And in this paper we introduced the notion of GNN design space,
and actually, we have, uh,
go into much deta- detail in the previous lecture,
so here we'll just do a quick recap.
Uh, so in the d- GNN design space,
we consider first, uh,
the intra-layer, uh, design.
And we have introduced that a GNN layer can be understood as,
uh, two parts, the transformation function,
and the aggregation function.
And, uh, here we propose a general instantiation under this perspective.
So concretely, it contains four different dimensions.
Uh, so we have, uh,
whether to add BatchNorm,
uh, whether to add dropout,
uh, the exact selection of the acti- activation function,
and the selection of the aggregation function.
Next, uh, we are going to design the inter-layer connectivity.
And the lecture, we have also introduced, uh,
different ways of organizing GNN layers.
And in- in this, uh, in this work, uh,
we consider adding some, uh,
pre-process layers and post-process layer,
uh, in addition to the GNN layers, uh,
which can, uh, jointly,
uh, form a complete graph neural network.
So the intuition of adding pre-process layer is that it could pretty important,
uh, when expressing node feature encoders are needed.
So for example, when our nodes are extracted from images or text,
we'll be consider using some, uh,
expressive, say, convolutional neural networks
or transformers to encode these node features.
And then we may also add
some post-process layer after applying graph neural network computation,
which are important when we are going to say
a reason or transformation over node embeddings.
And some example, uh, are, say, uh,
doing gra- graph classification,
or some applications around are not expressed.
And the core, uh,
core of the graph neural network are, uh, GNN layers.
And there we consider different strategies to add skip connections.
And we found that this really helps improve,
uh, deep GNNs performance.
Uh, then finally, we'll cover different learning configurations for GNNs.
And actually, this is often neglected,
uh, in current literature, uh,
but in practice, we found that
these learning configurations have high impact on- on a GNN's performance.
So specifically, we'll consider, uh,
the batch size, the learning rate,
the optimizer for gradient update,
and- and how many epochs do we train our models.
So in summary, uh,
we have proposed a general GNN design space that consist of,
uh intra-layer design, inter-layer design, and learning configuration.
And If you com, uh, consider all the possible combinations,
this really lead to a huge space,
so it contains, uh,
315,000 possible GNN designs.
And, um, to clarify,
our purpose here, uh,
is that we don't want to and what cannot cover
all the possible GNN designs because for example,
we can even add more, uh, design dimension,
say rather to add attention,
how many attention it has to use, and etc.
So this space is really a very- very huge.
So what we're trying to do is to propose a mindset transition.
So we want to demonstrate that studying
a design space is more effective than studying individual GNN designs,
such as, uh, uh, considering- only considering GraphSAGE,
GAT, those individual designs.
So after introducing the GNN design space,
we'll then introduce the GNN task space.
And we'll categorize GNN task,
uh, into different, uh, categories.
Um, so the common practice is to categorize GNN task into node classification,
edge, uh, prediction, and graph level prediction tasks.
And we have covered how do we do this in previous lectures.
Although this, uh, this technology is reasonable, it is not precise.
So for example, if we consider a node prediction and we could do say,
predict node clustering coefficient.
Another task could be, uh,
we will predict a node subject area in a citation network.
So although these tasks are all node classification, uh,
they are completely, uh, completely different in terms of their semantic meaning.
However, creating a precise taxono- ta-
taxonomy of GNN tasks is very hard because first, uh,
this is really subjective how you want to categorize different task,
and second there is normal GNN task can always merge and you cannot,
uh, uh, predict the future of the- the unknown, uh, GNN tasks.
So our innovation here is to propose a quantitative task similarity metric.
And our purpose here is to understand GNN task and, uh,
uh, uh result we can transfer the best GNN models across different tasks.
And- so here's a concrete,
uh, our innovation, uh,
where we propose, uh,
quantitative task similarity metric.
So to do this, uh, we will first select a notion called anchor models.
So here's a concrete example.
Suppose we want to, uh,
measure the similarity between tasks A, B,
and C, and then the anchor models are M_1 through M_5.
The second step is that we'll characterize a task by ranking the performance,
uh, of anchor models.
So here I say task A have the ranking of say, 1, 2, 3, 4, 5.
Task B have the ranking, uh,
which is different, which is a 1, 3, 2, 4, 5.
And task C again has another ranking
among the anchor models in terms of their performance.
And our argue-, uh, the key insight here is that, uh,
the task with simi- similarity rankings,
uh, similar rankings are considered as similar.
So for example, um,
here we can see the similarity between the rankings of,
uh, task A and task B is pretty high.
And the similarity between task A and C is pretty low.
And this way, we can give a quantitative measure between different- different tasks.
Uh, the next question is that how do we select the anchor models?
So more concretely, we will do, uh,
three steps to select the anchor models.
Uh, first, we'll pick a small dataset that it easy to work on.
And second, we'll randomly sample N models
from our design space and we'll run them on our dataset.
For example, we can sample 100 models,
uh, from our entire design space.
The third step is that we'll sort these models based on their performance and then we'll
evenly select M models as
the anchor models whose performance range from the worst to the best.
So for example, we have picked,
uh, random 100 models,
we will sort them by their performance and then say we'll pick the top model as
the first anchor set- anchor model and then set the 10th percentile,
uh, model as the second anchor model.
And then up to the worst model among 100 models.
And our goal here,
is really to come up with a wide spectrum of models.
And our integration is that a bad model in
one task could actually be great for another task.
And we have verified this,
uh, with our experiments results.
Contrarily, we co- can collect, uh,
32 tasks, uh, which are,
uh, nodes and graph classification tasks.
And we have six real-world node classification tasks, uh,
12 synthetic node classification tasks, uh,
including a predicting node clustering coefficient, and node PageRank.
And then we also have six real-world graph classification tasks
and eight synthetic graph classification tasks,
uh, including, uh, predicting graph average path lengths.
The final topic we will cover is that having defined, uh,
our GNN design space and task space,
how do we evaluate the GNN designs?
For example, we want to answer the question like, uh,
is graph- is BatchNorm generally useful for GNNs?
Um, here the common practice is just to pick one model,
for example, a five layer,
64-dimensional GCN, and compare two models,
uh, with or without BatchNorm.
Uh, our approach here is that,
uh, is more rigorous.
Uh, that is, uh,
we know that we have defined 300,000 models and 32 tasks,
and this data leads to, uh, uh,
about 10 million model-task combinations.
And what we are gonna do is to first sample from
the 10 million possible model-task combinations and we'll
rank the models with BatchNorm equals true or false.
The next question is that how do we make it scalable and convincing?
[NOISE] And more concretely,
our proposed approach called,
uh, controlled random search.
So the first step, is to sample
random model-task configurations from the entire design space.
And we perturb the BatchNorm equals true or false.
So for example, uh, we have different, uh,
uh, models with different,
uh, GNN designs, such as, uh,
ReLu activation, PReLu activation,
and different number of layers,
different la- layer connectivity and they're applied to different GNN tasks.
What we're going to do is that,
we will fix the all- the rest of design and task dimensions,
but only perturb its BatchNorm dimensions into true or false.
And in the meantime, we will control the computational budget for
all the models so that this comparison is really rigorous.
And then we will rank BatchNorm equals true or false by their performance.
Here, lower ranking is better.
So for example, we can see, okay, uh,
in one application BatchNorm equals true have validation accuracy of 0.75, uh,
but false with only, uh,
0.54, which means that BatchNorm equals true is better.
So it has a lower ranking of 1.
And sometimes there could be a tie because the two,
uh, choices are pretty close in terms of their performance.
The final step is to plot average or
distribution of the ranking of the BatchNorm equals true or false.
So for example, here we see the average ranking of the BatchNorm is true is lower,
which means that, uh, in general BatchNorm is equals true often provokes better.
So to summarize, um, here,
we really propose an approach to convincingly evaluate any new design dimensions.
And for example, we can use
the same strategy to evaluate a new GNN layer that we propose.
So here are the, uh, key results.
First, we will demonstrate a general guideline for GNN designs.
So, uh, we showed that certain design choices exhibit clear advantages.
So we'll first look at those intralayer designs.
Um, the first, uh, conclusion that,
uh, BatchNorm equals true, are generally better.
And our explanation is that GNNs are hard to optimize,
therefore, batch normalization can really help,
um, the gradient update.
And then we found that dropout equals 0,
which means no dropout is often better.
Because we found that GNNs actually experience
under fitting more often than over fitting.
So BatchNorm-, uh, sorry.
So our drop out doesn't-, uh, it doesn't help too much.
And then we found that, uh,
PRelu activation actually really stands out.
And this is our new findings in this paper and, ah,
versus the common practice of only using the ReLu activation.
And finally, we found that sum aggregation is always
better because we have explained in the lecture,
that sum is the most expressive agg- aggregator that we could have.
And then we'll go on to look at the inter-layer designs.
Um, first, we found that the optimal number of layers is really hard to decide.
You can see their rankings are pretty, uh, even.
And we argue that this is really highly dependent on the task that we have.
And also we find that, uh,
sk- skip connections can really enable
hierarchical node representation therefore is much desired.
And finally, we will look at the learning configurations.
We found that the optimal batch size and learning rate is also hard to decide.
And therefore it's highly dependent on the task.
And we found that the Adam optimizer and training more epochs are generally better.
The second key result is the understanding of GNN tasks.
First, we found that GNN designs in different tasks vary significantly.
So this motivates that studying the task space is really crucial.
So if we look at design,
um, tradeoff in different tasks,
like BZR proteins and smallworld,
sometimes max aggregation is better,
sometimes mean is better and sometimes sum is better.
And similarly for a number of layers,
sometimes a eight-layer is better,
sometimes two-layer is better, uh, and etc.
So this, uh, argues that our GNN task space is pretty helpful.
So what we're going to do is to compute pairwise similarities between all GNN tasks.
So, uh, recall how we compute GNN task.
We will measure the similarity based on anchor model performance.
And then, uh, the argument is that our task similarity computation is really cheap.
And we found that using 12-anchor models is already a good approximation.
And our key result is that the proposed GNN task space is pretty informative.
So we identify two group of GNN task.
Group A relies on feature information.
Uh, and these are some node cla- or graph
classification task where input graphs have high dimensional features.
And Group B, our task relies on structural information where nods have fewer of,
uh, features but predictions are highly dependent on the graph structure.
And then we'll do PCA and do dimension,
uh, reduction, uh, to visualize this in 2D space.
And indeed we verified that similar tasks can have similar best architecture designs.
And finally, we will go on to transfer,
uh, our approach to novel task.
So here we conduct a case study that is to generalize
the best models to unseen OGB task and to,
uh, that the observation that the OGB, uh,
molecule, uh, prediction task is unique from other tasks.
So it's 20 times larger,
highly imbalanced, and requires out-of-distribution generalization.
So this is really a novel task compared to the tasks that we have seen.
And here's a concrete step to apply our approach to a novel task.
So the first step is to measure 12,
uh, anchor model performance on a new task.
And then we're going to compute similarity between the new task and the existing task.
Finally, we'll recommend the best design
from the existing task with the highest similarity.
So here are the concrete results.
Um, so we'll pick two models,
uh, using our task similarity metric.
So task A is highly similar to OGB and task B are not similar to OGB.
And our finding is that transferring the best model from task A,
really achieves SOTA performance on OGB.
However, uh, transfer the best model from task B performs badly on OGB.
So this really, uh, illustrates that the proposed task metric is really helpful.
And our task space can really guide the best model transfer to node tasks.
To summarize- to summary,
uh, in this paper,
we proposed the first systematic investigation of general guidelines for GNN design.
And the understandings of GNN tasks as well
as transferring best GNN designs across tasks.
In addition, we also released GraphGym as an easy to use code platform for GNNs.
Uh, thank you for your attention.

